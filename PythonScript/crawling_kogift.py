import asyncio
import pandas as pd
from playwright.async_api import Browser, Page, Error as PlaywrightError
import random
import logging
from urllib.parse import urlparse, urljoin
import re
from typing import Optional, List, Dict, Any, Tuple
from utils import generate_keyword_variations
import configparser # Import configparser
import os
import time
import httpx
import aiohttp
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import (
    TimeoutException, 
    NoSuchElementException, 
    StaleElementReferenceException,
    ElementClickInterceptedException,
    ElementNotInteractableException
)
import requests
from concurrent.futures import ThreadPoolExecutor
from image_downloader import predownload_kogift_images, verify_image_url
import argparse
from pathlib import Path
from PIL import Image
import shutil
import hashlib
import json

# --- Ìï¥Ïò§Î¶Ñ Í∏∞ÌîÑÌä∏ ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ÏóêÏÑú ÏàòÎüâ Ï∂îÏ∂ú Ìï®Ïàò ---

# Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ Í≤ΩÎ°ú Ï§ëÏöî Ï†ïÎ≥¥:
# /ez/ Í≤ΩÎ°úÍ∞Ä Ïù¥ÎØ∏ÏßÄ URLÏóê Î∞òÎìúÏãú Ìè¨Ìï®ÎêòÏñ¥Ïïº Ìï©ÎãàÎã§.
# ÏûòÎ™ªÎêú ÌòïÏãù: https://koreagift.com/upload/mall/shop_1736386408518966_0.jpg
# Ïò¨Î∞îÎ•∏ ÌòïÏãù: https://koreagift.com/ez/upload/mall/shop_1736386408518966_0.jpg
# ÏúÑÏùò /ez/ Í≤ΩÎ°úÍ∞Ä ÏóÜÏúºÎ©¥ Ïù¥ÎØ∏ÏßÄ Î°úÎìúÍ∞Ä Ïã§Ìå®ÌïòÎØÄÎ°ú Î™®Îì† Ïù¥ÎØ∏ÏßÄ URL Ï≤òÎ¶¨ Ïãú ÌôïÏù∏Ìï¥Ïïº Ìï©ÎãàÎã§.

# Î°úÍ±∞ ÏÑ§Ï†ï (basicConfigÎäî Î©îÏù∏ÏóêÏÑú Ìïú Î≤àÎßå Ìò∏Ï∂úÌïòÎäî Í≤ÉÏù¥ Ï¢ãÏùå)
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__) # Get logger instance

def get_kogift_urls(config: configparser.ConfigParser) -> List[str]:
    """
    Get list of Kogift URLs from config file.
    
    Args:
        config: ConfigParser object containing configuration
        
    Returns:
        List[str]: List of Kogift URLs to scrape
    """
    urls = []
    try:
        if config.has_section('ScraperSettings'):
            # Get URLs from config
            if config.has_option('ScraperSettings', 'kogift_urls'):
                urls_str = config.get('ScraperSettings', 'kogift_urls')
                urls = [url.strip() for url in urls_str.split(',') if url.strip()]
            
            # If no URLs in config, use defaults
            if not urls:
                urls = [
                    'https://koreagift.com/ez/index.php',
                    'https://adpanchok.co.kr/ez/index.php'
                ]
    except Exception as e:
        logger.error(f"Error getting Kogift URLs from config: {e}")
        # Fallback to default URLs
        urls = [
            'https://koreagift.com/ez/index.php',
            'https://adpanchok.co.kr/ez/index.php'
        ]
    
    return urls

def get_max_items_per_variation(config: configparser.ConfigParser) -> int:
    """
    Get maximum number of items to scrape per keyword variation from config.
    
    Args:
        config: ConfigParser object containing configuration
        
    Returns:
        int: Maximum number of items to scrape per variation
    """
    try:
        if config.has_section('ScraperSettings'):
            return config.getint('ScraperSettings', 'kogift_max_items', fallback=100)
    except Exception as e:
        logger.error(f"Error getting max items per variation from config: {e}")
    
    return 100  # Default value from config.ini

# Constants removed, now loaded from config
# KOGIFT_URLS = [...]
# USER_AGENT = "..."
# MIN_RESULTS_THRESHOLD = 5

# Add browser context timeout settings
BROWSER_CONTEXT_TIMEOUT = 300000  # 5 minutes
PAGE_TIMEOUT = 120000  # 2 minutes
NAVIGATION_TIMEOUT = 60000  # 1 minute

# --- Helper function to download images ---
def download_image(url: str, save_dir: str, file_name: Optional[str] = None) -> Optional[str]:
    """Download an image from a URL and save it to the specified disk directory."""
    if not url or not save_dir:
        logger.warning("URL or save directory not provided to download_image")
        return None
    
    # Normalize URL
    if not url.startswith(('http://', 'https://')):
        if url.startswith('//'):
            url = f"https:{url}"
        else:
            url = f"https://{url}"
    
    # Extract filename from URL if not provided
    if not file_name:
        # URLÏùò Ìï¥ÏãúÍ∞íÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Í≥†Ïú†Ìïú ÌååÏùºÎ™Ö ÏÉùÏÑ±
        url_hash = hashlib.md5(url.encode()).hexdigest()
        original_ext = os.path.splitext(urlparse(url).path)[1].lower()
        if not original_ext or original_ext not in ['.jpg', '.jpeg', '.png']:
            original_ext = '.jpg'
        file_name = f"kogift_{url_hash}{original_ext}"
    
    # Create save directory if it doesn't exist
    try:
        os.makedirs(save_dir, exist_ok=True)
    except PermissionError:
        logger.error(f"Permission denied when creating directory: {save_dir}")
        return None
    
    # Generate file path and check if it exists
    file_path = os.path.join(save_dir, file_name)
    if os.path.exists(file_path):
        # Check if file size is > 0 to consider it valid
        if os.path.getsize(file_path) > 0:
            logger.debug(f"Image already exists: {file_path}")
            return file_path
    
    # Create a unique temporary file path
    temp_file_path = os.path.join(save_dir, f"{os.path.splitext(file_name)[0]}_{random.randint(1000, 9999)}.tmp")
    
    # Try to download with retries and exponential backoff
    max_attempts = 3
    for attempt in range(max_attempts):
        try:
            # Download image
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            }
            response = requests.get(url, headers=headers, timeout=15, stream=True)
            
            # Check response
            if not response.ok:
                logger.warning(f"Failed to download image from {url} (attempt {attempt+1}): HTTP {response.status_code}")
                # Exponential backoff for retry delay
                delay = 1.0 * (2 ** attempt) + random.uniform(0, 0.5)
                logger.debug(f"Retrying in {delay:.2f} seconds...")
                time.sleep(delay)
                continue
            
            # Validate content type
            content_type = response.headers.get('Content-Type', '').lower()
            if 'image' not in content_type and 'octet-stream' not in content_type:
                # Some sites may return valid images without proper content type
                if len(response.content) < 1000 and 'text/html' in content_type:
                    logger.warning(f"Not an image (HTML page received): {url}")
                    delay = 1.0 * (2 ** attempt) + random.uniform(0, 0.5)
                    time.sleep(delay)
                    continue
            
            # Write to temp file
            try:
                with open(temp_file_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                    f.flush()
                
                # Small delay after writing to ensure file handles are closed
                time.sleep(0.2)  # Increased delay to ensure file handles are released
            except (PermissionError, OSError) as e:
                logger.warning(f"Error writing to temp file {temp_file_path}: {e}")
                # Try a different temp filename
                temp_file_path = os.path.join(save_dir, f"{os.path.splitext(file_name)[0]}_{int(time.time())}_{random.randint(1000, 9999)}.tmp")
                delay = 1.0 * (2 ** attempt) + random.uniform(0, 0.5)
                time.sleep(delay)
                continue
            
            # Validate image
            try:
                # Check file size
                if os.path.getsize(temp_file_path) < 100:
                    logger.warning(f"Downloaded file too small: {url}")
                    try:
                        os.remove(temp_file_path)
                    except:
                        pass
                    delay = 1.0 * (2 ** attempt) + random.uniform(0, 0.5)
                    time.sleep(delay)
                    continue
                
                # Validate image with PIL
                try:
                    with Image.open(temp_file_path) as img:
                        img.verify()  # Verify it's a valid image
                    
                    # Open again to check dimensions (verify closes the file)
                    with Image.open(temp_file_path) as img:
                        width, height = img.size
                        if width < 10 or height < 10:
                            logger.warning(f"Image too small ({width}x{height}): {url}")
                            try:
                                os.remove(temp_file_path)
                            except:
                                pass
                            delay = 1.0 * (2 ** attempt) + random.uniform(0, 0.5)
                            time.sleep(delay)
                            continue
                except Exception as img_err:
                    logger.warning(f"Invalid image data: {img_err}")
                    try:
                        os.remove(temp_file_path)
                    except:
                        pass
                    delay = 1.0 * (2 ** attempt) + random.uniform(0, 0.5)
                    time.sleep(delay)
                    continue
                
                # Try to move the temporary file to the final location
                move_success = False
                for move_attempt in range(3):
                    try:
                        # If file exists, try to remove it first
                        if os.path.exists(file_path):
                            try:
                                os.remove(file_path)
                                # Add additional delay after file removal
                                time.sleep(0.5)
                            except (OSError, PermissionError) as e:
                                if "WinError 32" in str(e):  # File being used by another process
                                    logger.warning(f"File in use (WinError 32): {file_path}")
                                    # Create alternative filename with timestamp and random number
                                    file_path = os.path.join(save_dir, f"{os.path.splitext(file_name)[0]}_{int(time.time())}_{random.randint(1000, 9999)}{os.path.splitext(file_name)[1]}")
                                    # Skip the remove operation and try with new filename
                                    time.sleep(0.5)
                                else:
                                    logger.warning(f"Cannot remove existing file {file_path}: {e}")
                                    # Create alternative filename
                                    file_path = os.path.join(save_dir, f"{os.path.splitext(file_name)[0]}_{int(time.time())}_{random.randint(1000, 9999)}{os.path.splitext(file_name)[1]}")
                        
                        # Try to rename (fastest method)
                        os.rename(temp_file_path, file_path)
                        move_success = True
                        break
                    except (OSError, PermissionError) as e:
                        err_msg = str(e)
                        logger.warning(f"OS error renaming file (attempt {move_attempt+1}): {err_msg}")
                        
                        # Handle "file in use" errors (Windows Error 32)
                        if "WinError 32" in err_msg:
                            logger.info(f"File in use (WinError 32), waiting before retry...")
                            # Longer delay for file access issues
                            time.sleep(1.5 + random.uniform(0, 1.0))
                            # Create alternative filename with more randomness
                            file_path = os.path.join(save_dir, f"{os.path.splitext(file_name)[0]}_{int(time.time())}_{random.randint(10000, 99999)}{os.path.splitext(file_name)[1]}")
                        else:
                            time.sleep(0.8 + random.uniform(0, 0.7))
                        
                        # On the second attempt, try with shutil.move
                        if move_attempt == 1:
                            try:
                                # Use copy2 + remove instead of move to reduce file locking issues
                                shutil.copy2(temp_file_path, file_path)
                                time.sleep(0.5)  # Wait before deleting source
                                try:
                                    os.remove(temp_file_path)
                                except:
                                    pass  # Ignore if temp file can't be deleted
                                move_success = True
                                break
                            except Exception as e2:
                                logger.warning(f"Shutil move error: {e2}")
                                time.sleep(0.8 + random.uniform(0, 0.5))
                
                # If move failed, try copy + delete approach
                if not move_success:
                    try:
                        # Try another unique filename for last attempt
                        file_path = os.path.join(save_dir, f"{os.path.splitext(file_name)[0]}_{int(time.time())}_{random.randint(100000, 999999)}{os.path.splitext(file_name)[1]}")
                        shutil.copy2(temp_file_path, file_path)
                        time.sleep(0.5)  # Wait before trying to delete the source
                        try:
                            os.remove(temp_file_path)
                        except:
                            pass  # Ignore failure to delete temp file
                        move_success = True
                    except Exception as e:
                        logger.error(f"OS error saving image to {temp_file_path} or {file_path} (attempt {attempt+1}): {e}")
                        # Last resort - use temp file as actual file
                        if os.path.exists(temp_file_path) and os.path.getsize(temp_file_path) > 0:
                            logger.warning(f"Using temp file as final file: {temp_file_path}")
                            file_path = temp_file_path
                            move_success = True
                
                if move_success:
                    logger.info(f"Downloaded image: {url} -> {file_path}")
                    return file_path
                
            except Exception as e:
                logger.warning(f"Invalid image data from {url}: {e}")
                if os.path.exists(temp_file_path):
                    try:
                        os.remove(temp_file_path)
                    except:
                        pass
                delay = 1.0 * (2 ** attempt) + random.uniform(0, 0.5)
                time.sleep(delay)
            
        except requests.exceptions.RequestException as e:
            logger.warning(f"Request error downloading {url} (attempt {attempt+1}): {e}")
            delay = 1.0 * (2 ** attempt) + random.uniform(0, 0.5)
            time.sleep(delay)
        except Exception as e:
            logger.error(f"Error downloading {url} (attempt {attempt+1}): {e}")
            delay = 1.0 * (2 ** attempt) + random.uniform(0, 0.5)
            time.sleep(delay)
    
    logger.error(f"Failed to download image after {max_attempts} attempts: {url}")
    return None

def download_images_batch(img_urls, save_dir='downloaded_images', max_workers=10):
    """
    Download multiple images in parallel using a thread pool.
    
    Args:
        img_urls: List of image URLs to download
        save_dir: Directory to save the images
        max_workers: Maximum number of concurrent downloads
        
    Returns:
        dict: Mapping of URL to local file path for successful downloads
    """
    results = {}
    
    logger.info(f"Downloading {len(img_urls)} images to {save_dir}")
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_url = {
            executor.submit(download_image, url, save_dir): url 
            for url in img_urls if url
        }
        
        for future in future_to_url:
            url = future_to_url[future]
            try:
                path = future.result()
                if path:
                    results[url] = path
            except Exception as e:
                logger.error(f"Error downloading image {url}: {e}")
    
    success_count = len(results)
    logger.info(f"Downloaded {success_count}/{len(img_urls)} images successfully")
    
    return results

# --- Helper function to block unnecessary resources --- 
def should_block_request(url: str) -> bool:
    """Determines if a network request should be blocked."""
    # Block images (except product images if needed - needs more specific pattern), 
    # stylesheets, fonts, and common tracking/ad domains.
    # Adjust the patterns based on actual site structure and needs.
    # Example: Allow product images from koreagift main server
    if ("koreagift.com" in url or "adpanchok.co.kr" in url) and any(ext in url for ext in [".jpg", ".png", ".jpeg"]):
        # Add more specific logic if needed, e.g., checking path like '/goods/'
        return False # Don't block potential product images from main domains

    blocked_domains = ["google-analytics.com", "googletagmanager.com", "facebook.net", "adservice.google.com", "googlesyndication.com", "doubleclick.net"]
    parsed_url = urlparse(url)
    # Block specific resource types based on typical file extensions
    if any(parsed_url.path.lower().endswith(ext) for ext in [".css", ".woff", ".woff2", ".ttf", ".eot", ".gif", ".svg", ".webp"]):
        # Consider allowing specific crucial CSS if site breaks
        return True # Block styles/fonts/non-product images by default
    # Block based on domain
    if any(domain in parsed_url.netloc for domain in blocked_domains):
        logger.debug(f"Blocking request to tracking/ad domain: {url}")
        return True
    return False

async def setup_page_optimizations(page: Page):
    """Applies optimizations like request blocking to a Playwright page."""
    try:
        await page.route("**/*", lambda route:
            route.abort() if should_block_request(route.request.url) else route.continue_()
        )
        logger.debug("Applied network request blocking rules.")
    except Exception as e:
        logger.warning(f"Failed to set up page optimizations: {e}")

# --- ÏÉÅÏÑ∏ ÌéòÏù¥ÏßÄÏóêÏÑú ÏàòÎüâ-Îã®Í∞Ä ÌÖåÏù¥Î∏î Ï∂îÏ∂ú Ìï®Ïàò Ï∂îÍ∞Ä ---
async def extract_price_table(page, product_url, timeout=30000):
    """
    ÏÉÅÌíà ÏÉÅÏÑ∏ ÌéòÏù¥ÏßÄÏóêÏÑú ÏàòÎüâ-Îã®Í∞Ä ÌÖåÏù¥Î∏îÏùÑ Ï∂îÏ∂úÌï©ÎãàÎã§.
    Î™®Îì† Í∞ÄÏö©Ìïú ÏàòÎüâ-Í∞ÄÍ≤© Ï†ïÎ≥¥Î•º Í∞ÄÏ†∏ÏòµÎãàÎã§.
    
    Args:
        page: Playwright Page Í∞ùÏ≤¥
        product_url: ÏÉÅÌíà ÏÉÅÏÑ∏ ÌéòÏù¥ÏßÄ URL
        timeout: ÌÉÄÏûÑÏïÑÏõÉ(ms)
        
    Returns:
        DataFrame: ÏàòÎüâ-Îã®Í∞Ä Ï†ïÎ≥¥Í∞Ä Îã¥Í∏¥ DataFrame ÎòêÎäî None
    """
    try:
        await page.goto(product_url, wait_until='domcontentloaded', timeout=timeout)
        
        # Í≥†Î†§Í∏∞ÌîÑÌä∏ ÏÇ¨Ïù¥Ìä∏Ïùò Îã§ÏñëÌïú ÌÖåÏù¥Î∏î ÏÑ†ÌÉùÏûê
        table_selectors = [
            "table.quantity_price__table",  # Í≥†Î†§Í∏∞ÌîÑÌä∏ ÏàòÎüâ-Îã®Í∞Ä ÌÖåÏù¥Î∏î
            "div.product_table table",      # Í≥†Î†§Í∏∞ÌîÑÌä∏ ÏÉÅÌíà ÌÖåÏù¥Î∏î
            "table.detail_table",           # ÏùºÎ∞òÏ†ÅÏù∏ ÏÉÅÏÑ∏ ÌÖåÏù¥Î∏î
            "div.detail_price table",       # Í∞ÄÍ≤© Ï†ïÎ≥¥ ÌÖåÏù¥Î∏î
            "div.goods_detail table"        # ÏÉÅÌíà ÏÉÅÏÑ∏ ÌÖåÏù¥Î∏î
        ]
        
        # Í≥†Î†§Í∏∞ÌîÑÌä∏ ÌäπÏú†Ïùò ÌÖåÏù¥Î∏î Íµ¨Ï°∞ Ï≤òÎ¶¨
        kogift_selector = "table.quantity_price__table"
        if await page.locator(kogift_selector).count() > 0:
            # ÏàòÎüâ ÌñâÍ≥º Í∞ÄÍ≤© ÌñâÏù¥ Í∞ÅÍ∞Å Î≥ÑÎèÑ ÌñâÏóê ÏûàÎäî ÌäπÎ≥ÑÌïú Íµ¨Ï°∞ Ï≤òÎ¶¨
            qty_cells = await page.locator(f"{kogift_selector} tr:first-child td").all()
            price_cells = await page.locator(f"{kogift_selector} tr:nth-child(2) td").all()
            
            # Ï≤´ Î≤àÏß∏ Ïó¥ÏùÄ Ìó§ÎçîÏù¥ÎØÄÎ°ú Ï†úÏô∏ (ÏàòÎüâ, Îã®Í∞Ä ÎùºÎäî ÌÖçÏä§Ìä∏Í∞Ä ÏûàÏùå)
            quantities = []
            prices = []
            
            # ÏàòÎüâ Ìñâ Ï∂îÏ∂ú
            for i, cell in enumerate(qty_cells):
                if i > 0:  # Ï≤´ Î≤àÏß∏ Ïó¥(Ìó§Îçî) Í±¥ÎÑàÎõ∞Í∏∞
                    qty_text = await cell.text_content()
                    # ÏàòÎüâÏóêÏÑú ÏâºÌëú Ï†úÍ±∞ÌïòÍ≥† Ïà´ÏûêÎßå Ï∂îÏ∂ú
                    qty_clean = ''.join(filter(str.isdigit, qty_text.replace(',', '')))
                    if qty_clean:
                        quantities.append(int(qty_clean))
            
            # Í∞ÄÍ≤© Ìñâ Ï∂îÏ∂ú
            for i, cell in enumerate(price_cells):
                if i > 0:  # Ï≤´ Î≤àÏß∏ Ïó¥(Ìó§Îçî) Í±¥ÎÑàÎõ∞Í∏∞
                    price_text = await cell.text_content()
                    # Í∞ÄÍ≤©ÏóêÏÑú ÏâºÌëú Ï†úÍ±∞ÌïòÍ≥† Ïà´ÏûêÎßå Ï∂îÏ∂ú
                    price_clean = ''.join(filter(str.isdigit, price_text.replace(',', '')))
                    if price_clean:
                        prices.append(int(price_clean))
            
            # Ïú†Ìö®Ìïú Îç∞Ïù¥ÌÑ∞Í∞Ä ÏûàÎäîÏßÄ ÌôïÏù∏
            if quantities and prices and len(quantities) == len(prices):
                # DataFrame ÏÉùÏÑ±
                result_df = pd.DataFrame({
                    'ÏàòÎüâ': quantities,
                    'Îã®Í∞Ä': prices
                })
                
                # Î∂ÄÍ∞ÄÏÑ∏ Ï†ïÎ≥¥ ÌôïÏù∏
                vat_info = await page.locator("div.quantity_price__wrapper div:last-child").text_content()
                has_vat = "Î∂ÄÍ∞ÄÏÑ∏Î≥ÑÎèÑ" in vat_info or "Î∂ÄÍ∞ÄÏÑ∏ Î≥ÑÎèÑ" in vat_info
                
                # Î∂ÄÍ∞ÄÏÑ∏ Î≥ÑÎèÑÎùºÎ©¥ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Î°ú Ï∂îÍ∞Ä
                if has_vat:
                    result_df.attrs['vat_excluded'] = True
                
                # ÏàòÎüâ Í∏∞Ï§ÄÏúºÎ°ú Ï†ïÎ†¨
                result_df = result_df.sort_values('ÏàòÎüâ')
                return result_df
        
        # Îã§Î•∏ ÏÑ†ÌÉùÏûê ÏãúÎèÑ
        for selector in table_selectors:
            # Ïù¥ÎØ∏ Ï≤òÎ¶¨Ìïú ÏÑ†ÌÉùÏûê Í±¥ÎÑàÎõ∞Í∏∞
            if selector == kogift_selector:
                continue
                
            if await page.locator(selector).count() > 0:
                try:
                    # ÌÖåÏù¥Î∏î HTML Í∞ÄÏ†∏Ïò§Í∏∞
                    table_html = await page.locator(selector).first.inner_html()
                    
                    # ÌÖåÏù¥Î∏îÏùÑ pandas DataFrameÏúºÎ°ú ÌååÏã±
                    tables = pd.read_html("<table>" + table_html + "</table>")
                    if not tables:
                        continue
                    
                    table_df = tables[0]
                    
                    # ÌÖåÏù¥Î∏îÏù¥ ÏàòÎüâ-Îã®Í∞Ä Ï†ïÎ≥¥Ïù∏ÏßÄ ÌôïÏù∏
                    if len(table_df.columns) >= 2:
                        # Ïª¨ÎüºÎ™ÖÏóê 'ÏàòÎüâ', 'Í∞ÄÍ≤©', 'Îã®Í∞Ä' Îì±Ïùò ÌÇ§ÏõåÎìúÍ∞Ä ÏûàÎäîÏßÄ ÌôïÏù∏
                        col_names = [str(col).lower() for col in table_df.columns]
                        qty_keywords = ['ÏàòÎüâ', 'qty', 'Í∞úÏàò', 'Í∞ØÏàò']
                        price_keywords = ['Í∞ÄÍ≤©', 'Îã®Í∞Ä', 'Í∏àÏï°', 'price']
                        
                        qty_col = None
                        price_col = None
                        
                        # ÏàòÎüâ Ïª¨Îüº Ï∞æÍ∏∞
                        for i, col in enumerate(col_names):
                            if any(keyword in col for keyword in qty_keywords):
                                qty_col = i
                                break
                        
                        # Í∞ÄÍ≤© Ïª¨Îüº Ï∞æÍ∏∞
                        for i, col in enumerate(col_names):
                            if any(keyword in col for keyword in price_keywords):
                                price_col = i
                                break
                        
                        # Ïª¨ÎüºÎ™ÖÏóêÏÑú Ï∞æÏßÄ Î™ªÌñàÎã§Î©¥ Ï≤´ Î≤àÏß∏ ÌñâÏóêÏÑú Ï∞æÍ∏∞
                        if qty_col is None and price_col is None and not table_df.empty:
                            first_row = table_df.iloc[0]
                            for i, value in enumerate(first_row):
                                value_str = str(value).lower()
                                if any(keyword in value_str for keyword in qty_keywords):
                                    qty_col = i
                                if any(keyword in value_str for keyword in price_keywords):
                                    price_col = i
                            
                            # Ï≤´ Î≤àÏß∏ ÌñâÏù¥ Ìó§ÎçîÏù∏ Í≤ΩÏö∞ Ï†úÍ±∞
                            if qty_col is not None or price_col is not None:
                                table_df = table_df.iloc[1:]
                        
                        # Í∑∏ÎûòÎèÑ Î™ª Ï∞æÏïòÎã§Î©¥ Ï≤´ Î≤àÏß∏ÏôÄ Îëê Î≤àÏß∏ Ïª¨Îüº ÏÇ¨Ïö©
                        if qty_col is None and price_col is None:
                            qty_col = 0
                            price_col = 1
                        
                        if qty_col is not None and price_col is not None:
                            # ÏàòÎüâ-Í∞ÄÍ≤© ÌÖåÏù¥Î∏î ÌôïÏù∏Îê®
                            result_df = table_df.copy()
                            
                            # Ïª¨ÎüºÎ™Ö Ïû¨ÏßÄÏ†ï
                            new_cols = result_df.columns.tolist()
                            new_cols[qty_col] = 'ÏàòÎüâ'
                            new_cols[price_col] = 'Îã®Í∞Ä'
                            result_df.columns = new_cols
                            
                            # ÌïÑÏöîÌïú Ïª¨ÎüºÎßå ÏÑ†ÌÉù
                            result_df = result_df[['ÏàòÎüâ', 'Îã®Í∞Ä']]
                            
                            # Îç∞Ïù¥ÌÑ∞ Ï†ïÏ†ú
                            result_df['ÏàòÎüâ'] = result_df['ÏàòÎüâ'].astype(str).apply(
                                lambda x: ''.join(filter(str.isdigit, str(x).replace(',', '')))
                            )
                            result_df['Îã®Í∞Ä'] = result_df['Îã®Í∞Ä'].astype(str).apply(
                                lambda x: ''.join(filter(str.isdigit, str(x).replace(',', '')))
                            )
                            
                            # Ïà´ÏûêÎ°ú Î≥ÄÌôò Í∞ÄÎä•Ìïú ÌñâÎßå Ïú†ÏßÄ
                            result_df = result_df[result_df['ÏàòÎüâ'].apply(lambda x: x.isdigit())]
                            result_df = result_df[result_df['Îã®Í∞Ä'].apply(lambda x: x.isdigit())]
                            
                            # Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ Î≥ÄÌôò
                            result_df['ÏàòÎüâ'] = result_df['ÏàòÎüâ'].astype(int)
                            result_df['Îã®Í∞Ä'] = result_df['Îã®Í∞Ä'].astype(int)
                            
                            # ÏàòÎüâ Í∏∞Ï§Ä Ï†ïÎ†¨
                            result_df = result_df.sort_values('ÏàòÎüâ')
                            
                            if not result_df.empty:
                                return result_df
                except Exception as table_error:
                    continue
        
        return None
        
    except Exception as e:
        logger.error(f"ÏàòÎüâ-Í∞ÄÍ≤© ÌÖåÏù¥Î∏î Ï∂îÏ∂ú Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")
        return None

# --- Ïù¥ÎØ∏ÏßÄ URL Ï≤òÎ¶¨ Ï†ÑÏö© Ìï®Ïàò Ï∂îÍ∞Ä ---
def normalize_kogift_image_url(img_url: str, base_url: str = "https://www.kogift.com") -> Tuple[str, bool]:
    """
    Í≥†Î†§Í∏∞ÌîÑÌä∏ Î∞è Ïï†ÎìúÌåêÏ¥â Ïù¥ÎØ∏ÏßÄ URLÏùÑ ÌëúÏ§ÄÌôîÌïòÍ≥† Ïú†Ìö®ÏÑ±ÏùÑ Í≤ÄÏÇ¨Ìï©ÎãàÎã§.
    '/ez/' Í≤ΩÎ°úÎ•º ÌïÑÏöîÌïú Í≤ΩÏö∞ Ï∂îÍ∞ÄÌï©ÎãàÎã§.

    Args:
        img_url: ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ URL ÎòêÎäî Í≤ΩÎ°ú
        base_url: Í∏∞Î≥∏ ÎèÑÎ©îÏù∏ URL

    Returns:
        Tuple[str, bool]: Ï†ïÍ∑úÌôîÎêú Ïù¥ÎØ∏ÏßÄ URLÍ≥º Ïú†Ìö®ÏÑ± Ïó¨Î∂Ä
    """
    if not img_url:
        return "", False

    # data:image URIÏù∏ Í≤ΩÏö∞ (Ïù∏ÎùºÏù∏ Ïù¥ÎØ∏ÏßÄ)
    if img_url.startswith('data:image/'):
        logger.warning(f"Data URI Ïù¥ÎØ∏ÏßÄ Î∞úÍ≤¨ (ÏÇ¨Ïö© Î∂àÍ∞Ä)")
        return "", False

    # ÎåÄÏÉÅ ÎèÑÎ©îÏù∏ Î¶¨Ïä§Ìä∏
    target_domains = ['koreagift.com', 'adpanchok.co.kr']

    # Ïù¥ÎØ∏ ÏôÑÏ†ÑÌïú URLÏù∏ Í≤ΩÏö∞
    if img_url.startswith(('http://', 'https://')):
        parsed_url = urlparse(img_url)
        domain = parsed_url.netloc
        path = parsed_url.path

        # ÎåÄÏÉÅ ÎèÑÎ©îÏù∏Ïù∏ÏßÄ ÌôïÏù∏
        is_target_domain = any(td in domain for td in target_domains)

        if is_target_domain:
            # Ïù¥ÎØ∏ /ez/Í∞Ä ÏûàÎäî Í≤ΩÏö∞ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©
            if '/ez/' in path:
                return img_url, True
            # /upload/Î°ú ÏãúÏûëÌïòÎäî Í≤ΩÎ°úÏóê /ez/ Ï∂îÍ∞Ä
            elif path.startswith('/upload/'):
                new_path = '/ez' + path
                return f"{parsed_url.scheme}://{domain}{new_path}", True
            # Î£®Ìä∏ Í≤ΩÎ°ú Îì± /ez/Í∞Ä ÌïÑÏöî ÏóÜÎäî Í≤ΩÏö∞ (Ïòà: /main/img.jpg)
            elif not path.startswith('/upload/'):
                 # /ez/ Í∞Ä ÏóÜÍ≥†, /upload/ ÎèÑ ÏïÑÎãàÎ©¥ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©
                 return img_url, True
            # Í∑∏ Ïô∏ ÎåÄÏÉÅ ÎèÑÎ©îÏù∏Ïùò Í≤ΩÎ°úÎäî ÏùºÎã® Ïú†Ìö®ÌïòÎã§Í≥† Í∞ÑÏ£º
            else:
                 return img_url, True
        else:
            # ÎåÄÏÉÅ ÎèÑÎ©îÏù∏Ïù¥ ÏïÑÎãàÎ©¥, Ïú†Ìö®Ìïú URL ÌòïÏãùÏù∏ÏßÄ ÌôïÏù∏ ÌõÑ Î∞òÌôò
            if domain and path: # Í∏∞Î≥∏Ï†ÅÏù∏ Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨
                return img_url, True
            else:
                return img_url, False # Ïú†Ìö®ÌïòÏßÄ ÏïäÏùÄ ÌòïÏãù

    # '//' ÏãúÏûëÌïòÎäî ÌîÑÎ°úÌÜ†ÏΩú-ÏÉÅÎåÄ URL Ï≤òÎ¶¨
    if img_url.startswith('//'):
        # // Îã§ÏùåÏù¥ ÎèÑÎ©îÏù∏Ïù¥Ïñ¥Ïïº Ìï®
        temp_url = f"https:{img_url}"
        parsed_temp = urlparse(temp_url)
        if parsed_temp.netloc:
            # Ïû¨Í∑Ä Ìò∏Ï∂úÎ°ú /ez/ Ï≤òÎ¶¨ ÏúÑÏûÑ
            return normalize_kogift_image_url(temp_url, base_url)
        else:
            return "", False # // Îã§ÏùåÏóê ÎèÑÎ©îÏù∏Ïù¥ ÏóÜÎäî ÏûòÎ™ªÎêú ÌòïÏãù

    # './' ÏãúÏûëÌïòÎäî ÏÉÅÎåÄ Í≤ΩÎ°ú Ï≤òÎ¶¨
    if img_url.startswith('./'):
        img_url = img_url[2:]  # './' Ï†úÍ±∞

    # Ï†àÎåÄ Í≤ΩÎ°ú ('/'Î°ú ÏãúÏûë)
    if img_url.startswith('/'):
        # ÎåÄÏÉÅ ÎèÑÎ©îÏù∏Ïù¥Í≥† /upload/Î°ú ÏãúÏûëÌïòÎ©¥ /ez/ Ï∂îÍ∞Ä
        is_target_domain = any(td in base_url for td in target_domains)
        if is_target_domain and img_url.startswith('/upload/'):
            img_url = '/ez' + img_url
        # Í∑∏ Ïô∏ Ï†àÎåÄ Í≤ΩÎ°úÎäî Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©
    # ÏÉÅÎåÄ Í≤ΩÎ°ú (ÌååÏùºÎ™Ö ÎòêÎäî ÌïòÏúÑ Í≤ΩÎ°ú)
    else:
        # ÎåÄÏÉÅ ÎèÑÎ©îÏù∏Ïù¥Í≥† 'upload/'Î°ú ÏãúÏûëÌïòÎ©¥ /ez/ Ï∂îÍ∞Ä
        is_target_domain = any(td in base_url for td in target_domains)
        if is_target_domain and img_url.startswith('upload/'):
            img_url = '/ez/' + img_url
        # Í∑∏ Ïô∏ ÏÉÅÎåÄ Í≤ΩÎ°úÎäî ÏïûÏóê '/' Ï∂îÍ∞Ä
        else:
            img_url = '/' + img_url

    # ÏµúÏ¢Ö URL ÏÉùÏÑ± (urljoin ÏÇ¨Ïö©)
    final_url = urljoin(base_url, img_url)

    # Ï§ëÎ≥µ Í≤ΩÎ°ú ÌôïÏù∏ Î∞è ÏàòÏ†ï ('/ez/ez/' -> '/ez/')
    if '/ez/ez/' in final_url:
        final_url = final_url.replace('/ez/ez/', '/ez/')

    # ÏµúÏ¢Ö URL Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨ (Í∞ÑÎã®Ìûà)
    parsed_final = urlparse(final_url)
    if parsed_final.scheme and parsed_final.netloc:
        return final_url, True
    else:
        logger.warning(f"ÏµúÏ¢Ö URL ÏÉùÏÑ± Ïã§Ìå®: base='{base_url}', img='{img_url}', final='{final_url}'")
        return final_url, False # ÏÉùÏÑ± Ïã§Ìå®

async def verify_kogift_images(product_list: List[Dict], sample_percent: int = 10) -> List[Dict]:
    """Í≥†Î†§Í∏∞ÌîÑÌä∏ ÏÉÅÌíà Î™©Î°ùÏùò Ïù¥ÎØ∏ÏßÄ URLÏùÑ Í≤ÄÏ¶ùÌïòÍ≥† ÌëúÏ§ÄÌôîÌïú ÌõÑ, Ïù¥ÎØ∏ÏßÄÎ•º Îã§Ïö¥Î°úÎìúÌï©ÎãàÎã§."""
    if not product_list:
        return []
    
    # ÏÑ§Ï†ïÏóêÏÑú Í≤ÄÏ¶ù Ïó¨Î∂Ä ÌôïÏù∏
    config = configparser.ConfigParser()
    config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'config.ini')
    config.read(config_path, encoding='utf-8')
    
    verify_enabled = config.getboolean('Matching', 'verify_image_urls', fallback=True)
    download_enabled = config.getboolean('Matching', 'download_images', fallback=True)
    
    # Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû• Í≤ΩÎ°ú ÏÑ§Ï†ï (Main ÎîîÎ†âÌÜ†Î¶¨Î°ú Î≥ÄÍ≤Ω)
    base_image_dir = config.get('Paths', 'image_main_dir', fallback='C:\\RPA\\Image\\Main') # Changed from image_target_dir
    images_dir = os.path.join(base_image_dir, 'kogift')  # kogift ÌïòÏúÑ ÎîîÎ†âÌÜ†Î¶¨ ÏÇ¨Ïö©
    os.makedirs(images_dir, exist_ok=True)
    
    logger.info(f"Í≥†Î†§Í∏∞ÌîÑÌä∏ ÏÉÅÌíà {len(product_list)}Í∞úÏùò Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ ÏãúÏûë (Ï†ÄÏû• Í≤ΩÎ°ú: {images_dir})")
    
    # Ïù¥ÎØ∏ÏßÄ URL ÌëúÏ§ÄÌôî
    for product in product_list:
        # 'image' ÎòêÎäî 'image_url' ÌÇ§ÏóêÏÑú Ïù¥ÎØ∏ÏßÄ URL Ï∞æÍ∏∞
        img_url = product.get('image') or product.get('image_url') or product.get('src')
        if img_url:
            product['original_image'] = img_url  # ÏõêÎ≥∏ URL Î∞±ÏóÖ
            
            # URL ÌëúÏ§ÄÌôî
            normalized_url, is_valid = normalize_kogift_image_url(img_url)
            
            if normalized_url:
                # ÌëúÏ§ÄÌôîÎêú URL Ï†ÄÏû•
                product['image'] = normalized_url
                product['image_url'] = normalized_url  # Ìò∏ÌôòÏÑ± Ïú†ÏßÄ
                product['src'] = normalized_url  # Ìò∏ÌôòÏÑ± Ïú†ÏßÄ
            else:
                # Ïú†Ìö®ÌïòÏßÄ ÏïäÏùÄ URLÏùÄ Îπà Î¨∏ÏûêÏó¥Î°ú ÌëúÏãú
                product['image'] = ""
                product['image_url'] = ""
                product['src'] = ""
    
    # Ïù¥ÎØ∏ÏßÄ Îã§Ïö¥Î°úÎìú Ï≤òÎ¶¨
    if download_enabled:
        # Ïú†Ìö®Ìïú Ïù¥ÎØ∏ÏßÄ URLÎßå ÏàòÏßë
        valid_urls = []
        url_to_product_map = {}
        
        for product in product_list:
            img_url = product.get('image')
            if img_url:
                valid_urls.append(img_url)
                url_to_product_map[img_url] = product
        
        logger.info(f"Ï¥ù {len(valid_urls)}Í∞ú Ïù¥ÎØ∏ÏßÄ Îã§Ïö¥Î°úÎìú ÏãúÏûë")
        
        # Ïù¥ÎØ∏ÏßÄ ÏùºÍ¥Ñ Îã§Ïö¥Î°úÎìú
        downloaded_images = download_images_batch(valid_urls, save_dir=images_dir)
        
        # Îã§Ïö¥Î°úÎìúÎêú Ïù¥ÎØ∏ÏßÄ Í≤ΩÎ°úÎ•º Ï†úÌíà Îç∞Ïù¥ÌÑ∞Ïóê Ï∂îÍ∞Ä
        for url, local_path in downloaded_images.items():
            if url in url_to_product_map:
                url_to_product_map[url]['local_image_path'] = local_path
        
        logger.info(f"Ïù¥ÎØ∏ÏßÄ Îã§Ïö¥Î°úÎìú ÏôÑÎ£å: {len(downloaded_images)}/{len(valid_urls)} ÏÑ±Í≥µ")
    
    # ÏÉòÌîåÎßÅ ÎπÑÏú®Ïóê Îî∞Îùº URL Í≤ÄÏ¶ù (Í∏∞Ï°¥ ÏΩîÎìúÎäî Ï£ºÏÑù Ï≤òÎ¶¨)
    if verify_enabled and sample_percent > 0 and not download_enabled:
        # Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏûàÎäî ÏÉÅÌíàÎßå ÏÑ†ÌÉù
        products_with_images = [p for p in product_list if p.get('image')]
        if not products_with_images:
            return product_list
            
        # Í≤ÄÏ¶ùÌï† ÏÉÅÌíà ÏÉòÌîåÎßÅ
        sample_size = max(1, int(len(products_with_images) * sample_percent / 100))
        sample_products = random.sample(products_with_images, min(sample_size, len(products_with_images)))
        
        logger.info(f"{sample_percent}% ÏÉòÌîåÎßÅÏúºÎ°ú {len(sample_products)}Í∞ú Ïù¥ÎØ∏ÏßÄ URL Í≤ÄÏ¶ù ÏãúÏûë")
        
        # Í≤ÄÏ¶ù Í≤∞Í≥º Ïπ¥Ïö¥ÌåÖ
        verified_count = 0
        failed_count = 0
        
        # ÎπÑÎèôÍ∏∞ ÏÑ∏ÏÖò ÏÉùÏÑ±
        async with aiohttp.ClientSession() as session:
            for product in sample_products:
                img_url = product['image']
                if not img_url:
                    continue
                
                # Ïù¥ÎØ∏ÏßÄ URL Ïã§Ï†ú Ï†ëÍ∑º Í≤ÄÏ¶ù
                url, is_valid, reason = await verify_image_url(session, img_url)
                
                if is_valid:
                    verified_count += 1
                else:
                    failed_count += 1
                    # koreagift.com Ïã§Ìå® URL Ï≤òÎ¶¨
                    if 'koreagift.com' in img_url and is_valid == False:
                        # URLÏùÑ Í≥†Ï≥êÎèÑ Ïã§Ìå®Ìï† Í∞ÄÎä•ÏÑ±Ïù¥ ÎÜíÏúºÎØÄÎ°ú Ï≤òÎ¶¨ÌïòÏßÄ ÏïäÏùå
                        pass
        
        logger.info(f"Ïù¥ÎØ∏ÏßÄ URL Í≤ÄÏ¶ù Í≤∞Í≥º: ÏÑ±Í≥µ {verified_count}, Ïã§Ìå® {failed_count}")
    
    return product_list

# --- ÏÉÅÏÑ∏ ÌéòÏù¥ÏßÄÏóêÏÑú ÏàòÎüâ ÏÑ§Ï†ïÌïòÍ≥† Í∞ÄÍ≤© Í∞ÄÏ†∏Ïò§Îäî Ìï®Ïàò Ï∂îÍ∞Ä ---
async def get_price_for_specific_quantity(page, product_url, target_quantity, timeout=30000):
    """
    ÏÉÅÌíà ÏÉÅÏÑ∏ ÌéòÏù¥ÏßÄÏóêÏÑú ÌäπÏ†ï ÏàòÎüâÏùÑ ÏûÖÎ†•ÌïòÍ≥† ÏóÖÎç∞Ïù¥Ìä∏Îêú Í∞ÄÍ≤©ÏùÑ Í∞ÄÏ†∏ÏòµÎãàÎã§.
    Í∏∞Î≥∏ÏàòÎüâ ÎØ∏Îßå Í≤ΩÍ≥† Î©îÏãúÏßÄÎèÑ Í∞êÏßÄÌï©ÎãàÎã§.
    
    Args:
        page: Playwright Page Í∞ùÏ≤¥
        product_url: ÏÉÅÌíà ÏÉÅÏÑ∏ ÌéòÏù¥ÏßÄ URL
        target_quantity: ÏÑ§Ï†ïÌï† ÏàòÎüâ (int)
        timeout: ÌÉÄÏûÑÏïÑÏõÉ(ms)
        
    Returns:
        dict: ÏàòÎüâ, Îã®Í∞Ä(Î∂ÄÍ∞ÄÏÑ∏ Ìè¨Ìï®/ÎØ∏Ìè¨Ìï®), ÏÑ±Í≥µ Ïó¨Î∂Ä, ÏµúÏÜå ÏàòÎüâ ÏïàÎÇ¥
    """
    result = {
        "quantity": target_quantity,
        "price": 0,
        "price_with_vat": 0,
        "success": False,
        "min_quantity_error": False,
        "min_quantity": None
    }
    
    try:
        # Navigate to the product page
        await page.goto(product_url, wait_until='domcontentloaded', timeout=timeout)
        
        # Wait for a short period for any initial scripts to run
        await page.wait_for_timeout(1000)
        
        # Try different selectors for the quantity input field
        input_selectors = [
            'input#buynum',            # Standard buynum
            'input[name="buynum"]',    # By name
            'input.buynum',            # By class
            'input[id^="buy"]',        # Starts with buy
            'input[id*="num"]',        # Contains num
            'input.btn_count'          # Common class for counter inputs
        ]
        
        # Try to find quantity input field with different selectors
        buynum_input = None
        for selector in input_selectors:
            if await page.locator(selector).count() > 0:
                buynum_input = page.locator(selector).first
                logger.info(f"Found quantity input with selector: {selector}")
                break
        
        if not buynum_input or await buynum_input.count() == 0:
            logger.warning(f"ÏàòÎüâ ÏûÖÎ†• ÌïÑÎìúÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {product_url}")
            return result
            
        # Get current quantity value to check if we need to change it
        current_value_text = await buynum_input.input_value()
        try:
            current_value = int(current_value_text)
        except ValueError:
            current_value = 0
            
        # Only change if quantity is different
        if current_value != target_quantity:
            # Focus on the input and clear existing value
            await buynum_input.click()
            await buynum_input.press("Control+a")
            await buynum_input.press("Delete")
            
            # Input the new quantity
            await buynum_input.fill(str(target_quantity))
            
            # Different ways to trigger price update
            # 1. Press Enter
            await buynum_input.press("Enter")
            
            # 2. Click outside the input
            try:
                await page.click('body', position={'x': 10, 'y': 10})
            except:
                pass
                
            # 3. Look for and click update buttons
            update_button_selectors = [
                'button.update_price',
                'button.btn_update',
                'a.update_price',
                'a.btn_update',
                'button:has-text("Í≥ÑÏÇ∞")',
                'button:has-text("Ï†ÅÏö©")',
                'button:has-text("Î≥ÄÍ≤Ω")'
            ]
            
            for btn_selector in update_button_selectors:
                try:
                    if await page.locator(btn_selector).count() > 0:
                        await page.locator(btn_selector).first.click()
                        logger.info(f"Clicked update button: {btn_selector}")
                        break
                except:
                    continue
            
            # Wait for price to update
            await page.wait_for_timeout(2000)  # Increased wait time
        
        # Í∏∞Î≥∏ÏàòÎüâ ÎØ∏Îßå Í≤ΩÍ≥† Î©îÏãúÏßÄ ÌôïÏù∏
        min_quantity_error_selectors = [
            'div.alert:has-text("Í∏∞Î≥∏ÏàòÎüâ ÎØ∏Îßå")',
            'div.notice:has-text("Í∏∞Î≥∏ÏàòÎüâ ÎØ∏Îßå")',
            'div.quantity_error:has-text("Í∏∞Î≥∏ÏàòÎüâ")',
            'div.alert:has-text("ÏµúÏÜå Ï£ºÎ¨∏")',
            'span.alert:has-text("ÏµúÏÜå Ï£ºÎ¨∏")',
            'div.notice:has-text("ÏµúÏÜå Ï£ºÎ¨∏")',
            'p.alert:has-text("Í∏∞Î≥∏ÏàòÎüâ")'
        ]
        
        # Í≤ΩÍ≥† Î©îÏãúÏßÄ ÌôïÏù∏
        for error_selector in min_quantity_error_selectors:
            if await page.locator(error_selector).count() > 0:
                error_text = await page.locator(error_selector).text_content()
                logger.info(f"ÏµúÏÜå ÏàòÎüâ Í≤ΩÍ≥† Î©îÏãúÏßÄ Î∞úÍ≤¨: {error_text}")
                result["min_quantity_error"] = True
                
                # ÏµúÏÜå ÏàòÎüâ Í∞í Ï∂îÏ∂ú ÏãúÎèÑ
                try:
                    # Í≤ΩÍ≥† Î©îÏãúÏßÄÏóêÏÑú Ïà´Ïûê Ï∂îÏ∂ú (Ïòà: "Í∏∞Î≥∏ÏàòÎüâÏùÄ 100Í∞ú Ïù¥ÏÉÅÏûÖÎãàÎã§")
                    min_qty_match = re.search(r'(\d+)(?:Í∞ú|EA|ea|pcs)', error_text)
                    if min_qty_match:
                        result["min_quantity"] = int(min_qty_match.group(1))
                        logger.info(f"ÏµúÏÜå Ï£ºÎ¨∏ ÏàòÎüâ: {result['min_quantity']}Í∞ú")
                except Exception as ex:
                    logger.warning(f"ÏµúÏÜå ÏàòÎüâ Ï∂îÏ∂ú Ïã§Ìå®: {ex}")
                break
        
        # ÏµúÏÜå ÏàòÎüâ ÌôïÏù∏ÏùÑ ÏúÑÌïú Ïù∏Ìíã ÌïÑÎìúÏùò min ÏÜçÏÑ± ÌôïÏù∏
        if not result["min_quantity"] and buynum_input:
            try:
                min_attr = await buynum_input.get_attribute('min')
                if min_attr and min_attr.isdigit():
                    result["min_quantity"] = int(min_attr)
                    logger.info(f"ÏûÖÎ†• ÌïÑÎìúÏùò ÏµúÏÜå ÏàòÎüâ: {result['min_quantity']}Í∞ú")
            except Exception:
                pass
        
        # Price selectors to try
        price_selectors = [
            'span#main_price',         # Standard kogift price
            'span.main_price',         # By class
            'strong.price',            # Common price element
            'div.price_wrap .price',   # Price in price wrapper
            'div.price_value',         # Price value
            'span.font_price',         # Price with special font
            '*[id*="price"]:not(input):not(select):not(button)', # Contains price but not form elements
            'div.total_price'          # Total price
        ]
        
        # Try different price selectors
        price_element = None
        for selector in price_selectors:
            if await page.locator(selector).count() > 0:
                price_element = page.locator(selector).first
                logger.info(f"Found price element with selector: {selector}")
                break
                
        if not price_element or await price_element.count() == 0:
            logger.warning(f"Í∞ÄÍ≤© ÏöîÏÜåÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {product_url}")
            
            # Last resort - try to get any visible price text
            try:
                page_content = await page.content()
                # Look for price patterns in text
                price_matches = re.findall(r'[\d,]+Ïõê', page_content)
                if price_matches:
                    price_text = price_matches[0]
                    logger.info(f"Found price using regex pattern: {price_text}")
                else:
                    return result
            except:
                return result
        else:
            # Get price text from element
            price_text = await price_element.text_content()
        
        # Clean and extract price value
        # Remove non-digit characters except commas, then remove commas
        price_clean = ''.join(filter(lambda c: c.isdigit() or c == ',', price_text)).replace(',', '')
        if not price_clean:
            logger.warning(f"Ïú†Ìö®Ìïú Í∞ÄÍ≤©ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {product_url} (ÌÖçÏä§Ìä∏: {price_text})")
            return result
            
        # Convert to integer
        try:
            price = int(price_clean)
        except ValueError:
            logger.warning(f"Í∞ÄÍ≤©ÏùÑ Ï†ïÏàòÎ°ú Î≥ÄÌôòÌï† Ïàò ÏóÜÏäµÎãàÎã§: {price_clean}")
            return result
        
        # Calculate price with VAT (10%)
        price_with_vat = round(price * 1.1)

        # Also check if price is per-unit
        per_unit_selectors = [
            'span:has-text("Îã®Í∞Ä")',
            'span:has-text("Í∞úÎãπ")',
            'span:has-text("EAÎãπ")',
            'span:has-text("Îã®ÏúÑÍ∞ÄÍ≤©")'
        ]
        
        is_per_unit = False
        for selector in per_unit_selectors:
            if await page.locator(selector).count() > 0:
                is_per_unit = True
                break
                
        result["price"] = price
        result["price_with_vat"] = price_with_vat
        result["is_per_unit"] = is_per_unit
        result["success"] = True

        return result
        
    except Exception as e:
        logger.error(f"ÏàòÎüâ ÏÑ§Ï†ï Î∞è Í∞ÄÍ≤© Ï°∞Ìöå Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")
        return result

# --- Main scraping functionÏóê ÏÉÅÏÑ∏ ÌéòÏù¥ÏßÄ ÌÅ¨Î°§ÎßÅ Î°úÏßÅ Ï∂îÍ∞Ä --- 
async def scrape_data(browser: Browser, original_keyword1: str, original_keyword2: Optional[str] = None, config: configparser.ConfigParser = None, fetch_price_tables: bool = False, custom_quantities: List[int] = None):
    """Scrape data from Kogift website."""
    
    # Initialize variables
    results = []
    kogift_urls = get_kogift_urls(config)
    max_items_per_variation = get_max_items_per_variation(config)
    
    # Generate keyword variations
    keyword_variations = generate_keyword_variations(original_keyword1, original_keyword2)
    logger.info(f"Generated {len(keyword_variations)} keyword variations for search: {keyword_variations}")
    logger.info(f"Will scrape up to {max_items_per_variation} items per keyword variation")
    
    # Check if we need to recreate the browser
    need_new_browser = not browser or not browser.is_connected()
    
    # Get quantities to check - use input quantities or fallback to defaults
    if custom_quantities is None or len(custom_quantities) == 0:
        # Try to get quantities from input Excel if config is provided
        if config and config.has_section('Input'):
            try:
                input_file = config.get('Input', 'input_file')
                df = pd.read_excel(input_file)
                if 'Í∏∞Î≥∏ÏàòÎüâ(1)' in df.columns:
                    custom_quantities = df['Í∏∞Î≥∏ÏàòÎüâ(1)'].dropna().unique().tolist()
                    custom_quantities = [int(qty) for qty in custom_quantities if str(qty).isdigit()]
                    logger.info(f"Using quantities from input Excel: {custom_quantities}")
            except Exception as e:
                logger.warning(f"Could not get quantities from input Excel: {e}")
                
        # If still no quantities, use defaults
        if not custom_quantities:
            custom_quantities = [300, 800, 1100, 2000]
            logger.info("Using default quantities")
    
    logger.info(f"Will check prices for quantities: {custom_quantities}")
    
    # Continue with the rest of the original function implementation
    if config is None:
        logger.error("Configuration object is required")
        return pd.DataFrame()

    # Get image directory from config
    try:
        images_dir = config.get('Paths', 'image_main_dir')
        if not images_dir or not os.path.exists(images_dir):
            logger.error("Invalid image_main_dir in config")
            return pd.DataFrame()
    except Exception as e:
        logger.error(f"Error getting image directory from config: {e}")
        return pd.DataFrame()
    
    all_results = []
    seen_product_urls = set()  # Track product URLs to avoid duplicates
    
    # Try each URL in sequence
    for base_url in kogift_urls:
        context = None
        page = None
        try:
            # --- Add check for browser connection and reconnect if needed --- 
            if not browser or not browser.is_connected():
                logger.warning(f"üî∂ Browser is not connected before processing URL: {base_url}. Attempting to reconnect.")
                
                # If the caller provided a disconnected browser, we'll try to create a new one
                if need_new_browser:
                    from playwright.async_api import async_playwright
                    p = await async_playwright().start()
                    
                    # Get browser launch arguments from config
                    browser_args = []
                    try:
                        browser_args_str = config.get('Playwright', 'playwright_browser_args', fallback='[]')
                        import json
                        browser_args = json.loads(browser_args_str)
                    except Exception as arg_err:
                        logger.warning(f"Could not parse browser arguments: {arg_err}. Using defaults.")
                        browser_args = ["--disable-gpu", "--disable-dev-shm-usage", "--no-sandbox"]
                    
                    # Launch a new browser
                    try:
                        headless = config.getboolean('Playwright', 'playwright_headless', fallback=True)
                        browser = await p.chromium.launch(
                            headless=headless,
                            args=browser_args,
                            timeout=60000  # 1 minute timeout for browser launch
                        )
                        logger.info("üü¢ Successfully launched a new browser instance")
                    except Exception as launch_err:
                        logger.error(f"Failed to launch new browser: {launch_err}")
                        return pd.DataFrame()
                else:
                    # Skip this URL if we couldn't reconnect
                    logger.error(f"üî¥ Browser is not connected and cannot be recreated for {base_url}. Skipping this URL.")
                    continue
            # --- End check ---
            
            # Create a new context for each URL
            logger.debug(f"Attempting to create new browser context for {base_url}")
            
            # Apply delay before creating a new context if configured
            context_delay = config.getint('Playwright', 'playwright_new_context_delay_ms', fallback=0)
            if context_delay > 0:
                await asyncio.sleep(context_delay / 1000)  # Convert ms to seconds
                
            context = await browser.new_context(
                user_agent=config.get('Network', 'user_agent', fallback='Mozilla/5.0 ...'),
                viewport={'width': 1920, 'height': 1080},
            )
            logger.debug(f"Successfully created context for {base_url}")
            
            # Create a new page
            page = await context.new_page()
            page.set_default_timeout(config.getint('Playwright', 'playwright_default_timeout_ms', fallback=120000))
            page.set_default_navigation_timeout(config.getint('Playwright', 'playwright_navigation_timeout_ms', fallback=60000))

            # Setup resource blocking if enabled
            if config.getboolean('Playwright', 'playwright_block_resources', fallback=True):
                await setup_page_optimizations(page)
            
            # Search with each keyword variation for this URL
            for keyword_index, keyword in enumerate(keyword_variations):
                try:
                    logger.info(f"Attempting to search with variation {keyword_index+1}/{len(keyword_variations)}: '{keyword}' on {base_url}")
                    
                    # Construct search URL
                    search_url = f"{base_url.strip()}/goods/goods_search.php"

                    try:
                        # Navigate to the search page with increased timeout and retry logic
                        for nav_attempt in range(3):  # Add retry logic
                            try:
                                await page.goto(search_url, wait_until="domcontentloaded", 
                                               timeout=config.getint('ScraperSettings', 'navigation_timeout', fallback=90000))
                                # Short pause after navigation to allow page to stabilize
                                await page.wait_for_timeout(3000)
                                break  # Break out of retry loop if successful
                            except PlaywrightError as nav_err:
                                if nav_attempt < 2:  # Try again if we haven't reached max retries
                                    logger.warning(f"Navigation error (attempt {nav_attempt+1}/3): {nav_err}")
                                    await asyncio.sleep(2)  # Wait before retry
                                else:
                                    raise  # Re-raise on final attempt

                        # --- Perform Search --- 
                        search_input_locator = page.locator('input#main_keyword[name="keyword"]')
                        search_button_locator = page.locator('img#search_submit')
                        
                        await search_input_locator.wait_for(state="visible", 
                                                           timeout=config.getint('ScraperSettings', 'action_timeout', fallback=15000))
                        
                        # Clear any default value in the search input
                        await search_input_locator.click()
                        await search_input_locator.press("Control+a")
                        await search_input_locator.press("Delete")
                        
                        # Fill the search input with the current keyword variation
                        await search_input_locator.fill(keyword)
                        await search_button_locator.wait_for(state="visible", 
                                                           timeout=config.getint('ScraperSettings', 'action_timeout', fallback=15000))
                        
                        logger.debug(f"üîç Clicking search for variation '{keyword}'...")
                        await search_button_locator.click()
                        logger.info(f"üîç Search submitted for: '{keyword}' on {base_url}")

                        # --- Wait for results OR "no results" message --- 
                        results_container_selector = 'div.product_lists'
                        no_results_selector = 'div.not_result span.icon_dot2:has-text("Í≤ÄÏÉâ Í≤∞Í≥ºÍ∞Ä ÏóÜÏäµÎãàÎã§")'
                        combined_selector = f"{results_container_selector}, {no_results_selector}"
                        
                        logger.debug(f"‚è≥ Waiting for search results or 'no results' message...")
                        try:
                            found_element = await page.wait_for_selector(
                                combined_selector, 
                                state='visible', 
                                timeout=config.getint('ScraperSettings', 'action_timeout', fallback=15000)
                            )
                            
                            # Check if the 'no results' text is visible
                            no_results_element = page.locator(no_results_selector)
                            if await no_results_element.is_visible():
                                no_results_text = await no_results_element.text_content(timeout=1000) or "[No text found]"
                                logger.info(f"‚ö†Ô∏è 'No results' message found for keyword '{keyword}' on {base_url}. Text: {no_results_text.strip()}")
                                continue # Skip to the next keyword variation
                            else:
                                logger.debug("‚úÖ Results container found. Proceeding to scrape.")
                                
                        except PlaywrightError as wait_error:
                            logger.warning(f"‚ö†Ô∏è Timeout or error waiting for results/no_results for keyword '{keyword}' on {base_url}: {wait_error}")
                            continue # Skip to the next keyword variation

                        # --- Scrape Results Pages --- 
                        page_number = 1
                        processed_items = 0
                        product_item_selector = 'div.product'
                        data = []

                        # Limit pages to scrape from config
                        max_pages = config.getint('ScraperSettings', 'kogift_max_pages', fallback=5)

                        while processed_items < max_items_per_variation and page_number <= max_pages:
                            try:
                                logger.info(f"üìÑ Scraping page {page_number} (Keyword: '{keyword}', URL: {base_url})... Items processed: {processed_items}")
                                
                                # Wait for at least one product item to be potentially visible
                                await page.locator(product_item_selector).first.wait_for(state="attached", 
                                                 timeout=config.getint('ScraperSettings', 'action_timeout', fallback=15000))
                                
                                # Short pause to ensure page is fully loaded
                                await page.wait_for_timeout(1000)
                                
                                rows = page.locator(product_item_selector)
                                count = await rows.count()
                                logger.debug(f"üìä Found {count} product elements on page {page_number}.")

                                if count == 0 and page_number > 1:
                                    logger.info(f"‚ö†Ô∏è No product elements found on page {page_number}. Stopping pagination.")
                                    break
                                elif count == 0 and page_number == 1:
                                    logger.info(f"‚ö†Ô∏è No product elements found on first page (page {page_number}). Stopping scrape for this keyword.")
                                    break

                                items_on_page = []
                                for i in range(count):
                                    if processed_items >= max_items_per_variation:
                                        break
                                    try:
                                        row = rows.nth(i)
                                        
                                        # Check for "ÌíàÏ†à" (Sold Out) before processing
                                        try:
                                            item_text = await row.text_content(timeout=2000)
                                            if item_text and "ÌíàÏ†à" in item_text:
                                                logger.info(f"Skipping item {i} as it appears to be sold out (ÌíàÏ†à).")
                                                continue # Skip this item
                                        except Exception as sold_out_check_err:
                                            logger.warning(f"Could not check for 'ÌíàÏ†à' on item {i}: {sold_out_check_err}")
                                            # Optionally continue processing or skip, depending on desired behavior
                                            # continue 
                                            
                                        item_data = {}
                                        
                                        # Extract data using locators with short timeouts
                                        try:
                                            img_locator = row.locator('div.pic > a > img')
                                            img_src = await img_locator.get_attribute('src', timeout=5000)
                                        except Exception as e:
                                            logger.debug(f"Error getting image source: {e}")
                                            img_src = None
                                        
                                        try:
                                            link_locator = row.locator('div.pic > a')
                                            a_href = await link_locator.get_attribute('href', timeout=5000)
                                        except Exception as e:
                                            logger.debug(f"Error getting link: {e}")
                                            a_href = None
                                        
                                        try:
                                            name_locator = row.locator('div.name > a')
                                            name = await name_locator.text_content(timeout=5000)
                                        except Exception as e:
                                            logger.debug(f"Error getting name: {e}")
                                            name = None
                                        
                                        try:
                                            price_locator = row.locator('div.price')
                                            price_text = await price_locator.text_content(timeout=5000)
                                        except Exception as e:
                                            logger.debug(f"Error getting price: {e}")
                                            price_text = None

                                        # Skip item if we couldn't get essential data
                                        if not a_href or not name:
                                            logger.debug(f"Skipping item due to missing essential data")
                                            continue

                                        # Process extracted data
                                        base_domain_url = f"{urlparse(base_url).scheme}://{urlparse(base_url).netloc}"
                                        
                                        # Ïù¥ÎØ∏ÏßÄ URL Ï†ïÍ∑úÌôî
                                        final_img_url, valid_img_url = normalize_kogift_image_url(img_src, base_domain_url) if img_src else ("", False)
                                        if not valid_img_url:
                                            logger.warning(f"‚ö†Ô∏è Invalid or unnormalizable image URL skipped: {img_src}")
                                        
                                        # ÏÉÅÌíà URL Ï≤òÎ¶¨
                                        if a_href:
                                            if a_href.startswith('http'):
                                                final_href_url = a_href
                                            elif a_href.startswith('./'):
                                                processed_href = '/' + a_href[2:]
                                                final_href_url = urljoin(base_domain_url, processed_href)
                                            elif a_href.startswith('/'):
                                                final_href_url = urljoin(base_domain_url, a_href)
                                            else:
                                                final_href_url = urljoin(base_domain_url, '/' + a_href)
                                        else:
                                            final_href_url = ""

                                        # Check for duplicates
                                        if final_href_url and final_href_url in seen_product_urls:
                                            logger.debug(f"Skipping duplicate product URL: {final_href_url}")
                                            continue

                                        if final_href_url:
                                            seen_product_urls.add(final_href_url)

                                        # ÎèÑÎ©îÏù∏ÏóêÏÑú Í≥µÍ∏âÏÇ¨ Ï†ïÎ≥¥ Ï∂îÏ∂ú
                                        supplier = urlparse(base_url).netloc.split('.')[0]
                                        if supplier == 'koreagift':
                                            supplier = 'Í≥†Î†§Í∏∞ÌîÑÌä∏'
                                        elif supplier == 'adpanchok':
                                            supplier = 'Ïï†ÎìúÌåêÏ¥â'
                                        
                                        # Ïú†Ìö®Ìïú Ïù¥ÎØ∏ÏßÄ URLÎßå Ï†ÄÏû•
                                        if valid_img_url:
                                            item_data['image_path'] = final_img_url
                                            item_data['image_url'] = final_img_url
                                            item_data['src'] = final_img_url
                                        else:
                                            item_data['image_path'] = None
                                            item_data['image_url'] = None
                                            item_data['src'] = None
                                        
                                        item_data['href'] = final_href_url
                                        item_data['link'] = final_href_url
                                        item_data['name'] = name.strip() if name else ""
                                        item_data['supplier'] = supplier
                                        item_data['search_keyword'] = keyword
                                        
                                        # Í∞ÄÍ≤© Ï†ïÎ≥¥ Ï≤òÎ¶¨ (Î™©Î°ù ÌéòÏù¥ÏßÄÏóê ÌëúÏãúÎêú Í∏∞Î≥∏ Í∞ÄÍ≤© - ÌäπÏ†ï ÏàòÎüâÏóê ÎåÄÌïú Í∞ÄÍ≤©Ïù¥ ÏïÑÎãò)
                                        price_cleaned = re.sub(r'[^\d.]', '', price_text) if price_text else ""
                                        try:
                                            price_value = float(price_cleaned) if price_cleaned else 0.0
                                        except ValueError:
                                            price_value = 0.0
                                        
                                        item_data['list_price'] = price_value
                                        item_data['list_price_with_vat'] = round(price_value * 1.1)
                                        
                                        # ÏÉÅÌíà ÏÉÅÏÑ∏ ÌéòÏù¥ÏßÄÏóêÏÑú ÏàòÎüâÎ≥Ñ Í∞ÄÍ≤© Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞
                                        quantity_prices = {}
                                        price_detail_context = await browser.new_context(
                                            user_agent=config.get('Network', 'user_agent', fallback='Mozilla/5.0 ...'),
                                            viewport={'width': 1920, 'height': 1080},
                                        )
                                        price_detail_page = await price_detail_context.new_page()
                                        
                                        # Í∞Å ÏàòÎüâÏóê ÎåÄÌïú Í∞ÄÍ≤© Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞
                                        logger.info(f"Fetching prices for {len(custom_quantities)} quantities for product: {item_data['name']}")
                                        
                                        # Î®ºÏ†Ä ÏàòÎüâ-Í∞ÄÍ≤© ÌÖåÏù¥Î∏î Í∞ÄÏ†∏Ïò§Í∏∞ ÏãúÎèÑ
                                        price_table = None
                                        if fetch_price_tables:
                                            price_table = await extract_price_table(price_detail_page, final_href_url, timeout=20000)
                                        
                                        # ÌÖåÏù¥Î∏îÏù¥ ÏûàÏúºÎ©¥ ÌÖåÏù¥Î∏îÏóêÏÑú Í∞ÄÍ≤© Ï†ïÎ≥¥ Ï∂îÏ∂ú
                                        if price_table is not None and not price_table.empty:
                                            logger.info(f"Using price table for {item_data['name']}, table has {len(price_table)} rows")
                                            
                                            # ÌÖåÏù¥Î∏îÏóêÏÑú ÏµúÏÜå ÏàòÎüâ ÌôïÏù∏
                                            min_table_quantity = price_table['ÏàòÎüâ'].min()
                                            logger.info(f"ÌÖåÏù¥Î∏î ÏµúÏÜå ÏàòÎüâ: {min_table_quantity}Í∞ú")
                                            
                                            for qty in custom_quantities:
                                                # Ï£ºÎ¨∏ ÏàòÎüâÏù¥ ÌÖåÏù¥Î∏îÏùò ÏµúÏÜå ÏàòÎüâÎ≥¥Îã§ ÏûëÏùÄ Í≤ΩÏö∞
                                                if qty < min_table_quantity:
                                                    logger.info(f"Ï£ºÎ¨∏ ÏàòÎüâ({qty})Ïù¥ ÏµúÏÜå ÏàòÎüâ({min_table_quantity})Î≥¥Îã§ ÏûëÏäµÎãàÎã§. ÏµúÏÜå ÏàòÎüâÏùò Í∞ÄÍ≤©ÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.")
                                                    # ÌÖåÏù¥Î∏îÏùò ÏµúÏÜå ÏàòÎüâÏóê Ìï¥ÎãπÌïòÎäî Í∞ÄÍ≤© Ï†ïÎ≥¥ ÏÇ¨Ïö©
                                                    min_qty_row = price_table[price_table['ÏàòÎüâ'] == min_table_quantity]
                                                    if not min_qty_row.empty:
                                                        min_qty_price = min_qty_row['Îã®Í∞Ä'].values[0]
                                                        quantity_prices[qty] = {
                                                            'price': min_qty_price,
                                                            'price_with_vat': round(min_qty_price * 1.1),
                                                            'exact_match': False,
                                                            'actual_quantity': min_table_quantity,
                                                            'note': f"ÏµúÏÜå Ï£ºÎ¨∏ ÏàòÎüâ({min_table_quantity}) Í∞ÄÍ≤© Ï†ÅÏö©"
                                                        }
                                                        continue
                                                
                                                # Ï£ºÎ¨∏ ÏàòÎüâÏù¥ ÌÖåÏù¥Î∏î Î≤îÏúÑ ÎÇ¥Ïóê ÏûàÎäî Í≤ΩÏö∞ Ï†ÅÏ†àÌïú Í∞ÄÍ≤© ÏÑ†ÌÉù
                                                # Î®ºÏ†Ä Ï†ïÌôïÌûà ÏùºÏπòÌïòÎäîÏßÄ ÌôïÏù∏
                                                exact_match = price_table[price_table['ÏàòÎüâ'] == qty]
                                                if not exact_match.empty:
                                                    exact_price = exact_match['Îã®Í∞Ä'].values[0]
                                                    quantity_prices[qty] = {
                                                        'price': exact_price,
                                                        'price_with_vat': round(exact_price * 1.1),
                                                        'exact_match': True
                                                    }
                                                    logger.info(f"ÏàòÎüâ {qty}Í∞ú Ï†ïÌôïÌûà ÏùºÏπò: {exact_price}Ïõê")
                                                    continue
                                                
                                                # Ï†ïÌôïÌûà ÏùºÏπòÌïòÏßÄ ÏïäÎäî Í≤ΩÏö∞, Î≤îÏúÑÏóê ÎßûÎäî Í∞ÄÍ≤© Ï∞æÍ∏∞
                                                # Ïòà: 100Í∞ú=5Ï≤úÏõê, 200Í∞ú=4Ï≤úÏõê Ïùº Îïå 120Í∞úÎäî 5Ï≤úÏõêÏùÑ Ï†ÅÏö©
                                                lower_rows = price_table[price_table['ÏàòÎüâ'] <= qty]
                                                if not lower_rows.empty:
                                                    # Ï£ºÎ¨∏ ÏàòÎüâÎ≥¥Îã§ ÏûëÍ±∞ÎÇò Í∞ôÏùÄ ÏµúÎåÄ ÏàòÎüâ Ï∞æÍ∏∞
                                                    max_lower_qty = lower_rows['ÏàòÎüâ'].max()
                                                    max_lower_row = price_table[price_table['ÏàòÎüâ'] == max_lower_qty]
                                                    max_lower_price = max_lower_row['Îã®Í∞Ä'].values[0]
                                                    
                                                    quantity_prices[qty] = {
                                                        'price': max_lower_price,
                                                        'price_with_vat': round(max_lower_price * 1.1),
                                                        'exact_match': False,
                                                        'actual_quantity': max_lower_qty,
                                                        'note': f"Íµ¨Í∞Ñ Í∞ÄÍ≤©({max_lower_qty}Í∞ú Ïù¥ÏÉÅ) Ï†ÅÏö©"
                                                    }
                                                    logger.info(f"ÏàòÎüâ {qty}Í∞úÎäî {max_lower_qty}Í∞ú Íµ¨Í∞Ñ Í∞ÄÍ≤© Ï†ÅÏö©: {max_lower_price}Ïõê")
                                                    continue
                                                
                                                # ÌÖåÏù¥Î∏îÏùò Î™®Îì† ÏàòÎüâÎ≥¥Îã§ ÌÅ∞ Í≤ΩÏö∞, Í∞ÄÏû• ÌÅ∞ ÏàòÎüâÏùò Í∞ÄÍ≤© Ï†ÅÏö©
                                                max_table_quantity = price_table['ÏàòÎüâ'].max()
                                                max_qty_row = price_table[price_table['ÏàòÎüâ'] == max_table_quantity]
                                                if not max_qty_row.empty:
                                                    max_qty_price = max_qty_row['Îã®Í∞Ä'].values[0]
                                                    quantity_prices[qty] = {
                                                        'price': max_qty_price,
                                                        'price_with_vat': round(max_qty_price * 1.1),
                                                        'exact_match': False,
                                                        'actual_quantity': max_table_quantity,
                                                        'note': f"ÏµúÎåÄ Íµ¨Í∞Ñ({max_table_quantity}Í∞ú) Í∞ÄÍ≤© Ï†ÅÏö©"
                                                    }
                                                    logger.info(f"ÏàòÎüâ {qty}Í∞úÎäî ÏµúÎåÄ Íµ¨Í∞Ñ {max_table_quantity}Í∞ú Í∞ÄÍ≤© Ï†ÅÏö©: {max_qty_price}Ïõê")
                                        
                                        # ÌÖåÏù¥Î∏îÏù¥ ÏóÜÏúºÎ©¥ ÏßÅÏ†ë ÏàòÎüâ Î≥ÄÍ≤ΩÌïòÏó¨ Í∞ÄÍ≤© Í∞ÄÏ†∏Ïò§Í∏∞
                                        else:
                                            # ÏµúÏÜå ÏàòÎüâ Ï†ïÎ≥¥Î•º Ï†ÄÏû•ÌïòÍ∏∞ ÏúÑÌïú Î≥ÄÏàò
                                            min_quantity_info = None
                                            
                                            for qty in custom_quantities:
                                                # Ïù¥ÎØ∏ ÏµúÏÜå ÏàòÎüâ Ïò§Î•òÍ∞Ä ÏûàÎäî Í≤ΩÏö∞, ÏµúÏÜå ÏàòÎüâ Ïù¥ÌïòÎäî Ï≤òÎ¶¨ ÏïàÌï®
                                                if min_quantity_info and qty < min_quantity_info['min_quantity']:
                                                    # ÏµúÏÜå ÏàòÎüâÏóê Ìï¥ÎãπÌïòÎäî Í∞ÄÍ≤© Ï†ïÎ≥¥ ÏÇ¨Ïö©
                                                    quantity_prices[qty] = {
                                                        'price': min_quantity_info['price'],
                                                        'price_with_vat': min_quantity_info['price_with_vat'],
                                                        'exact_match': False,
                                                        'actual_quantity': min_quantity_info['min_quantity'],
                                                        'note': f"ÏµúÏÜå Ï£ºÎ¨∏ ÏàòÎüâ({min_quantity_info['min_quantity']}) Í∞ÄÍ≤© Ï†ÅÏö©"
                                                    }
                                                    logger.info(f"ÏàòÎüâ {qty}Í∞úÎäî ÏµúÏÜå Íµ¨Í∞Ñ {min_quantity_info['min_quantity']}Í∞ú Í∞ÄÍ≤© Ï†ÅÏö©: {min_quantity_info['price']}Ïõê")
                                                    continue
                                                
                                                # ÌäπÏ†ï ÏàòÎüâÏóê ÎåÄÌïú Í∞ÄÍ≤© Ï°∞Ìöå
                                                price_result = await get_price_for_specific_quantity(price_detail_page, final_href_url, qty, timeout=20000)
                                                
                                                # ÏµúÏÜå ÏàòÎüâ Ïò§Î•òÍ∞Ä Î∞úÏÉùÌïú Í≤ΩÏö∞
                                                if price_result['min_quantity_error'] and price_result['min_quantity']:
                                                    logger.info(f"ÏàòÎüâ {qty}Í∞úÎäî ÏµúÏÜå Ï£ºÎ¨∏ ÏàòÎüâ({price_result['min_quantity']})Î≥¥Îã§ ÏûëÏäµÎãàÎã§.")
                                                    
                                                    # ÏµúÏÜå ÏàòÎüâÏóê ÎåÄÌïú Í∞ÄÍ≤© Ï°∞Ìöå
                                                    min_qty = price_result['min_quantity']
                                                    min_price_result = await get_price_for_specific_quantity(price_detail_page, final_href_url, min_qty, timeout=20000)
                                                    
                                                    if min_price_result['success']:
                                                        # ÏµúÏÜå ÏàòÎüâ Ï†ïÎ≥¥ Ï†ÄÏû•
                                                        min_quantity_info = {
                                                            'min_quantity': min_qty,
                                                            'price': min_price_result['price'],
                                                            'price_with_vat': min_price_result['price_with_vat']
                                                        }
                                                        
                                                        # ÌòÑÏû¨ ÏàòÎüâÏóê ÎåÄÌïú Í∞ÄÍ≤© Ï†ïÎ≥¥ ÏÑ§Ï†ï
                                                        quantity_prices[qty] = {
                                                            'price': min_price_result['price'],
                                                            'price_with_vat': min_price_result['price_with_vat'],
                                                            'exact_match': False,
                                                            'actual_quantity': min_qty,
                                                            'note': f"ÏµúÏÜå Ï£ºÎ¨∏ ÏàòÎüâ({min_qty}) Í∞ÄÍ≤© Ï†ÅÏö©"
                                                        }
                                                        logger.info(f"ÏàòÎüâ {qty}Í∞úÏóê ÏµúÏÜå ÏàòÎüâ({min_qty})Ïùò Í∞ÄÍ≤© {min_price_result['price']}Ïõê Ï†ÅÏö©")
                                                    else:
                                                        logger.warning(f"ÏµúÏÜå Ï£ºÎ¨∏ ÏàòÎüâ({min_qty})Ïóê ÎåÄÌïú Í∞ÄÍ≤© Ï°∞Ìöå Ïã§Ìå®")
                                                # Ï†ïÏÉÅÏ†ÅÏúºÎ°ú Í∞ÄÍ≤©ÏùÑ Í∞ÄÏ†∏Ïò® Í≤ΩÏö∞
                                                elif price_result['success']:
                                                    quantity_prices[qty] = {
                                                        'price': price_result['price'],
                                                        'price_with_vat': price_result['price_with_vat'],
                                                        'exact_match': True
                                                    }
                                                    logger.info(f"ÏàòÎüâ {qty}Í∞ú Í∞ÄÍ≤© Ï°∞Ìöå ÏÑ±Í≥µ: {price_result['price']}Ïõê")
                                                else:
                                                    logger.warning(f"ÏàòÎüâ {qty}Í∞úÏóê ÎåÄÌïú Í∞ÄÍ≤© Ï°∞Ìöå Ïã§Ìå®: {item_data['name']}")
                                        
                                        # ÏàòÎüâÎ≥Ñ Í∞ÄÍ≤© Ï†ïÎ≥¥ Ï†ÄÏû•
                                        item_data['quantity_prices'] = quantity_prices
                                        
                                        # Í∏∞Î≥∏ Í∞ÄÍ≤© Ï†ïÎ≥¥ ÏÑ§Ï†ï (Í∞ÄÏû• ÏûëÏùÄ ÏàòÎüâÏùò Í∞ÄÍ≤© ÎòêÎäî Î™©Î°ù ÌéòÏù¥ÏßÄ Í∞ÄÍ≤©)
                                        if quantity_prices:
                                            min_qty = min(quantity_prices.keys())
                                            item_data['price'] = quantity_prices[min_qty]['price']
                                            item_data['price_with_vat'] = quantity_prices[min_qty]['price_with_vat']
                                        else:
                                            # ÏàòÎüâÎ≥Ñ Í∞ÄÍ≤©ÏùÑ Í∞ÄÏ†∏Ïò§ÏßÄ Î™ªÌïú Í≤ΩÏö∞ Î™©Î°ù ÌéòÏù¥ÏßÄ Í∞ÄÍ≤© ÏÇ¨Ïö©
                                            item_data['price'] = price_value
                                            item_data['price_with_vat'] = round(price_value * 1.1)
                                        
                                        # Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨
                                        await price_detail_page.close()
                                        await price_detail_context.close()
                                        
                                        items_on_page.append(item_data)
                                        processed_items += 1
                                        
                                    except Exception as item_error:
                                        logger.warning(f"‚ö†Ô∏è Error processing item {i} on page {page_number}: {item_error}")
                                        continue
                                
                                data.extend(items_on_page)
                                logger.debug(f"üìä Scraped {len(items_on_page)} items from page {page_number}. Total processed: {processed_items}")

                                if processed_items >= max_items_per_variation:
                                    logger.info(f"‚úÖ Reached scrape limit ({max_items_per_variation}) for keyword '{keyword}'.")
                                    break

                                # --- Pagination --- 
                                next_page_locator_str = f'div.custom_paging > div[onclick*="getPageGo1({page_number + 1})"]'
                                next_page_locator = page.locator(next_page_locator_str)
                                
                                try:
                                    if await next_page_locator.is_visible(timeout=5000):
                                        logger.debug(f"üìÑ Clicking next page ({page_number + 1})")
                                        await next_page_locator.click(timeout=5000)
                                        await page.wait_for_load_state('domcontentloaded', 
                                                                     timeout=config.getint('ScraperSettings', 'navigation_timeout', fallback=90000))
                                        # Extra delay after pagination to ensure page stability
                                        await page.wait_for_timeout(2000)
                                        page_number += 1
                                    else:
                                        logger.info("‚ö†Ô∏è Next page element not found or not visible. Ending pagination.")
                                        break
                                except Exception as pagination_error:
                                    logger.warning(f"‚ö†Ô∏è Error during pagination: {pagination_error}")
                                    break
                                    
                            except Exception as page_error:
                                logger.error(f"‚ö†Ô∏è Error processing page {page_number}: {page_error}")
                                break
                        
                        # Add scraped data to results if we found anything
                        if data:
                            logger.info(f"‚úÖ Successfully scraped {len(data)} items for keyword '{keyword}' from {base_url}")
                            df = pd.DataFrame(data)
                            all_results.append(df)
                        else:
                            logger.warning(f"‚ö†Ô∏è No data could be scraped for keyword '{keyword}' from {base_url}")

                    except Exception as search_error:
                        logger.error(f"‚ö†Ô∏è Error during search for keyword '{keyword}': {search_error}")
                        continue
                        
                except Exception as keyword_error:
                    logger.error(f"‚ö†Ô∏è Error processing keyword '{keyword}': {keyword_error}")
                    continue

        except Exception as url_error:
            logger.error(f"‚ö†Ô∏è Error processing URL {base_url}: {url_error}")
        finally:
            # Clean up resources
            if page:
                try:
                    await page.close()
                except Exception as page_close_error:
                    logger.warning(f"‚ö†Ô∏è Error closing page: {page_close_error}")
            if context:
                try:
                    await context.close()
                except Exception as context_close_error:
                    logger.warning(f"‚ö†Ô∏è Error closing context: {context_close_error}")

    # Combine all results
    if all_results:
        final_df = pd.concat(all_results, ignore_index=True)
        if 'href' in final_df.columns:
            final_df = final_df.drop_duplicates(subset=['href'], keep='first')
        logger.info(f"Total unique results from all keyword variations: {len(final_df)}")
        return final_df
    else:
        logger.warning("No results found from any keyword variation or Kogift URL")
        return pd.DataFrame()

# Simple function to test direct image download
def test_kogift_scraper():
    """Test Kogift scraper functionality"""
    import sys
    import os
    import logging
    import requests
    import random
    import time
    import pandas as pd
    from datetime import datetime
    from playwright.async_api import async_playwright
    from utils import load_config

    # --- command-line args ---
    parser = argparse.ArgumentParser(description='Test Kogift scraper functionality')
    parser.add_argument('--test-type', choices=['all','images','products','quantities','test2'], default='all',
                        help='Which test to run')
    parser.add_argument('--max-items', type=int, default=5,
                        help='Max items per keyword')
    parser.add_argument('--quantity', type=int, action='append',
                        help='Quantities to test')
    parser.add_argument('--input-notepad', type=str,
                        help='Path to tab-delimited input file (test2)')
    parser.add_argument('--search-terms', nargs='+', 
                        default=["Î™®Ïä§ÎãàÏóê Ï†úÎ°úÏõ®Ïù¥Ïä§Ìä∏ ÎåÄÎÇòÎ¨¥Ïπ´ÏÜî", "ÌïòÎ™®Îãà Ïã¨ÌîåÏπ´ÏÜîÏÑ∏Ìä∏ 805", "CLIO ÌÅ¨Î¶¨Ïò§ ÏïåÌååÏÜîÎ£®ÏÖò Ìú¥ÎåÄÏö© ÏñëÏπòÏÑ∏Ìä∏"],
                        help='Search terms to use for testing. Can include multiple terms separated by spaces.')
    parser.add_argument('--max-variations', type=int, default=2,
                        help='Maximum number of keyword variations to generate (default: 2)')
    parser.add_argument('--headless', action='store_true',
                        help='Run browser in headless mode')
    
    args = parser.parse_args()
    if not args.quantity:
        args.quantity = [30, 50, 100, 300]

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logger = logging.getLogger('kogift_test')

    # Load config
    config = load_config(os.path.join(os.path.dirname(__file__), '..','config.ini'))
    # adjust max items
    if config.has_section('ScraperSettings'):
        config.set('ScraperSettings','kogift_max_items', str(args.max_items))
    
    # --- Helper tests inside test_kogift_scraper() ---
    # 1) Image download test (synchronous)
    def test_image_download():
        logger.info("=== TESTING IMAGE DOWNLOAD FUNCTIONALITY ===")
        
        test_urls = [
            "https://koreagift.com/ez/upload/mall/shop_1707873892937710_0.jpg",  # Ïò¨Î∞îÎ•∏ ÌòïÏãù (ez Ìè¨Ìï®)
            "https://koreagift.com/upload/mall/shop_1736386408518966_0.jpg",     # ÏûòÎ™ªÎêú ÌòïÏãù (ez ÎØ∏Ìè¨Ìï®)
            "https://adpanchok.co.kr/upload/mall/shop_1234567890_0.jpg",         # Ïï†ÎìúÌåêÏ¥â Ïù¥ÎØ∏ÏßÄ
            "https://koreagift.com/ez/upload/no_image.jpg"                       # Ï°¥Ïû¨ÌïòÏßÄ ÏïäÎäî Ïù¥ÎØ∏ÏßÄ
        ]
        
        print(f"\n{'=' * 70}")
        print(f"üîç Ïù¥ÎØ∏ÏßÄ URL Ï†ïÍ∑úÌôî Î∞è Îã§Ïö¥Î°úÎìú ÌÖåÏä§Ìä∏ (Ï¥ù {len(test_urls)}Í∞ú URL)")
        print(f"{'=' * 70}")
        
        # ÌÖåÏä§Ìä∏ ÎîîÎ†âÌÜ†Î¶¨ ÏÑ§Ï†ï
        save_dir = os.path.join(config.get('Paths','image_target_dir',fallback='downloaded_images'), 'kogift_test_images')
        os.makedirs(save_dir, exist_ok=True)
        print(f"üìÅ ÌÖåÏä§Ìä∏ Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû• Í≤ΩÎ°ú: {save_dir}")
        
        # Í≤∞Í≥º ÏöîÏïΩÏùÑ ÏúÑÌïú Ïπ¥Ïö¥ÌÑ∞
        successful_downloads = 0
        failed_downloads = 0
        normalized_count = 0
        
        # Í∞Å URLÏóê ÎåÄÌïú ÌÖåÏä§Ìä∏ ÏàòÌñâ
        for i, url in enumerate(test_urls):
            print(f"\n[ÌÖåÏä§Ìä∏ {i+1}/{len(test_urls)}]")
            print(f"ÏõêÎ≥∏ URL: {url}")
            
            # URL Ï†ïÍ∑úÌôî
            norm_url, valid = normalize_kogift_image_url(url)
            
            if norm_url != url:
                normalized_count += 1
                print(f"Ï†ïÍ∑úÌôî URL: {norm_url} (Î≥ÄÍ≤ΩÎê®)")
            else:
                print(f"Ï†ïÍ∑úÌôî URL: {norm_url} (Î≥ÄÍ≤Ω ÏóÜÏùå)")
                
            print(f"URL Ïú†Ìö®ÏÑ±: {'‚úÖ Ïú†Ìö®Ìï®' if valid else '‚ùå Ïú†Ìö®ÌïòÏßÄ ÏïäÏùå'}")
            
            if not valid:
                failed_downloads += 1
                print(f"‚ö†Ô∏è Ïú†Ìö®ÌïòÏßÄ ÏïäÏùÄ URL - Îã§Ïö¥Î°úÎìú Í±¥ÎÑàÎúÄ")
                continue
                
            # Ïù¥ÎØ∏ÏßÄ Îã§Ïö¥Î°úÎìú
            print(f"Ïù¥ÎØ∏ÏßÄ Îã§Ïö¥Î°úÎìú ÏãúÎèÑ Ï§ë...")
            test_filename = f"test_{i+1}_{hashlib.md5(url.encode()).hexdigest()[:6]}.jpg"
            path = download_image(norm_url, save_dir, test_filename)
            
            if path:
                successful_downloads += 1
                file_size = os.path.getsize(path) if os.path.exists(path) else 0
                print(f"‚úÖ Îã§Ïö¥Î°úÎìú ÏÑ±Í≥µ: {os.path.basename(path)} ({file_size/1024:.1f} KB)")
                
                # Ïù¥ÎØ∏ÏßÄ Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨ (PIL ÏÇ¨Ïö©)
                try:
                    with Image.open(path) as img:
                        width, height = img.size
                        print(f"   Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞: {width}x{height} ÌîΩÏÖÄ")
                        print(f"   Ïù¥ÎØ∏ÏßÄ ÌòïÏãù: {img.format}")
                except Exception as img_err:
                    print(f"‚ö†Ô∏è Ïù¥ÎØ∏ÏßÄ Í≤ÄÏ¶ù Ïò§Î•ò: {img_err}")
            else:
                failed_downloads += 1
                print(f"‚ùå Îã§Ïö¥Î°úÎìú Ïã§Ìå®")
        
        # ÌÖåÏä§Ìä∏ Í≤∞Í≥º ÏöîÏïΩ
        print(f"\n{'=' * 70}")
        print(f"üìä Ïù¥ÎØ∏ÏßÄ ÌÖåÏä§Ìä∏ Í≤∞Í≥º ÏöîÏïΩ")
        print(f"{'=' * 70}")
        print(f"Ï¥ù ÌÖåÏä§Ìä∏ URL: {len(test_urls)}Í∞ú")
        print(f"Ï†ïÍ∑úÌôîÎêú URL: {normalized_count}Í∞ú")
        print(f"Îã§Ïö¥Î°úÎìú ÏÑ±Í≥µ: {successful_downloads}Í∞ú")
        print(f"Îã§Ïö¥Î°úÎìú Ïã§Ìå®: {failed_downloads}Í∞ú")
        print(f"Îã§Ïö¥Î°úÎìú ÏÑ±Í≥µÎ•†: {successful_downloads/len(test_urls)*100:.1f}%")
        
        # Ïã§Ï†ú Îã§Ïö¥Î°úÎìúÎêú Î™®Îì† ÌååÏùº ÌëúÏãú
        if os.path.exists(save_dir):
            downloaded_files = [f for f in os.listdir(save_dir) if os.path.isfile(os.path.join(save_dir, f))]
            if downloaded_files:
                print(f"\nüìÅ Îã§Ïö¥Î°úÎìúÎêú ÌååÏùº Î™©Î°ù:")
                for i, file in enumerate(downloaded_files[:10]):  # ÏµúÎåÄ 10Í∞úÎßå ÌëúÏãú
                    file_path = os.path.join(save_dir, file)
                    file_size = os.path.getsize(file_path)
                    print(f"   {i+1}. {file} ({file_size/1024:.1f} KB)")
                
                if len(downloaded_files) > 10:
                    print(f"   ... Ïô∏ {len(downloaded_files) - 10}Í∞ú ÌååÏùº")
                    
        print(f"{'=' * 70}")

    # 2) Product info test (requires browser)
    async def test_product_info(browser):
        logger.info("=== TESTING PRODUCT INFORMATION RETRIEVAL ===")
        
        # Use specified search terms
        test_keywords = args.search_terms
        
        # Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû• ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ± (ÌÖåÏä§Ìä∏Ïö©)
        test_image_dir = os.path.join(config.get('Paths', 'image_target_dir', fallback='downloaded_images'), 'kogift_test')
        os.makedirs(test_image_dir, exist_ok=True)
        
        for keyword in test_keywords:
            logger.info(f"\n--- Searching for '{keyword}' ---")
            try:
                # Pass the custom quantities to scrape_data
                df = await scrape_data(browser, keyword, config=config, 
                                 custom_quantities=args.quantity, 
                                 fetch_price_tables=True)  # ÌÖåÏù¥Î∏î Îç∞Ïù¥ÌÑ∞ÎèÑ Í∞ÄÏ†∏Ïò§Í∏∞
                
                if df.empty:
                    print(f"No products found for '{keyword}'")
                    continue
                    
                print(f"Found {len(df)} products for '{keyword}'")
                
                # Display image URLs and prices for each product
                for idx, row in df.iterrows():
                    print(f"\n{'=' * 70}")
                    print(f"Product {idx+1}: {row.get('name', 'Unknown Name')}")
                    print(f"  URL: {row.get('href', 'N/A')}")
                    print(f"  Supplier: {row.get('supplier', 'Unknown')}")
                    
                    # Ïù¥ÎØ∏ÏßÄ Ï†ïÎ≥¥ Ï∂úÎ†• Î∞è ÌÖåÏä§Ìä∏ Îã§Ïö¥Î°úÎìú
                    img_url = row.get('image_url')
                    if img_url:
                        norm_url, valid = normalize_kogift_image_url(img_url)
                        print(f"  Image URL: {img_url}")
                        print(f"  Normalized URL: {norm_url}")
                        print(f"  Image URL valid: {'Yes' if valid else 'No'}")
                        
                        # Ïù¥ÎØ∏ÏßÄ Îã§Ïö¥Î°úÎìú ÌÖåÏä§Ìä∏
                        if valid:
                            print("  Testing image download...")
                            product_name_hash = hashlib.md5(row.get('name', '').encode()).hexdigest()[:8]
                            img_filename = f"test_{idx}_{product_name_hash}.jpg"
                            
                            download_path = download_image(norm_url, test_image_dir, img_filename)
                            if download_path:
                                img_size = os.path.getsize(download_path) if os.path.exists(download_path) else 0
                                print(f"  ‚úÖ Image downloaded: {os.path.basename(download_path)} ({img_size/1024:.1f} KB)")
                            else:
                                print(f"  ‚ùå Failed to download image")
                    else:
                        print(f"  ‚ùå No image URL available")
                    
                    print(f"\n  Price Information:")
                    print(f"  Basic Price (excl. VAT): {row.get('price', 'N/A')} KRW")
                    print(f"  Basic Price (incl. VAT): {row.get('price_with_vat', 'N/A')} KRW")
                    
                    # ÏàòÎüâÎ≥Ñ Í∞ÄÍ≤© Ï†ïÎ≥¥ ÏÉÅÏÑ∏ Î∂ÑÏÑù Î∞è ÌëúÏãú
                    if 'quantity_prices' in row and row['quantity_prices']:
                        print("\n  Quantity-based prices:")
                        print("  " + "-" * 68)
                        print("  | {:^8} | {:^12} | {:^12} | {:^28} |".format("ÏàòÎüâ", "Îã®Í∞Ä(VATÏ†úÏô∏)", "Îã®Í∞Ä(VATÌè¨Ìï®)", "ÎπÑÍ≥†"))
                        print("  " + "-" * 68)
                        
                        # ÏàòÎüâ ÏàúÏÑúÎåÄÎ°ú Ï†ïÎ†¨ÌïòÏó¨ ÌëúÏãú
                        sorted_quantities = sorted(row['quantity_prices'].keys())
                        
                        for qty in sorted_quantities:
                            price_info = row['quantity_prices'][qty]
                            price = price_info['price']
                            price_with_vat = price_info['price_with_vat']
                            
                            # ÎπÑÍ≥† Ï†ïÎ≥¥ Íµ¨ÏÑ±
                            if price_info.get('exact_match', False):
                                note = "Ï†ïÌôïÌïú ÏàòÎüâ ÏùºÏπò"
                            elif 'note' in price_info:
                                note = price_info['note']
                            elif 'actual_quantity' in price_info:
                                note = f"Í∑ºÏÇ¨Í∞í (Ïã§Ï†ú ÏàòÎüâ: {price_info['actual_quantity']}Í∞ú)"
                            else:
                                note = "-"
                                
                            print("  | {:>8,d} | {:>12,d} | {:>12,d} | {:<28} |".format(
                                qty, price, price_with_vat, note))
                        
                        print("  " + "-" * 68)
                        
                        # ÏàòÎüâÎ≥Ñ Í∞ÄÍ≤© Î≥ÄÌôî Ï∂îÏù¥ Î∂ÑÏÑù
                        if len(sorted_quantities) > 1:
                            min_qty = min(sorted_quantities)
                            max_qty = max(sorted_quantities)
                            min_price = row['quantity_prices'][min_qty]['price']
                            max_price = row['quantity_prices'][max_qty]['price']
                            
                            if min_price > max_price:
                                price_trend = f"ÏàòÎüâÏù¥ Ï¶ùÍ∞ÄÌï†ÏàòÎ°ù Îã®Í∞Ä Í∞êÏÜå ({min_price}Ïõê ‚Üí {max_price}Ïõê), Ìï†Ïù∏Ïú®: {(1 - max_price/min_price)*100:.1f}%"
                            elif min_price < max_price:
                                price_trend = f"ÏàòÎüâÏù¥ Ï¶ùÍ∞ÄÌï†ÏàòÎ°ù Îã®Í∞Ä Ï¶ùÍ∞Ä ({min_price}Ïõê ‚Üí {max_price}Ïõê), ÏÉÅÏäπÎ•†: {(max_price/min_price - 1)*100:.1f}%"
                            else:
                                price_trend = "ÏàòÎüâÏóê Í¥ÄÍ≥ÑÏóÜÏù¥ Îã®Í∞Ä ÏùºÏ†ï"
                                
                            print(f"\n  Í∞ÄÍ≤© Ï∂îÏù¥ Î∂ÑÏÑù: {price_trend}")
                    else:
                        print("\n  ‚ùå No quantity-based price information available")
                    
                    # ÏàòÎüâÍ≥º Í∞ÄÍ≤© Ï°∞Ìï©Ïù¥ Ï†ÅÏ†àÌïúÏßÄ Í≤ÄÏ¶ù
                    if 'price' in row and 'quantity_prices' in row and row['quantity_prices']:
                        min_qty_price = min([info['price'] for info in row['quantity_prices'].values()])
                        base_price = row.get('price', 0)
                        
                        if abs(min_qty_price - base_price) > base_price * 0.1:  # 10% Ïù¥ÏÉÅ Ï∞®Ïù¥
                            print(f"\n  ‚ö†Ô∏è Warning: Base price ({base_price}Ïõê) differs significantly from minimum quantity price ({min_qty_price}Ïõê)")
                    
                    print(f"{'=' * 70}")
                    
                    # Limit display to first 3 products per keyword to avoid too much output
                    if idx >= 2:
                        print(f"... and {len(df) - 3} more products")
                        break
            except Exception as e:
                print(f"Error searching for '{keyword}': {e}")
                logger.error(f"Error during test_product_info for keyword '{keyword}': {e}", exc_info=True)
                print(f"Skipping to next keyword...")
                continue

    # 3) Custom quantities pricing test (requires browser)
    async def test_custom_quantities(browser):
        logger.info("=== TESTING CUSTOM QUANTITIES FUNCTIONALITY ===")
        
        # Use the first search term for quantity testing
        keyword = args.search_terms[0]
        logger.info(f"Testing quantities for '{keyword}'...")
        
        try:
            # Check if browser is connected
            if not browser or not browser.is_connected():
                logger.warning("Browser is not connected. Attempting to create a new browser...")
                from playwright.async_api import async_playwright
                p = await async_playwright().start()
                browser = await p.chromium.launch(
                    headless=config.getboolean('Playwright', 'playwright_headless', fallback=True),
                    args=json.loads(config.get('Playwright', 'playwright_browser_args', fallback='["--disable-gpu", "--disable-dev-shm-usage", "--no-sandbox"]')),
                    timeout=60000
                )
                logger.info("Successfully created new browser instance")

            # Create a new context for price testing
            context = await browser.new_context(
                user_agent=config.get('Network', 'user_agent', fallback='Mozilla/5.0 ...'),
                viewport={'width': 1920, 'height': 1080},
            )
            page = await context.new_page()
            
            # Test direct product search first to get product URL
            print(f"\n{'=' * 70}")
            print(f"Í≤ÄÏÉâÏñ¥: '{keyword}'Ïóê ÎåÄÌïú ÏÉÅÌíà Í≤ÄÏÉâ Ï§ë...")
            df = await scrape_data(browser, keyword, config=config, custom_quantities=args.quantity)
            
            if df.empty:
                print(f"‚ùå Í≤ÄÏÉâÏñ¥ '{keyword}'Ïóê ÎåÄÌïú ÏÉÅÌíàÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
                await context.close()
                return
                
            print(f"‚úÖ {len(df)}Í∞ú ÏÉÅÌíàÏùÑ Ï∞æÏïòÏäµÎãàÎã§.")
            
            # ÌÖåÏä§Ìä∏Ìï† ÏÉÅÌíà ÏÑ†ÌÉù (ÏµúÎåÄ 2Í∞ú)
            test_products = min(2, len(df))
            for product_idx in range(test_products):
                # Get product info
                product = df.iloc[product_idx]
                product_url = product.get('href', None)
                product_name = product.get('name', 'Unknown Product')
                
                if not product_url:
                    print(f"‚ùå ÏÉÅÌíà URLÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
                    continue
                    
                print(f"\n{'=' * 70}")
                print(f"üëâ ÏÉÅÌíà ÌÖåÏä§Ìä∏ #{product_idx+1}: {product_name}")
                print(f"   URL: {product_url}")
                
                # 1. ÏßÅÏ†ë ÏàòÎüâ ÏûÖÎ†• Î∞©Ïãù ÌÖåÏä§Ìä∏
                print(f"\n[1] ÏßÅÏ†ë ÏàòÎüâ ÏûÖÎ†• Î∞©Ïãù ÌÖåÏä§Ìä∏")
                print(f"{'-' * 50}")
                
                # ÏàòÎüâÎ≥Ñ Í≤∞Í≥º Ï†ÄÏû•
                qty_results = []
                
                for qty in sorted(args.quantity):
                    try:
                        result = await get_price_for_specific_quantity(page, product_url, qty, timeout=20000)
                        qty_results.append({
                            'quantity': qty,
                            'success': result['success'],
                            'price': result.get('price', 0),
                            'price_with_vat': result.get('price_with_vat', 0),
                            'min_quantity_error': result.get('min_quantity_error', False),
                            'min_quantity': result.get('min_quantity', None)
                        })
                        
                        if result['success']:
                            print(f"‚úÖ ÏàòÎüâ {qty:,d}Í∞ú: {result['price']:,d}Ïõê (VATÌè¨Ìï®: {result['price_with_vat']:,d}Ïõê)")
                        else:
                            if result.get('min_quantity_error'):
                                print(f"‚ö†Ô∏è ÏàòÎüâ {qty:,d}Í∞ú: ÏµúÏÜå Ï£ºÎ¨∏ ÏàòÎüâÏùÄ {result['min_quantity']:,d}Í∞ú ÏûÖÎãàÎã§.")
                            else:
                                print(f"‚ùå ÏàòÎüâ {qty:,d}Í∞ú: Í∞ÄÍ≤© Ï°∞Ìöå Ïã§Ìå®")
                            
                    except Exception as e:
                        logger.error(f"Error getting price for quantity {qty}: {e}")
                        print(f"‚ùå ÏàòÎüâ {qty:,d}Í∞ú: Ïò§Î•ò Î∞úÏÉù - {str(e)}")
                        
                # 2. Í∞ÄÍ≤© ÌÖåÏù¥Î∏î ÌÖåÏä§Ìä∏
                print(f"\n[2] Í∞ÄÍ≤© ÌÖåÏù¥Î∏î ÌÖåÏä§Ìä∏")
                print(f"{'-' * 50}")
                
                try:
                    price_table = await extract_price_table(page, product_url)
                    
                    if price_table is not None and not price_table.empty:
                        print("‚úÖ Í∞ÄÍ≤© ÌÖåÏù¥Î∏î Î∞úÍ≤¨!")
                        print("\nüìä Í∞ÄÍ≤© ÌÖåÏù¥Î∏î ÎÇ¥Ïö©:")
                        print("-" * 50)
                        print("| {:^8} | {:^12} | {:^12} | {:^15} |".format(
                            "ÏàòÎüâ", "Îã®Í∞Ä(VATÏ†úÏô∏)", "Îã®Í∞Ä(VATÌè¨Ìï®)", "ÎπÑÍ≥†"))
                        print("-" * 50)
                        
                        for _, row in price_table.iterrows():
                            qty = row['ÏàòÎüâ']
                            price = row['Îã®Í∞Ä']
                            price_with_vat = round(price * 1.1)
                            note = row.get('ÎπÑÍ≥†', '')
                            
                            print("| {:>8,d} | {:>12,d} | {:>12,d} | {:<15} |".format(
                                qty, price, price_with_vat, note))
                        
                        print("-" * 50)
                        
                        # Í∞ÄÍ≤© Ï∂îÏù¥ Î∂ÑÏÑù
                        if len(price_table) > 1:
                            min_price = price_table['Îã®Í∞Ä'].min()
                            max_price = price_table['Îã®Í∞Ä'].max()
                            price_diff = max_price - min_price
                            if price_diff > 0:
                                discount_rate = (price_diff / max_price) * 100
                                print(f"\nÍ∞ÄÍ≤© Ï∂îÏù¥ Î∂ÑÏÑù: ÏàòÎüâÏù¥ Ï¶ùÍ∞ÄÌï†ÏàòÎ°ù Îã®Í∞Ä Í∞êÏÜå ({max_price:,d}Ïõê ‚Üí {min_price:,d}Ïõê), Ìï†Ïù∏Ïú®: {discount_rate:.1f}%")
                    else:
                        print(f"‚ùå Í∞ÄÍ≤© ÌÖåÏù¥Î∏îÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
                        
                        # ÏßÅÏ†ë ÏûÖÎ†• Î∞©Ïãù Í≤∞Í≥ºÎßå ÏöîÏïΩ ÌëúÏãú
                        if qty_results:
                            print(f"\nüìä ÏßÅÏ†ë ÏàòÎüâ ÏûÖÎ†• Í≤∞Í≥º ÏöîÏïΩ:")
                            print("-" * 50)
                            print("| {:^8} | {:^12} | {:^12} | {:^15} |".format(
                                "ÏàòÎüâ", "Îã®Í∞Ä(VATÏ†úÏô∏)", "Îã®Í∞Ä(VATÌè¨Ìï®)", "ÎπÑÍ≥†"))
                            print("-" * 50)
                            
                            for result in qty_results:
                                note = ""
                                if result['min_quantity_error']:
                                    note = f"ÏµúÏÜåÏàòÎüâ({result['min_quantity']})"
                                elif not result['success']:
                                    note = "Ï°∞ÌöåÏã§Ìå®"
                                    
                                print("| {:>8,d} | {:>12,d} | {:>12,d} | {:<15} |".format(
                                    result['quantity'], result['price'], result['price_with_vat'], note))
                            
                            print("-" * 50)
                        
                except Exception as e:
                    logger.error(f"Error extracting price table: {e}")
                    print(f"‚ùå Í∞ÄÍ≤© ÌÖåÏù¥Î∏î Ï∂îÏ∂ú Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}")
            
            await page.close()
            await context.close()
            
        except Exception as e:
            logger.error(f"Error in test_custom_quantities: {e}")
            print(f"‚ùå ÏàòÎüâÎ≥Ñ Í∞ÄÍ≤© ÌÖåÏä§Ìä∏ Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}")
            # Try to clean up resources even if there was an error
            try:
                if 'page' in locals():
                    await page.close()
                if 'context' in locals():
                    await context.close()
            except Exception as cleanup_error:
                logger.error(f"Error during cleanup: {cleanup_error}")

    # 4) Standard test dispatcher
    async def run_standard_tests():
        print(f"\n{'=' * 70}")
        print(f"üß™ Í≥†Î†§Í∏∞ÌîÑÌä∏ ÌÅ¨Î°§ÎßÅ ÌÖåÏä§Ìä∏ ÏãúÏûë")
        print(f"{'=' * 70}")
        print(f"ÌÖåÏä§Ìä∏ Ïú†Ìòï: {args.test_type}")
        print(f"Í≤ÄÏÉâÏñ¥: {args.search_terms}")
        print(f"ÌÖåÏä§Ìä∏ ÏàòÎüâ: {args.quantity}")
        
        # ÌÖåÏä§Ìä∏ ÏãúÏûë ÏãúÍ∞Ñ Í∏∞Î°ù
        start_time = time.time()
        
        async with async_playwright() as p:
            # Use headless mode from args
            headless = True
            if hasattr(args, 'headless'):
                headless = args.headless
            else:
                headless = config.getboolean('Playwright','playwright_headless',fallback=True)
                
            logger.info(f"Î∏åÎùºÏö∞Ï†Ä Ïã§Ìñâ Ï§ë (headless: {headless})")
            
            browser_args = []
            try:
                browser_args_str = config.get('Playwright', 'playwright_browser_args', fallback='[]')
                import json
                browser_args = json.loads(browser_args_str)
            except Exception as arg_err:
                logger.warning(f"Î∏åÎùºÏö∞Ï†Ä Ïù∏Ïàò ÌååÏã± Ïò§Î•ò, Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©: {arg_err}")
                browser_args = ["--disable-gpu", "--disable-dev-shm-usage", "--no-sandbox"]
            
            # Î∏åÎùºÏö∞Ï†Ä Ïã§Ìñâ
            browser = await p.chromium.launch(
                headless=headless,
                args=browser_args,
                timeout=60000  # 1Î∂Ñ ÌÉÄÏûÑÏïÑÏõÉ
            )
            
            print(f"\n{'=' * 70}")
            print(f"üîç ÌÖåÏä§Ìä∏ Ïã§Ìñâ ÏàúÏÑú")
            print(f"{'=' * 70}")
            
            tests_to_run = []
            if args.test_type in ['all', 'images']:
                tests_to_run.append("1. Ïù¥ÎØ∏ÏßÄ URL Ï†ïÍ∑úÌôî Î∞è Îã§Ïö¥Î°úÎìú ÌÖåÏä§Ìä∏")
            if args.test_type in ['all', 'products']:
                tests_to_run.append("2. ÏÉÅÌíà Í≤ÄÏÉâ Î∞è Ï†ïÎ≥¥ Ï°∞Ìöå ÌÖåÏä§Ìä∏")
            if args.test_type in ['all', 'quantities']:
                tests_to_run.append("3. ÏàòÎüâÎ≥Ñ Í∞ÄÍ≤© Ï°∞Ìöå Î∞è Í∞ÄÍ≤© ÌÖåÏù¥Î∏î ÌÖåÏä§Ìä∏")
                
            for i, test in enumerate(tests_to_run):
                print(f"  {test}")
            
            # ÌÖåÏä§Ìä∏ Ïã§Ìñâ
            test_results = {}
            
            if args.test_type in ['all', 'images']:
                print(f"\n{'=' * 70}")
                print(f"üñºÔ∏è Ïù¥ÎØ∏ÏßÄ URL Ï†ïÍ∑úÌôî Î∞è Îã§Ïö¥Î°úÎìú ÌÖåÏä§Ìä∏ ÏãúÏûë")
                print(f"{'=' * 70}")
                
                img_test_start = time.time()
                test_image_download()
                img_test_time = time.time() - img_test_start
                test_results['images'] = {'time': img_test_time, 'status': 'completed'}
            
            if args.test_type in ['all', 'products']:
                print(f"\n{'=' * 70}")
                print(f"üìù ÏÉÅÌíà Í≤ÄÏÉâ Î∞è Ï†ïÎ≥¥ Ï°∞Ìöå ÌÖåÏä§Ìä∏ ÏãúÏûë")
                print(f"{'=' * 70}")
                
                prod_test_start = time.time()
                await test_product_info(browser)
                prod_test_time = time.time() - prod_test_start
                test_results['products'] = {'time': prod_test_time, 'status': 'completed'}
            
            if args.test_type in ['all', 'quantities']:
                print(f"\n{'=' * 70}")
                print(f"üìä ÏàòÎüâÎ≥Ñ Í∞ÄÍ≤© Ï°∞Ìöå Î∞è Í∞ÄÍ≤© ÌÖåÏù¥Î∏î ÌÖåÏä§Ìä∏ ÏãúÏûë")
                print(f"{'=' * 70}")
                
                qty_test_start = time.time()
                await test_custom_quantities(browser)
                qty_test_time = time.time() - qty_test_start
                test_results['quantities'] = {'time': qty_test_time, 'status': 'completed'}
                
            # Î∏åÎùºÏö∞Ï†Ä Ï¢ÖÎ£å
            logger.info("Î∏åÎùºÏö∞Ï†Ä Ï¢ÖÎ£å Ï§ë...")
            await browser.close()
            
            # ÌÖåÏä§Ìä∏ Í≤∞Í≥º ÏöîÏïΩ
            total_time = time.time() - start_time
            
            print(f"\n{'=' * 70}")
            print(f"üìã ÌÖåÏä§Ìä∏ Í≤∞Í≥º ÏöîÏïΩ")
            print(f"{'=' * 70}")
            print(f"Ï¥ù ÌÖåÏä§Ìä∏ Ïã§Ìñâ ÏãúÍ∞Ñ: {total_time:.2f}Ï¥à")
            
            if test_results:
                print(f"\nÏÑ∏Î∂Ä ÌÖåÏä§Ìä∏ Ïã§Ìñâ ÏãúÍ∞Ñ:")
                for test_name, result in test_results.items():
                    test_desc = {
                        'images': 'Ïù¥ÎØ∏ÏßÄ URL Î∞è Îã§Ïö¥Î°úÎìú ÌÖåÏä§Ìä∏',
                        'products': 'ÏÉÅÌíà Í≤ÄÏÉâ Î∞è Ï†ïÎ≥¥ Ï°∞Ìöå ÌÖåÏä§Ìä∏',
                        'quantities': 'ÏàòÎüâÎ≥Ñ Í∞ÄÍ≤© Ï°∞Ìöå ÌÖåÏä§Ìä∏'
                    }.get(test_name, test_name)
                    
                    print(f"  - {test_desc}: {result['time']:.2f}Ï¥à")
            
            print(f"\n‚úÖ ÌÖåÏä§Ìä∏ ÏôÑÎ£å")
            print(f"{'=' * 70}")

    # dispatch
    print(f"Test mode: {args.test_type}")
    print(f"Search terms: {args.search_terms}")
    print(f"Test quantities: {args.quantity}")
    if args.test_type == 'test2':
        # TODO: Implement run_test2 or remove this branch if not needed
        # import asyncio; asyncio.run(run_test2())
        logger.warning("Test type 'test2' selected but run_test2 is not implemented.")
    else:
        import asyncio; asyncio.run(run_standard_tests())

if __name__ == "__main__":
    # If this file is run directly, run the test
    if os.path.basename(__file__) == "crawling_kogift.py":
        # Setup basic logging FOR THE TEST ONLY
        # In production, logging is set up by initialize_environment
        logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
        logger.info("Running Kogift scraper test...")
        
        # Run the comprehensive test function
        test_kogift_scraper()

# --- Ìï¥Ïò§Î¶Ñ Í∏∞ÌîÑÌä∏ ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ÏóêÏÑú ÏàòÎüâ Ï∂îÏ∂ú Ìï®Ïàò ---
def extract_quantities_from_input(input_data: str) -> List[int]:
    """
    ÌÉ≠ÏúºÎ°ú Íµ¨Î∂ÑÎêú ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ÏóêÏÑú ÏàòÎüâ Ïª¨ÎüºÏùÑ Ï∞æÏïÑ Ïú†ÎãàÌÅ¨ ÏàòÎüâ Î¶¨Ïä§Ìä∏Î•º Î∞òÌôòÌï©ÎãàÎã§.
    """
    quantities = []
    if not input_data:
        return quantities
    lines = input_data.strip().split('\n')
    if len(lines) < 2:
        return quantities
    headers = lines[0].split('\t')
    qty_idx = next((i for i, h in enumerate(headers) if 'ÏàòÎüâ' in h), None)
    if qty_idx is None:
        return quantities
    for row in lines[1:]:
        cols = row.split('\t')
        if len(cols) > qty_idx:
            raw = ''.join(filter(str.isdigit, cols[qty_idx]))
            if raw:
                quantities.append(int(raw))
    return sorted(set(quantities))

# --- Ìï¥Ïò§Î¶Ñ Í∏∞ÌîÑÌä∏ ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ÏóêÏÑú ÏÉÅÌíàÎ™Ö/ÏàòÎüâ/Îã®Í∞Ä Ï∂îÏ∂ú Ìï®Ïàò ---
def extract_products_from_input(input_data: str) -> List[Dict[str, Any]]:
    """
    ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ÏóêÏÑú ÏÉÅÌíàÎ™Ö, ÏàòÎüâ, Îã®Í∞Ä Ïª¨ÎüºÏùÑ ÌååÏã±ÌïòÏó¨ ÎîïÏÖîÎÑàÎ¶¨ Î¶¨Ïä§Ìä∏Î°ú Î∞òÌôòÌï©ÎãàÎã§.
    """
    products = []
    if not input_data:
        return products
    lines = input_data.strip().split('\n')
    if len(lines) < 2:
        return products
    headers = lines[0].split('\t')
    idx_name = next((i for i,h in enumerate(headers) if 'ÏÉÅÌíàÎ™Ö' in h), None)
    idx_qty  = next((i for i,h in enumerate(headers) if 'ÏàòÎüâ' in h), None)
    idx_prc  = next((i for i,h in enumerate(headers) if 'Îã®Í∞Ä' in h or 'Í∞ÄÍ≤©' in h), None)
    if idx_name is None:
        return products
    for row in lines[1:]:
        cols = row.split('\t')
        if len(cols) <= idx_name:
            continue
        item = {'name': cols[idx_name].strip()}
        if idx_qty is not None and len(cols)>idx_qty:
            raw_q=''.join(filter(str.isdigit,cols[idx_qty])); item['quantity']=int(raw_q) if raw_q else None
        if idx_prc is not None and len(cols)>idx_prc:
            raw_p=''.join(filter(str.isdigit,cols[idx_prc])); item['price']=int(raw_p) if raw_p else None
        products.append(item)
    return products

