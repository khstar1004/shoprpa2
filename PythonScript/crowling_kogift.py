import asyncio
import pandas as pd
from playwright.async_api import Browser, Page, Error as PlaywrightError
import random
import logging
from urllib.parse import urlparse, urljoin
import re
from typing import Optional, List, Dict, Any, Tuple
from utils import generate_keyword_variations
import configparser # Import configparser
import os
import time
import httpx
import aiohttp
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import (
    TimeoutException, 
    NoSuchElementException, 
    StaleElementReferenceException,
    ElementClickInterceptedException,
    ElementNotInteractableException
)
import requests
from concurrent.futures import ThreadPoolExecutor
from image_downloader import predownload_kogift_images, verify_image_url
import argparse

# ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€ ê²½ë¡œ ì¤‘ìš” ì •ë³´:
# /ez/ ê²½ë¡œê°€ ì´ë¯¸ì§€ URLì— ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
# ì˜ëª»ëœ í˜•ì‹: https://koreagift.com/upload/mall/shop_1736386408518966_0.jpg
# ì˜¬ë°”ë¥¸ í˜•ì‹: https://koreagift.com/ez/upload/mall/shop_1736386408518966_0.jpg
# ìœ„ì˜ /ez/ ê²½ë¡œê°€ ì—†ìœ¼ë©´ ì´ë¯¸ì§€ ë¡œë“œê°€ ì‹¤íŒ¨í•˜ë¯€ë¡œ ëª¨ë“  ì´ë¯¸ì§€ URL ì²˜ë¦¬ ì‹œ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.

# ë¡œê±° ì„¤ì • (basicConfigëŠ” ë©”ì¸ì—ì„œ í•œ ë²ˆë§Œ í˜¸ì¶œí•˜ëŠ” ê²ƒì´ ì¢‹ìŒ)
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__) # Get logger instance

# Constants removed, now loaded from config
# KOGIFT_URLS = [...]
# USER_AGENT = "..."
# MIN_RESULTS_THRESHOLD = 5

# Add semaphore for concurrent task limiting
MAX_CONCURRENT_TASKS = 5
scraping_semaphore = asyncio.Semaphore(MAX_CONCURRENT_TASKS)

# --- Helper function to download images ---
def download_image(img_url, save_dir='downloaded_images', filename=None):
    """
    Download an image from URL and save it locally.
    
    Args:
        img_url: URL of the image to download
        save_dir: Directory to save the image
        filename: Optional filename; if None, will be derived from URL
        
    Returns:
        str: Path to the saved image or None if download failed
    """
    if not img_url:
        return None
        
    # Create save directory if it doesn't exist
    os.makedirs(save_dir, exist_ok=True)
    
    try:
        # Extract filename from URL if not provided
        if not filename:
            parsed_url = urlparse(img_url)
            filename = os.path.basename(parsed_url.path)
            
            # Ensure filename is valid
            if not filename or filename == '':
                # Generate random filename if URL doesn't have a valid one
                filename = f"image_{int(time.time())}_{random.randint(1000, 9999)}.jpg"
                
            # Ensure kogift images always have jpg extension
            is_kogift = "kogift" in img_url.lower() or "koreagift" in img_url.lower() or "adpanchok" in img_url.lower()
            if is_kogift or not filename.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp')):
                # Get base name without extension
                base_name = os.path.splitext(filename)[0]
                # Force jpg extension for kogift images
                filename = f"{base_name}.jpg"
        
        # Full path to save the image
        save_path = os.path.join(save_dir, filename)
        
        # Check if file already exists
        if os.path.exists(save_path):
            logger.debug(f"Image already exists at {save_path}")
            return save_path
            
        # Download image
        response = requests.get(img_url, stream=True, timeout=10)
        response.raise_for_status()  # Raise exception for HTTP errors
        
        # Check if response contains image data - more permissive check
        content_type = response.headers.get('Content-Type', '')
        logger.info(f"Content-Type for {img_url}: {content_type}")
        
        # We're being more permissive with content types since Kogift sometimes returns 
        # incorrect content types for images
        if content_type and 'text/html' in content_type and len(response.content) < 1000:
            logger.warning(f"URL likely returns HTML error page instead of image: {img_url}")
            return None
        
        # Save image
        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
                
        logger.info(f"Image downloaded successfully: {save_path}")
        return save_path
        
    except Exception as e:
        logger.error(f"Failed to download image from {img_url}: {e}")
        return None

def download_images_batch(img_urls, save_dir='downloaded_images', max_workers=10):
    """
    Download multiple images in parallel using a thread pool.
    
    Args:
        img_urls: List of image URLs to download
        save_dir: Directory to save the images
        max_workers: Maximum number of concurrent downloads
        
    Returns:
        dict: Mapping of URL to local file path for successful downloads
    """
    results = {}
    
    logger.info(f"Downloading {len(img_urls)} images to {save_dir}")
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_url = {
            executor.submit(download_image, url, save_dir): url 
            for url in img_urls if url
        }
        
        for future in future_to_url:
            url = future_to_url[future]
            try:
                path = future.result()
                if path:
                    results[url] = path
            except Exception as e:
                logger.error(f"Error downloading image {url}: {e}")
    
    success_count = len(results)
    logger.info(f"Downloaded {success_count}/{len(img_urls)} images successfully")
    
    return results

# --- Helper function to block unnecessary resources --- 
def should_block_request(url: str) -> bool:
    """Determines if a network request should be blocked."""
    # Block images (except product images if needed - needs more specific pattern), 
    # stylesheets, fonts, and common tracking/ad domains.
    # Adjust the patterns based on actual site structure and needs.
    # Example: Allow product images from koreagift main server
    if ("koreagift.com" in url or "adpanchok.co.kr" in url) and any(ext in url for ext in [".jpg", ".png", ".jpeg"]):
        # Add more specific logic if needed, e.g., checking path like '/goods/'
        return False # Don't block potential product images from main domains

    blocked_domains = ["google-analytics.com", "googletagmanager.com", "facebook.net", "adservice.google.com", "googlesyndication.com", "doubleclick.net"]
    parsed_url = urlparse(url)
    # Block specific resource types based on typical file extensions
    if any(parsed_url.path.lower().endswith(ext) for ext in [".css", ".woff", ".woff2", ".ttf", ".eot", ".gif", ".svg", ".webp"]):
        # Consider allowing specific crucial CSS if site breaks
        return True # Block styles/fonts/non-product images by default
    # Block based on domain
    if any(domain in parsed_url.netloc for domain in blocked_domains):
        logger.debug(f"Blocking request to tracking/ad domain: {url}")
        return True
    return False

async def setup_page_optimizations(page: Page):
    """Applies optimizations like request blocking to a Playwright page."""
    try:
        await page.route("**/*", lambda route:
            route.abort() if should_block_request(route.request.url) else route.continue_()
        )
        logger.debug("Applied network request blocking rules.")
    except Exception as e:
        logger.warning(f"Failed to set up page optimizations: {e}")

# --- ìƒì„¸ í˜ì´ì§€ì—ì„œ ìˆ˜ëŸ‰-ë‹¨ê°€ í…Œì´ë¸” ì¶”ì¶œ í•¨ìˆ˜ ì¶”ê°€ ---
async def extract_price_table(page, product_url, timeout=30000):
    """
    ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ì—ì„œ ìˆ˜ëŸ‰-ë‹¨ê°€ í…Œì´ë¸”ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        page: Playwright Page ê°ì²´
        product_url: ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ URL
        timeout: íƒ€ì„ì•„ì›ƒ(ms)
        
    Returns:
        DataFrame: ìˆ˜ëŸ‰-ë‹¨ê°€ ì •ë³´ê°€ ë‹´ê¸´ DataFrame ë˜ëŠ” None
    """
    try:
        await page.goto(product_url, wait_until='domcontentloaded', timeout=timeout)
        
        # ê³ ë ¤ê¸°í”„íŠ¸ ì‚¬ì´íŠ¸ì˜ ë‹¤ì–‘í•œ í…Œì´ë¸” ì„ íƒì
        table_selectors = [
            "table.quantity_price__table",  # ê³ ë ¤ê¸°í”„íŠ¸ ìˆ˜ëŸ‰-ë‹¨ê°€ í…Œì´ë¸”
            "div.product_table table",      # ê³ ë ¤ê¸°í”„íŠ¸ ìƒí’ˆ í…Œì´ë¸”
            "table.detail_table",           # ì¼ë°˜ì ì¸ ìƒì„¸ í…Œì´ë¸”
            "div.detail_price table",       # ê°€ê²© ì •ë³´ í…Œì´ë¸”
            "div.goods_detail table"        # ìƒí’ˆ ìƒì„¸ í…Œì´ë¸”
        ]
        
        # ê³ ë ¤ê¸°í”„íŠ¸ íŠ¹ìœ ì˜ í…Œì´ë¸” êµ¬ì¡° ì²˜ë¦¬
        kogift_selector = "table.quantity_price__table"
        if await page.locator(kogift_selector).count() > 0:
            # ìˆ˜ëŸ‰ í–‰ê³¼ ê°€ê²© í–‰ì´ ê°ê° ë³„ë„ í–‰ì— ìˆëŠ” íŠ¹ë³„í•œ êµ¬ì¡° ì²˜ë¦¬
            qty_cells = await page.locator(f"{kogift_selector} tr:first-child td").all()
            price_cells = await page.locator(f"{kogift_selector} tr:nth-child(2) td").all()
            
            # ì²« ë²ˆì§¸ ì—´ì€ í—¤ë”ì´ë¯€ë¡œ ì œì™¸ (ìˆ˜ëŸ‰, ë‹¨ê°€ ë¼ëŠ” í…ìŠ¤íŠ¸ê°€ ìˆìŒ)
            quantities = []
            prices = []
            
            # ìˆ˜ëŸ‰ í–‰ ì¶”ì¶œ
            for i, cell in enumerate(qty_cells):
                if i > 0:  # ì²« ë²ˆì§¸ ì—´(í—¤ë”) ê±´ë„ˆë›°ê¸°
                    qty_text = await cell.text_content()
                    # ìˆ˜ëŸ‰ì—ì„œ ì‰¼í‘œ ì œê±°í•˜ê³  ìˆ«ìë§Œ ì¶”ì¶œ
                    qty_clean = ''.join(filter(str.isdigit, qty_text.replace(',', '')))
                    if qty_clean:
                        quantities.append(int(qty_clean))
            
            # ê°€ê²© í–‰ ì¶”ì¶œ
            for i, cell in enumerate(price_cells):
                if i > 0:  # ì²« ë²ˆì§¸ ì—´(í—¤ë”) ê±´ë„ˆë›°ê¸°
                    price_text = await cell.text_content()
                    # ê°€ê²©ì—ì„œ ì‰¼í‘œ ì œê±°í•˜ê³  ìˆ«ìë§Œ ì¶”ì¶œ
                    price_clean = ''.join(filter(str.isdigit, price_text.replace(',', '')))
                    if price_clean:
                        prices.append(int(price_clean))
            
            # ìœ íš¨í•œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
            if quantities and prices and len(quantities) == len(prices):
                # DataFrame ìƒì„±
                result_df = pd.DataFrame({
                    'ìˆ˜ëŸ‰': quantities,
                    'ë‹¨ê°€': prices
                })
                
                # ë¶€ê°€ì„¸ ì •ë³´ í™•ì¸
                vat_info = await page.locator("div.quantity_price__wrapper div:last-child").text_content()
                has_vat = "ë¶€ê°€ì„¸ë³„ë„" in vat_info or "ë¶€ê°€ì„¸ ë³„ë„" in vat_info
                
                # ë¶€ê°€ì„¸ ë³„ë„ë¼ë©´ ë©”íƒ€ë°ì´í„°ë¡œ ì¶”ê°€
                if has_vat:
                    result_df.attrs['vat_excluded'] = True
                
                # ìˆ˜ëŸ‰ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
                result_df = result_df.sort_values('ìˆ˜ëŸ‰')
                return result_df
        
        # ë‹¤ë¥¸ ì„ íƒì ì‹œë„
        for selector in table_selectors:
            # ì´ë¯¸ ì²˜ë¦¬í•œ ì„ íƒì ê±´ë„ˆë›°ê¸°
            if selector == kogift_selector:
                continue
                
            if await page.locator(selector).count() > 0:
                try:
                    # í…Œì´ë¸” HTML ê°€ì ¸ì˜¤ê¸°
                    table_html = await page.locator(selector).first.inner_html()
                    
                    # í…Œì´ë¸”ì„ pandas DataFrameìœ¼ë¡œ íŒŒì‹±
                    tables = pd.read_html("<table>" + table_html + "</table>")
                    if not tables:
                        continue
                    
                    table_df = tables[0]
                    
                    # í…Œì´ë¸”ì´ ìˆ˜ëŸ‰-ë‹¨ê°€ ì •ë³´ì¸ì§€ í™•ì¸
                    if len(table_df.columns) >= 2:
                        # ì»¬ëŸ¼ëª…ì— 'ìˆ˜ëŸ‰', 'ê°€ê²©', 'ë‹¨ê°€' ë“±ì˜ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                        col_names = [str(col).lower() for col in table_df.columns]
                        qty_keywords = ['ìˆ˜ëŸ‰', 'qty', 'ê°œìˆ˜', 'ê°¯ìˆ˜']
                        price_keywords = ['ê°€ê²©', 'ë‹¨ê°€', 'ê¸ˆì•¡', 'price']
                        
                        qty_col = None
                        price_col = None
                        
                        # ìˆ˜ëŸ‰ ì»¬ëŸ¼ ì°¾ê¸°
                        for i, col in enumerate(col_names):
                            if any(keyword in col for keyword in qty_keywords):
                                qty_col = i
                                break
                        
                        # ê°€ê²© ì»¬ëŸ¼ ì°¾ê¸°
                        for i, col in enumerate(col_names):
                            if any(keyword in col for keyword in price_keywords):
                                price_col = i
                                break
                        
                        # ì»¬ëŸ¼ëª…ì—ì„œ ì°¾ì§€ ëª»í–ˆë‹¤ë©´ ì²« ë²ˆì§¸, ë‘ ë²ˆì§¸ ì»¬ëŸ¼ìœ¼ë¡œ ê°€ì •
                        if qty_col is None and price_col is None and len(table_df.columns) >= 2:
                            # ì²« ë²ˆì§¸ í–‰ì— ìˆ˜ëŸ‰, ë‹¨ê°€ ë“±ì˜ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                            if not table_df.empty:
                                first_row = table_df.iloc[0]
                                for i, value in enumerate(first_row):
                                    value_str = str(value).lower()
                                    if any(keyword in value_str for keyword in qty_keywords):
                                        qty_col = i
                                    if any(keyword in value_str for keyword in price_keywords):
                                        price_col = i
                            
                            # ê·¸ë˜ë„ ëª» ì°¾ì•˜ë‹¤ë©´ ì²« ë²ˆì§¸ì™€ ë‘ ë²ˆì§¸ ì»¬ëŸ¼ ì‚¬ìš©
                            if qty_col is None and price_col is None:
                                qty_col = 0
                                price_col = 1
                        
                        if qty_col is not None and price_col is not None:
                            # ìˆ˜ëŸ‰-ê°€ê²© í…Œì´ë¸” í™•ì¸ë¨
                            # ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½
                            result_df = table_df.copy()
                            new_cols = result_df.columns.tolist()
                            
                            # ì²« ë²ˆì§¸ í–‰ì´ í—¤ë”ì¸ ê²½ìš° ì²˜ë¦¬
                            if any(keyword in str(result_df.iloc[0, qty_col]).lower() for keyword in qty_keywords) and \
                               any(keyword in str(result_df.iloc[0, price_col]).lower() for keyword in price_keywords):
                                # ì²« ë²ˆì§¸ í–‰ì„ ì œì™¸í•˜ê³  ì²˜ë¦¬
                                result_df = result_df.iloc[1:].copy()
                            
                            # ì»¬ëŸ¼ëª… ì¬ì§€ì •
                            new_cols = result_df.columns.tolist()
                            new_cols[qty_col] = 'ìˆ˜ëŸ‰'
                            new_cols[price_col] = 'ë‹¨ê°€'
                            result_df.columns = new_cols
                            
                            # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
                            result_df = result_df[['ìˆ˜ëŸ‰', 'ë‹¨ê°€']]
                            
                            # ë°ì´í„° ì •ì œ
                            result_df['ìˆ˜ëŸ‰'] = result_df['ìˆ˜ëŸ‰'].astype(str).apply(
                                lambda x: ''.join(filter(str.isdigit, str(x).replace(',', '')))
                            )
                            result_df['ë‹¨ê°€'] = result_df['ë‹¨ê°€'].astype(str).apply(
                                lambda x: ''.join(filter(str.isdigit, str(x).replace(',', '')))
                            )
                            
                            # ìˆ«ìë¡œ ë³€í™˜ ê°€ëŠ¥í•œ í–‰ë§Œ ìœ ì§€
                            result_df = result_df[result_df['ìˆ˜ëŸ‰'].apply(lambda x: x.isdigit())]
                            result_df = result_df[result_df['ë‹¨ê°€'].apply(lambda x: x.isdigit())]
                            
                            # ë°ì´í„° íƒ€ì… ë³€í™˜
                            result_df['ìˆ˜ëŸ‰'] = result_df['ìˆ˜ëŸ‰'].astype(int)
                            result_df['ë‹¨ê°€'] = result_df['ë‹¨ê°€'].astype(int)
                            
                            # ìˆ˜ëŸ‰ ê¸°ì¤€ ì •ë ¬
                            result_df = result_df.sort_values('ìˆ˜ëŸ‰')
                            
                            if not result_df.empty:
                                return result_df
                except Exception as table_error:
                    # í…Œì´ë¸” íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ ì„ íƒìë¡œ ì§„í–‰
                    continue
        
        # ì…€ë ‰íŠ¸ ë°•ìŠ¤ì—ì„œ ë‹¨ê°€ ì •ë³´ ì°¾ê¸°
        option_selector = "select[name='chadung_list'] option"
        if await page.locator(option_selector).count() > 0:
            options = await page.locator(option_selector).all()
            
            quantities = []
            prices = []
            
            for option in options:
                value = await option.get_attribute('value')
                text = await option.text_content()
                
                # ìƒí’ˆ ì„ íƒ ì•ˆë‚´ ì˜µì…˜ ìŠ¤í‚µ
                if not value or "ì„ íƒí•´ ì£¼ì„¸ìš”" in text or "----------" in text:
                    continue
                
                # ë‹¨ê°€ ì •ë³´ê°€ ìˆëŠ” ì˜µì…˜ ì²˜ë¦¬
                if "ë‹¨ê°€::" in value:
                    parts = value.split('|^|')
                    if len(parts) >= 3:
                        qty_part = parts[0].replace('ë‹¨ê°€::', '')
                        price_part = parts[1]
                        
                        # ìˆ˜ëŸ‰ê³¼ ê°€ê²© ì¶”ì¶œ
                        if qty_part.isdigit() and price_part.isdigit():
                            quantities.append(int(qty_part))
                            prices.append(int(price_part))
            
            # ìœ íš¨í•œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
            if quantities and prices:
                # DataFrame ìƒì„±
                result_df = pd.DataFrame({
                    'ìˆ˜ëŸ‰': quantities,
                    'ë‹¨ê°€': prices
                })
                
                # ìˆ˜ëŸ‰ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
                result_df = result_df.sort_values('ìˆ˜ëŸ‰')
                return result_df
        
        # í…Œì´ë¸”ì„ ì°¾ì§€ ëª»í•¨
        return None
        
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ None ë°˜í™˜
        return None

# --- ì´ë¯¸ì§€ URL ì²˜ë¦¬ ì „ìš© í•¨ìˆ˜ ì¶”ê°€ ---
def normalize_kogift_image_url(img_url: str, base_url: str = "https://www.kogift.com") -> Tuple[str, bool]:
    """
    ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€ URLì„ í‘œì¤€í™”í•˜ê³  ìœ íš¨ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤.
    
    Args:
        img_url: ì›ë³¸ ì´ë¯¸ì§€ URL ë˜ëŠ” ê²½ë¡œ
        base_url: ê¸°ë³¸ ë„ë©”ì¸ URL
        
    Returns:
        Tuple[str, bool]: ì •ê·œí™”ëœ ì´ë¯¸ì§€ URLê³¼ ìœ íš¨ì„± ì—¬ë¶€
    """
    if not img_url:
        return "", False
    
    # data:image URIì¸ ê²½ìš° (ì¸ë¼ì¸ ì´ë¯¸ì§€)
    if img_url.startswith('data:image/'):
        logger.warning(f"Data URI ì´ë¯¸ì§€ ë°œê²¬ (ì‚¬ìš© ë¶ˆê°€)")
        return "", False
    
    # ì´ë¯¸ ì™„ì „í•œ URLì¸ ê²½ìš°
    if img_url.startswith(('http://', 'https://')):
        parsed_url = urlparse(img_url)
        domain = parsed_url.netloc
        path = parsed_url.path
        
        # koreagift.com ë„ë©”ì¸ì¸ ê²½ìš° í•­ìƒ /ez/ ê²½ë¡œê°€ ìˆëŠ”ì§€ í™•ì¸
        if 'koreagift.com' in domain:
            # ì´ë¯¸ /ez/ê°€ ìˆëŠ” ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if '/ez/' in path:
                return img_url, True
            # /upload/ë¡œ ì‹œì‘í•˜ëŠ” ê²½ë¡œì— /ez/ ì¶”ê°€
            elif path.startswith('/upload/'):
                new_path = '/ez' + path
                return f"{parsed_url.scheme}://{domain}{new_path}", True
            # ê·¸ ì™¸ ê²½ë¡œëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©
            else:
                return img_url, True
        
        # ìœ íš¨í•œ ë„ë©”ì¸ í™•ì¸
        kogift_domains = ['kogift.com', 'www.kogift.com', 'img.kogift.com', 'adpanchok.co.kr', 'www.adpanchok.co.kr']
        if any(kogift_domain in domain for kogift_domain in kogift_domains):
            return img_url, True
        else:
            # ë‹¤ë¥¸ ë„ë©”ì¸ì´ë©´ ê¸°ì¡´ URL ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ë˜ ìœ íš¨í•˜ì§€ ì•ŠìŒ í‘œì‹œ
            return img_url, False
    
    # '//' ì‹œì‘í•˜ëŠ” í”„ë¡œí† ì½œ-ìƒëŒ€ URL ì²˜ë¦¬
    if img_url.startswith('//'):
        return f"https:{img_url}", True
    
    # './ì›¹ ê²½ë¡œ' ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
    if img_url.startswith('./'):
        img_url = img_url[2:]  # './' ì œê±°
    
    # ì ˆëŒ€ ê²½ë¡œ('/upload/'ë¡œ ì‹œì‘)ì¸ ê²½ìš°
    if img_url.startswith('/upload/'):
        # koreagift.com ë„ë©”ì¸ì— ëŒ€í•´ì„œëŠ” í•­ìƒ /ez/ ê²½ë¡œ ì¶”ê°€
        if 'koreagift.com' in base_url:
            img_url = '/ez' + img_url
    # ê¸°íƒ€ ì ˆëŒ€ ê²½ë¡œ
    elif img_url.startswith('/'):
        # ê·¸ëŒ€ë¡œ ì‚¬ìš©
        pass
    # ìƒëŒ€ ê²½ë¡œ(íŒŒì¼ëª… ë˜ëŠ” í•˜ìœ„ ê²½ë¡œ)
    else:
        # ê²½ë¡œê°€ 'upload/'ë¡œ ì‹œì‘í•˜ë©´ ì•ì— '/'ë¥¼ ì¶”ê°€
        if img_url.startswith('upload/'):
            # koreagift.com ë„ë©”ì¸ì— ëŒ€í•´ì„œëŠ” í•­ìƒ /ez/ ê²½ë¡œ ì¶”ê°€
            if 'koreagift.com' in base_url:
                img_url = '/ez/' + img_url
            else:
                img_url = '/' + img_url
        # ê¸°íƒ€ ê²½ë¡œëŠ” ê·¸ëŒ€ë¡œ /ë¥¼ ë¶™ì—¬ì„œ ì‚¬ìš©
        else:
            img_url = '/' + img_url
    
    # ìµœì¢… URL ìƒì„±
    final_url = urljoin(base_url, img_url)
    
    # ì¤‘ë³µ ê²½ë¡œ í™•ì¸ ë° ìˆ˜ì •
    if '/ez/ez/' in final_url:
        final_url = final_url.replace('/ez/ez/', '/ez/')
    
    return final_url, True

async def verify_kogift_images(product_list: List[Dict], sample_percent: int = 10) -> List[Dict]:
    """
    ê³ ë ¤ê¸°í”„íŠ¸ ìƒí’ˆ ëª©ë¡ì˜ ì´ë¯¸ì§€ URLì„ ê²€ì¦í•˜ê³  í‘œì¤€í™”í•œ í›„, ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        product_list: ìƒí’ˆ ëª©ë¡ (ê° í•­ëª©ì€ 'image' ë˜ëŠ” 'image_url' í‚¤ë¥¼ í¬í•¨í•´ì•¼ í•¨)
        sample_percent: ì „ì²´ URL ì¤‘ ì‹¤ì œë¡œ ê²€ì¦í•  ë¹„ìœ¨ (%)
        
    Returns:
        List[Dict]: ì´ë¯¸ì§€ URLì´ í‘œì¤€í™”ë˜ê³  ë¡œì»¬ ì´ë¯¸ì§€ ê²½ë¡œê°€ ì¶”ê°€ëœ ìƒí’ˆ ëª©ë¡
    """
    if not product_list:
        return []
    
    # ì„¤ì •ì—ì„œ ê²€ì¦ ì—¬ë¶€ í™•ì¸
    config = configparser.ConfigParser()
    config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'config.ini')
    config.read(config_path, encoding='utf-8')
    
    verify_enabled = config.getboolean('Matching', 'verify_image_urls', fallback=True)
    download_enabled = config.getboolean('Matching', 'download_images', fallback=True)
    images_dir = config.get('Matching', 'images_dir', fallback='downloaded_images')
    
    logger.info(f"ê³ ë ¤ê¸°í”„íŠ¸ ìƒí’ˆ {len(product_list)}ê°œì˜ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œì‘")
    
    # ì´ë¯¸ì§€ URL í‘œì¤€í™”
    for product in product_list:
        # 'image' ë˜ëŠ” 'image_url' í‚¤ì—ì„œ ì´ë¯¸ì§€ URL ì°¾ê¸°
        img_url = product.get('image') or product.get('image_url') or product.get('src')
        if img_url:
            product['original_image'] = img_url  # ì›ë³¸ URL ë°±ì—…
            
            # URL í‘œì¤€í™”
            normalized_url, is_valid = normalize_kogift_image_url(img_url)
            
            if normalized_url:
                # í‘œì¤€í™”ëœ URL ì €ì¥
                product['image'] = normalized_url
                product['image_url'] = normalized_url  # í˜¸í™˜ì„± ìœ ì§€
                product['src'] = normalized_url  # í˜¸í™˜ì„± ìœ ì§€
            else:
                # ìœ íš¨í•˜ì§€ ì•Šì€ URLì€ ë¹ˆ ë¬¸ìì—´ë¡œ í‘œì‹œ
                product['image'] = ""
                product['image_url'] = ""
                product['src'] = ""
    
    # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì²˜ë¦¬
    if download_enabled:
        # ìœ íš¨í•œ ì´ë¯¸ì§€ URLë§Œ ìˆ˜ì§‘
        valid_urls = []
        url_to_product_map = {}
        
        for product in product_list:
            img_url = product.get('image')
            if img_url:
                valid_urls.append(img_url)
                url_to_product_map[img_url] = product
        
        logger.info(f"ì´ {len(valid_urls)}ê°œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
        
        # ì´ë¯¸ì§€ ì¼ê´„ ë‹¤ìš´ë¡œë“œ
        downloaded_images = download_images_batch(valid_urls, save_dir=images_dir)
        
        # ë‹¤ìš´ë¡œë“œëœ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ì œí’ˆ ë°ì´í„°ì— ì¶”ê°€
        for url, local_path in downloaded_images.items():
            if url in url_to_product_map:
                url_to_product_map[url]['local_image_path'] = local_path
        
        logger.info(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(downloaded_images)}/{len(valid_urls)} ì„±ê³µ")
    
    # ìƒ˜í”Œë§ ë¹„ìœ¨ì— ë”°ë¼ URL ê²€ì¦ (ê¸°ì¡´ ì½”ë“œëŠ” ì£¼ì„ ì²˜ë¦¬)
    if verify_enabled and sample_percent > 0 and not download_enabled:
        # ì´ë¯¸ì§€ê°€ ìˆëŠ” ìƒí’ˆë§Œ ì„ íƒ
        products_with_images = [p for p in product_list if p.get('image')]
        if not products_with_images:
            return product_list
            
        # ê²€ì¦í•  ìƒí’ˆ ìƒ˜í”Œë§
        sample_size = max(1, int(len(products_with_images) * sample_percent / 100))
        sample_products = random.sample(products_with_images, min(sample_size, len(products_with_images)))
        
        logger.info(f"{sample_percent}% ìƒ˜í”Œë§ìœ¼ë¡œ {len(sample_products)}ê°œ ì´ë¯¸ì§€ URL ê²€ì¦ ì‹œì‘")
        
        # ê²€ì¦ ê²°ê³¼ ì¹´ìš´íŒ…
        verified_count = 0
        failed_count = 0
        
        # ë¹„ë™ê¸° ì„¸ì…˜ ìƒì„±
        async with aiohttp.ClientSession() as session:
            for product in sample_products:
                img_url = product['image']
                if not img_url:
                    continue
                
                # ì´ë¯¸ì§€ URL ì‹¤ì œ ì ‘ê·¼ ê²€ì¦
                url, is_valid, reason = await verify_image_url(session, img_url)
                
                if is_valid:
                    verified_count += 1
                else:
                    failed_count += 1
                    # koreagift.com ì‹¤íŒ¨ URL ì²˜ë¦¬
                    if 'koreagift.com' in img_url and is_valid == False:
                        # URLì„ ê³ ì³ë„ ì‹¤íŒ¨í•  ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
                        pass
        
        logger.info(f"ì´ë¯¸ì§€ URL ê²€ì¦ ê²°ê³¼: ì„±ê³µ {verified_count}, ì‹¤íŒ¨ {failed_count}")
    
    return product_list

# --- Main scraping functionì— ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ë¡œì§ ì¶”ê°€ --- 
async def scrape_data(browser: Browser, original_keyword1: str, original_keyword2: Optional[str] = None, config: configparser.ConfigParser = None, fetch_price_tables: bool = False):
    """Scrape product data from Koreagift using a shared Browser instance.
    
    Args:
        browser: An active Playwright Browser instance.
        original_keyword1: The primary keyword to search for.
        original_keyword2: An optional secondary keyword for re-search if results >= 100.
        config: ConfigParser object containing configuration settings.
        fetch_price_tables: ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ì—ì„œ ìˆ˜ëŸ‰-ë‹¨ê°€ ì •ë³´ë„ í•¨ê»˜ ê°€ì ¸ì˜¬ì§€ ì—¬ë¶€

    Returns:
        A pandas DataFrame containing the best found results, or an empty DataFrame.
    """
    async with scraping_semaphore:  # Acquire semaphore before starting
        if config is None:
            logger.error("ğŸ”´ Configuration object (ConfigParser) is missing for Kogift scrape.")
            return pd.DataFrame() # Return empty dataframe on critical config error
        
        # Get settings from config with defaults using ConfigParser methods
        try:
            kogift_urls_str = config.get('ScraperSettings', 'kogift_urls', 
                                       fallback='https://koreagift.com/ez/index.php,https://adpanchok.co.kr/ez/index.php')
            kogift_urls = [url.strip() for url in kogift_urls_str.split(',') if url.strip()]
            if not kogift_urls:
                 logger.error("ğŸ”´ Kogift URLs are missing or invalid in [ScraperSettings] config.")
                 return pd.DataFrame()
            
            user_agent = config.get('ScraperSettings', 'user_agent', 
                                  fallback='Mozilla/5.0 ...') # Use actual default from utils/DEFAULT_CONFIG if desired
            min_results_threshold = config.getint('ScraperSettings', 'kogift_min_results_threshold', fallback=5)
            max_items_to_scrape = config.getint('ScraperSettings', 'kogift_max_items', fallback=200)
            max_pages_to_scrape = config.getint('ScraperSettings', 'kogift_max_pages', fallback=10)
            
            default_timeout = config.getint('Playwright', 'playwright_default_timeout_ms', fallback=120000)  # 2ë¶„
            navigation_timeout = config.getint('Playwright', 'playwright_navigation_timeout_ms', fallback=120000)  # 2ë¶„
            action_timeout = config.getint('Playwright', 'playwright_action_timeout_ms', fallback=30000)  # 30ì´ˆ
            # Add a shorter timeout specifically for waiting for search results/no results
            search_results_wait_timeout = config.getint('Playwright', 'playwright_search_results_timeout_ms', fallback=60000)  # 1ë¶„
            block_resources = config.getboolean('Playwright', 'playwright_block_resources', fallback=True)
            
            # Image download settings
            download_images = config.getboolean('Matching', 'download_images', fallback=True)
            images_dir = config.get('Matching', 'images_dir', fallback='downloaded_images')
        except (configparser.NoSectionError, configparser.NoOptionError, ValueError) as e:
            logger.error(f"ğŸ”´ Error reading Kogift/Playwright config: {e}. Using hardcoded defaults where possible.")
            # Set critical defaults again or decide to return empty
            kogift_urls = ["https://koreagift.com/ez/index.php", "https://adpanchok.co.kr/ez/index.php"]
            user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36"
            min_results_threshold = 5
            max_items_to_scrape = 200
            max_pages_to_scrape = 10
            default_timeout = 120000
            navigation_timeout = 120000
            action_timeout = 30000
            search_results_wait_timeout = 60000
            block_resources = True
            download_images = True
            images_dir = 'downloaded_images'

        keywords_to_try = generate_keyword_variations(original_keyword1)
        best_result_df = pd.DataFrame() 

        logger.info(f"ğŸ” Generated keywords for '{original_keyword1}': {keywords_to_try}")

        # Shared browser context for this scrape attempt
        context = None
        page = None
        try:
            context = await browser.new_context(user_agent=user_agent)
            page = await context.new_page()
            page.set_default_timeout(default_timeout)
            page.set_default_navigation_timeout(navigation_timeout)
            
            if block_resources:
                await setup_page_optimizations(page)

            for keyword in keywords_to_try:
                logger.info(f"ğŸ” Trying keyword variation: '{keyword}' --- ({keywords_to_try.index(keyword) + 1}/{len(keywords_to_try)}) ---")
                current_keyword_best_df = pd.DataFrame()
                keyword_found_sufficient = False

                for base_url in kogift_urls:
                    logger.info(f"ğŸŒ Attempting scrape: URL='{base_url}', Keyword='{keyword}'")
                    data = []
                    page_instance = page
                    
                    try:
                        # Navigate to the base URL
                        await page_instance.goto(base_url, wait_until='domcontentloaded')
                        logger.debug(f"ğŸŒ Navigated to {base_url}")

                        # --- Perform Search --- 
                        search_input_locator = page_instance.locator('input#main_keyword[name="keyword"]') # More specific selector
                        search_button_locator = page_instance.locator('img#search_submit')
                        
                        await search_input_locator.wait_for(state="visible", timeout=action_timeout)
                        await search_input_locator.fill(keyword)
                        await search_button_locator.wait_for(state="visible", timeout=action_timeout)
                        
                        results_container_selector = 'div.product_lists' # Selector for the container holding results
                        # Refined selector for "no results" message based on provided HTML
                        no_results_selector = 'div.not_result span.icon_dot2:has-text("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")' 
                        combined_selector = f"{results_container_selector}, {no_results_selector}"
                        
                        logger.debug("ğŸ” Clicking search...")
                        await search_button_locator.click()
                        logger.info(f"ğŸ” Search submitted for: '{keyword}' on {base_url}")

                        # --- Wait for results OR "no results" message --- 
                        logger.debug(f"â³ Waiting for search results or 'no results' message (timeout: {search_results_wait_timeout}ms)...")
                        try:
                            found_element = await page_instance.wait_for_selector(
                                combined_selector, 
                                state='visible', 
                                timeout=search_results_wait_timeout
                            )
                            
                            # Check if the 'no results' text is visible
                            no_results_element = page_instance.locator(no_results_selector)
                            if await no_results_element.is_visible():
                                no_results_text = await no_results_element.first.text_content(timeout=1000) or "[No text found]"
                                logger.info(f"âš ï¸ 'No results' message found for keyword '{keyword}' on {base_url}. Text: {no_results_text.strip()}")
                                continue # Skip to the next URL/keyword
                            else:
                                logger.debug("âœ… Results container found. Proceeding to scrape.")
                                # Results container is visible, fall through to scraping logic
                                pass 
                                
                        except PlaywrightError as wait_error:
                            logger.warning(f"âš ï¸ Timeout or error waiting for results/no_results for keyword '{keyword}' on {base_url}: {wait_error}")
                            continue # Skip to the next URL/keyword

                        # --- Check product count (Optional Re-search) --- 
                        # This section remains largely the same, but runs ONLY if results were found
                        productCont = 0
                        try:
                            product_count_element = page_instance.locator('div.list_info span').first # Simpler selector
                            productContText = await product_count_element.text_content(timeout=5000) 
                            productContDigits = re.findall(r'\d+', productContText.replace(',', ''))
                            if productContDigits:
                                productCont = int("".join(productContDigits))
                            logger.info(f"ğŸ“Š Reported product count: {productCont}")
                        except (PlaywrightError, Exception) as e:
                            logger.warning(f"âš ï¸ Could not find/parse product count: {e}")

                        # Re-search logic (only if initial search had results)
                        if original_keyword2 and original_keyword2.strip() != "" and productCont >= 100:
                            logger.info(f"ğŸ” Initial count >= 100. Performing re-search with: '{original_keyword2}'")
                            try:
                                re_search_input = page_instance.locator('input#re_keyword')
                                re_search_button = page_instance.locator('button[onclick^="re_search"]')
                                await re_search_input.fill(original_keyword2)
                                
                                logger.debug("ğŸ” Clicking re-search...")
                                await re_search_button.click()
                                
                                # Wait again after re-search, checking for no results again
                                logger.debug(f"â³ Waiting after re-search (timeout: {search_results_wait_timeout}ms)...")
                                try:
                                    await page_instance.wait_for_selector(
                                        combined_selector, 
                                        state='visible', 
                                        timeout=search_results_wait_timeout
                                    )
                                    if await page_instance.locator(no_results_selector).is_visible():
                                         logger.info(f"âš ï¸ 'No results' found after re-searching with '{original_keyword2}'.")
                                         # Decide whether to break or continue based on re-search logic
                                         # For now, let's assume re-search failure means stop for this URL
                                         continue # Skip to next URL
                                    else:
                                         logger.info(f"âœ… Re-search completed for: '{original_keyword2}'. Proceeding with scraping new results.")
                                         # Reset page number and counts for scraping re-search results
                                         page_number = 1
                                         processed_items = 0
                                         data = [] # Clear previous data if re-search successful
                                except PlaywrightError as re_wait_error:
                                    logger.warning(f"âš ï¸ Timeout/error waiting for results after re-search with '{original_keyword2}': {re_wait_error}")
                                    continue # Skip to next URL
                                    
                            except (PlaywrightError, Exception) as e:
                                logger.warning(f"âš ï¸ Failed during re-search attempt: {e}")
                                # Continue with initial results if re-search fails here.

                        # --- Scrape Results Pages --- 
                        page_number = 1
                        processed_items = 0
                        product_item_selector = 'div.product' # Selector for individual product blocks

                        while processed_items < max_items_to_scrape and page_number <= max_pages_to_scrape:
                            logger.info(f"ğŸ“„ Scraping page {page_number} (Keyword: '{keyword}', URL: {base_url})... Items processed: {processed_items}")
                            try:
                                 # Wait for at least one product item to be potentially visible
                                 await page_instance.locator(product_item_selector).first.wait_for(state="attached", timeout=action_timeout)
                            except PlaywrightError:
                                 logger.warning(f"âš ï¸ Product items selector ('{product_item_selector}') not found/attached on page {page_number}. Stopping scrape for this URL/Keyword.")
                                 break
                                 
                            rows = page_instance.locator(product_item_selector)
                            count = await rows.count()
                            logger.debug(f"ğŸ“Š Found {count} product elements on page {page_number}.")

                            if count == 0 and page_number > 1: # Allow page 1 to have 0 if count check failed earlier
                                 logger.info(f"âš ï¸ No product elements found on page {page_number}. Stopping pagination.")
                                 break
                            elif count == 0 and page_number == 1:
                                 logger.info(f"âš ï¸ No product elements found on first page (page {page_number}). Stopping scrape for this URL/Keyword.")
                                 break

                            items_on_page = []
                            for i in range(count):
                                if processed_items >= max_items_to_scrape:
                                    break
                                row = rows.nth(i)
                                item_data = {}
                                try:
                                    # Extract data using locators with short timeouts
                                    img_locator = row.locator('div.pic > a > img')
                                    img_src = await img_locator.get_attribute('src', timeout=action_timeout)
                                    
                                    link_locator = row.locator('div.pic > a')
                                    a_href = await link_locator.get_attribute('href', timeout=action_timeout)
                                    
                                    name_locator = row.locator('div.name > a')
                                    name = await name_locator.text_content(timeout=action_timeout)
                                    
                                    price_locator = row.locator('div.price')
                                    price_text = await price_locator.text_content(timeout=action_timeout)

                                    # Process extracted data
                                    base_domain_url = f"{urlparse(base_url).scheme}://{urlparse(base_url).netloc}"
                                    
                                    # ë””ë²„ê¹…: ì›ë³¸ URL ë° ë³€í™˜ ê³¼ì • ë¡œê¹…
                                    logger.debug(f"ğŸ”— Raw image src: {img_src}")
                                    logger.debug(f"ğŸ”— Raw product href: {a_href}")
                                    logger.debug(f"ğŸŒ Base domain URL: {base_domain_url}")
                                    
                                    # ì´ë¯¸ì§€ URL ì²˜ë¦¬
                                    if img_src:
                                        # ì´ë¯¸ì§€ ì†ŒìŠ¤ ì²˜ë¦¬
                                        if img_src.startswith('http'):
                                            # ì´ë¯¸ ì™„ì „í•œ URLì¸ ê²½ìš°
                                            processed_img_src = img_src
                                        elif img_src.startswith('./'):
                                            # './ë¡œ ì‹œì‘í•˜ëŠ” ìƒëŒ€ ê²½ë¡œë¥¼ /ez/ë¡œ ë³€í™˜ (koreagift.com)
                                            if 'koreagift.com' in base_domain_url:
                                                processed_img_src = '/ez/' + img_src[2:]  # './' ì œê±°í•˜ê³  /ez/ ì¶”ê°€
                                            else:
                                                processed_img_src = '/' + img_src[2:]  # './' ì œê±°
                                        elif img_src.startswith('/upload/'):
                                            # /upload/ë¡œ ì‹œì‘í•˜ëŠ” ê²½ë¡œì— /ez/ ì¶”ê°€ (koreagift.com)
                                            if 'koreagift.com' in base_domain_url:
                                                processed_img_src = '/ez' + img_src
                                            else:
                                                processed_img_src = img_src
                                        elif img_src.startswith('/'):
                                            # ë‹¤ë¥¸ ì ˆëŒ€ ê²½ë¡œëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©
                                            processed_img_src = img_src
                                        else:
                                            # ìƒëŒ€ ê²½ë¡œëŠ” ì ì ˆíˆ ì²˜ë¦¬
                                            if 'koreagift.com' in base_domain_url and img_src.startswith('upload/'):
                                                processed_img_src = f"/ez/{img_src}"
                                            else:
                                                processed_img_src = f"/{img_src}"
                                        
                                        # /ez/ez/ ì¤‘ë³µ ìˆ˜ì •
                                        if '/ez/ez/' in processed_img_src:
                                            processed_img_src = processed_img_src.replace('/ez/ez/', '/ez/')
                                            
                                        # ìµœì¢… URL ìƒì„±
                                        final_img_url = urljoin(base_domain_url, processed_img_src)
                                        
                                        # ì´ë¯¸ì§€ URL ê²€ì¦ - ê¸°ë³¸ êµ¬ì¡°ë§Œ í™•ì¸
                                        valid_img_url = False
                                        if final_img_url and final_img_url.startswith('http'):
                                            url_parts = urlparse(final_img_url)
                                            if url_parts.netloc and url_parts.path:
                                                valid_img_url = True
                                    else:
                                        final_img_url = ""
                                        valid_img_url = False
                                    
                                    # ìƒí’ˆ URL ì²˜ë¦¬
                                    if a_href:
                                        if a_href.startswith('http'):
                                            # ì´ë¯¸ ì™„ì „í•œ URL
                                            final_href_url = a_href
                                        elif a_href.startswith('./'):
                                            # ìƒëŒ€ ê²½ë¡œ
                                            processed_href = '/' + a_href[2:]  # './' ì œê±°
                                            final_href_url = urljoin(base_domain_url, processed_href)
                                        elif a_href.startswith('/'):
                                            # ì ˆëŒ€ ê²½ë¡œ
                                            final_href_url = urljoin(base_domain_url, a_href)
                                        else:
                                            # ê¸°íƒ€ ìƒëŒ€ ê²½ë¡œ
                                            final_href_url = urljoin(base_domain_url, '/' + a_href)
                                    else:
                                        final_href_url = ""

                                    # ë„ë©”ì¸ì—ì„œ ê³µê¸‰ì‚¬ ì •ë³´ ì¶”ì¶œ
                                    supplier = urlparse(base_url).netloc.split('.')[0]
                                    if supplier == 'koreagift':
                                        supplier = 'ê³ ë ¤ê¸°í”„íŠ¸'
                                    elif supplier == 'adpanchok':
                                        supplier = 'ì• ë“œíŒì´‰'
                                    
                                    # ìœ íš¨í•œ ì´ë¯¸ì§€ URLë§Œ ì €ì¥
                                    if valid_img_url:
                                        item_data['image_path'] = final_img_url
                                    else:
                                        item_data['image_path'] = None
                                        logger.warning(f"âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ì´ë¯¸ì§€ URL ë¬´ì‹œ: {final_img_url}")
                                    
                                    item_data['src'] = final_img_url  # ì´ì „ í˜¸í™˜ì„± ìœ ì§€
                                    item_data['href'] = final_href_url
                                    item_data['link'] = final_href_url  # ë§¤ì¹­ ë¡œì§ í˜¸í™˜ì„±
                                    item_data['name'] = name.strip() if name else ""
                                    price_cleaned = re.sub(r'[^\d.]', '', price_text) if price_text else ""
                                    item_data['price'] = float(price_cleaned) if price_cleaned else 0.0
                                    item_data['supplier'] = supplier  # ê³µê¸‰ì‚¬ ì •ë³´ ì¶”ê°€
                                    
                                    logger.debug(f"ğŸ“¦ Extracted item: {item_data}")

                                    items_on_page.append(item_data)
                                    processed_items += 1
                                except (PlaywrightError, Exception) as e:
                                    logger.warning(f"âš ï¸ Could not extract data for item index {i} on page {page_number}: {e}")
                                    continue # Skip this item
                            
                            data.extend(items_on_page)
                            logger.debug(f"ğŸ“Š Scraped {len(items_on_page)} items from page {page_number}. Total processed: {processed_items}")

                            if processed_items >= max_items_to_scrape:
                                logger.info(f"âœ… Reached scrape limit ({max_items_to_scrape}) for keyword '{keyword}'.")
                                break

                            # --- Pagination --- 
                            next_page_locator_str = f'div.custom_paging > div[onclick*="getPageGo1({page_number + 1})"]' # CSS selector
                            next_page_locator = page_instance.locator(next_page_locator_str)
                            
                            try:
                                 if await next_page_locator.is_visible(timeout=5000):
                                     logger.debug(f"ğŸ“„ Clicking next page ({page_number + 1})")
                                     # Click and wait for navigation/load state
                                     await next_page_locator.click(timeout=action_timeout)
                                     # Wait for content to likely reload after click
                                     await page_instance.wait_for_load_state('domcontentloaded', timeout=navigation_timeout) 
                                     page_number += 1
                                 else:
                                     logger.info("âš ï¸ Next page element not found or not visible. Ending pagination.")
                                     break 
                            except (PlaywrightError, Exception) as e:
                                 logger.warning(f"âš ï¸ Failed to click or load next page ({page_number + 1}): {e}")
                                 break 

                    except PlaywrightError as pe:
                        logger.error(f"âŒ Playwright error during setup/search for URL '{base_url}', Keyword '{keyword}': {pe}")
                    except Exception as e:
                        logger.error(f"âŒ Unexpected error during scrape setup/search for URL '{base_url}', Keyword '{keyword}': {e}", exc_info=True)
                    # Loop continues to next URL or keyword if error occurred before scraping loop

                    # --- End of single URL/Keyword attempt --- 
                    logger.info(f"âœ… Scraping attempt finished for URL='{base_url}', Keyword='{keyword}'. Found {len(data)} items.")
                    current_attempt_df = pd.DataFrame(data)

                    # Keep track of the best result for the current keyword across URLs
                    if len(current_attempt_df) > len(current_keyword_best_df):
                        current_keyword_best_df = current_attempt_df
                        logger.debug(f"ğŸ“Š Updating best result for keyword '{keyword}' with {len(current_keyword_best_df)} items from {base_url}.")
                    
                    # If this URL attempt yielded enough results, use it and maybe stop checking other URLs for this keyword
                    if len(current_attempt_df) >= min_results_threshold:
                         logger.info(f"âœ… Found sufficient results ({len(current_attempt_df)}) with keyword '{keyword}' from {base_url}. Using this result.")
                         # current_keyword_best_df = current_attempt_df # Already assigned if it's the best
                         # keyword_found_sufficient = True # Optional: break inner URL loop if one URL is enough
                         # break # Uncomment to stop checking other URLs for this keyword once one works well

                # --- End of URL loop for the current keyword --- 
                # Update overall best result if current keyword's best is better
                if len(current_keyword_best_df) > len(best_result_df):
                    best_result_df = current_keyword_best_df
                    logger.debug(f"ğŸ“Š Updating overall best result with {len(best_result_df)} items from keyword '{keyword}'.")

                # Check if the best result found *for this keyword* is sufficient to stop trying other keywords
                if len(current_keyword_best_df) >= min_results_threshold:
                    logger.info(f"âœ… Found sufficient results ({len(current_keyword_best_df)}) with keyword '{keyword}'. Stopping keyword variations.")
                    # Instead of returning early, break the keyword loop to ensure cleanup runs
                    break # Stop trying further keywords

        except Exception as e:
            logger.error(f"âŒ Major error during Kogift scrape execution for '{original_keyword1}': {e}", exc_info=True)
            best_result_df = pd.DataFrame() # Ensure empty DataFrame on major error
        finally:
            # Ensure page and context are closed if they were created
            if page:
                try: 
                    await page.close()
                    logger.debug("âœ… Closed Playwright page.")
                except Exception as page_close_err:
                    logger.warning(f"âš ï¸ Error closing page: {page_close_err}")
            if context:
                try:
                    await context.close()
                    logger.debug("âœ… Closed Playwright context.")
                except Exception as context_close_err:
                    logger.warning(f"âš ï¸ Error closing context: {context_close_err}")

        # Final log based on results
        if len(best_result_df) < min_results_threshold:
            logger.warning(f"âš ï¸ Could not find sufficient results ({min_results_threshold} needed) for '{original_keyword1}' after trying variations. Max found: {len(best_result_df)} items.")
        else:
            logger.info(f"âœ… KoGift scraping finished for '{original_keyword1}'. Final result count: {len(best_result_df)} items.")

        # Map DataFrame columns before returning
        if not best_result_df.empty:
            try:
                # Define final column mapping
                column_mapping = {
                    'name': 'name', 
                    'price': 'price',
                    'href': 'link', 
                    'src': 'image_url',
                    'supplier': 'supplier'  # ê³µê¸‰ì‚¬ ì»¬ëŸ¼ ì¶”ê°€
                }
                # Select and rename columns that exist in the DataFrame
                rename_map = {k: v for k, v in column_mapping.items() if k in best_result_df.columns}
                best_result_df = best_result_df[list(rename_map.keys())].rename(columns=rename_map)
                
                # Ensure correct dtypes (e.g., price as float)
                if 'price' in best_result_df.columns:
                    best_result_df['price'] = pd.to_numeric(best_result_df['price'], errors='coerce').fillna(0.0)
                    
                # ìˆ˜ëŸ‰-ë‹¨ê°€ ì •ë³´ ì¶”ì¶œ (ì˜µì…˜)
                if fetch_price_tables and not best_result_df.empty:
                    logger.info(f"ìƒì„¸ í˜ì´ì§€ì—ì„œ ìˆ˜ëŸ‰-ë‹¨ê°€ ì •ë³´ ì¶”ì¶œ ì‹œì‘ (ì´ {len(best_result_df)}ê°œ ìƒí’ˆ)")
                    
                    # ìƒì„¸ í˜ì´ì§€ ë³„ë„ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
                    detail_context = await browser.new_context()
                    detail_page = await detail_context.new_page()
                    
                    # ìˆ˜ëŸ‰-ë‹¨ê°€ ì •ë³´ë¥¼ ì €ì¥í•  ì‚¬ì „
                    price_tables = {}
                    
                    # ì²˜ìŒ 5ê°œ ìƒí’ˆì— ëŒ€í•´ì„œë§Œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ (ì‹œê°„ ì ˆì•½ì„ ìœ„í•´)
                    max_details = min(5, len(best_result_df))
                    for i, (idx, row) in enumerate(best_result_df.head(max_details).iterrows()):
                        product_link = row['link']
                        product_name = row['name']
                        
                        logger.info(f"ìƒí’ˆ {i+1}/{max_details} ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì¤‘: {product_name[:30]}...")
                        price_table = await extract_price_table(detail_page, product_link)
                        
                        if price_table is not None and not price_table.empty:
                            price_tables[idx] = price_table
                            logger.info(f"ìƒí’ˆ {i+1}/{max_details} ë‹¨ê°€í‘œ ì¶”ì¶œ ì„±ê³µ: {len(price_table)}ê°œ í–‰")
                        else:
                            logger.warning(f"ìƒí’ˆ {i+1}/{max_details} ë‹¨ê°€í‘œ ì¶”ì¶œ ì‹¤íŒ¨")
                    
                    # í˜ì´ì§€ ë° ì»¨í…ìŠ¤íŠ¸ ë‹«ê¸°
                    await detail_page.close()
                    await detail_context.close()
                    
                    # ì¶”ì¶œëœ ë‹¨ê°€í‘œ ì •ë³´ ë¡œê¹…
                    logger.info(f"ì´ {len(price_tables)}/{max_details} ìƒí’ˆì—ì„œ ë‹¨ê°€í‘œ ì¶”ì¶œ ì„±ê³µ")
                    
                    # ê²°ê³¼ DataFrameì— ë‹¨ê°€í‘œ ì •ë³´ ì»¬ëŸ¼ ì¶”ê°€
                    best_result_df['price_table'] = pd.Series(price_tables)
                
                # ì´ë¯¸ì§€ URL ì •ê·œí™” ë° ë‹¤ìš´ë¡œë“œ
                logger.info("ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œì‘")
                best_result_df_list = best_result_df.to_dict('records')
                
                # ì´ë¯¸ì§€ URL ì •ê·œí™” ë° ë‹¤ìš´ë¡œë“œ ìˆ˜í–‰
                best_result_df_list = await verify_kogift_images(best_result_df_list)
                
                # ë¦¬ìŠ¤íŠ¸ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
                best_result_df = pd.DataFrame(best_result_df_list)
                
                # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ í™•ì¸ (ë””ë²„ê¹…)
                if 'local_image_path' in best_result_df.columns:
                    downloaded_count = best_result_df['local_image_path'].notnull().sum()
                    logger.info(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ê²°ê³¼: {downloaded_count}/{len(best_result_df)} íŒŒì¼ ì €ì¥ë¨")
                else:
                    logger.warning("ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œê°€ ì‹¤í–‰ë˜ì—ˆì§€ë§Œ ë¡œì»¬ ê²½ë¡œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                
            except KeyError as ke:
                logger.error(f"Error mapping columns for Kogift results: Missing key {ke}. Returning raw data.")
            except Exception as map_err:
                logger.error(f"Error during final column mapping for Kogift: {map_err}")

        return best_result_df

# Simple function to test direct image download
def test_kogift_scraper():
    """Test both image download and product information retrieval from Kogift"""
    import asyncio
    import sys
    import os
    import requests
    import logging
    import random
    import time
    import pandas as pd
    import argparse
    from datetime import datetime
    from urllib.parse import urlparse, urljoin
    from playwright.async_api import async_playwright
    from utils import load_config
    
    # Setup command-line arguments
    parser = argparse.ArgumentParser(description='Test Kogift scraper functionality')
    parser.add_argument('--test-type', choices=['all', 'images', 'products'], default='all',
                        help='Specify which tests to run (all, images, or products)')
    parser.add_argument('--keywords', nargs='+', default=["777", "ì“°ë¦¬ì„ë¸", "ì†í†±ê¹ì´"],
                        help='Keywords to use for product search testing')
    parser.add_argument('--max-items', type=int, default=5,
                        help='Maximum number of items to fetch per keyword')
    
    # Parse command-line arguments
    if len(sys.argv) > 1 and sys.argv[1] == '--test-run':
        # Remove the --test-run argument that was added automatically when running the script
        sys.argv.pop(1)
    args = parser.parse_args()
    
    # Setup logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logger = logging.getLogger("kogift_test")
    
    # Test image download functionality
    def test_image_download():
        logger.info("=== TESTING IMAGE DOWNLOAD FUNCTIONALITY ===")
        
        # Example image URLs to test
        test_urls = [
            "https://koreagift.com/ez/upload/mall/shop_1707873892937710_0.jpg",
            "https://koreagift.com/upload/mall/shop_1736386408518966_0.jpg",  # Missing /ez/ path
            "https://adpanchok.co.kr/upload/mall/shop_1234567890_0.jpg"
        ]
        
        print(f"Testing direct image download for {len(test_urls)} URLs")
        
        # Create save directory
        save_dir = 'test_images'
        os.makedirs(save_dir, exist_ok=True)
        
        # Normalize URLs first
        normalized_urls = []
        for url in test_urls:
            normalized_url, is_valid = normalize_kogift_image_url(url)
            print(f"Original: {url}")
            print(f"Normalized: {normalized_url} (Valid: {is_valid})")
            if is_valid:
                normalized_urls.append(normalized_url)
        
        # Download images
        if normalized_urls:
            results = {}
            for url in normalized_urls:
                result = download_image(url, save_dir)
                if result:
                    results[url] = result
            
            # Print results
            print(f"\nDownload results: {len(results)}/{len(normalized_urls)} successful")
            for url, path in results.items():
                print(f"URL: {url}")
                print(f"Saved to: {path}")
                print(f"File exists: {os.path.exists(path)}")
                if os.path.exists(path):
                    size_kb = os.path.getsize(path) / 1024
                    print(f"File size: {size_kb:.2f} KB")
                print("-" * 50)
    
    # Test product information retrieval 
    async def test_product_info():
        logger.info("=== TESTING PRODUCT INFORMATION RETRIEVAL ===")
        
        # Load configuration
        config_path = os.path.join(os.path.dirname(__file__), '..', 'config.ini')
        config = load_config(config_path)
        if not config.sections():
            logger.error(f"Could not load config from {config_path}")
            return
        
        # Use keywords from command line args
        test_products = args.keywords
        logger.info(f"Using keywords for testing: {test_products}")
        
        # Modify config to limit number of items
        if config.has_section('ScraperSettings'):
            config.set('ScraperSettings', 'kogift_max_items', str(args.max_items))
        else:
            config.add_section('ScraperSettings')
            config.set('ScraperSettings', 'kogift_max_items', str(args.max_items))
            
        # Launch browser
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=config.getboolean('Playwright', 'playwright_headless', fallback=False))
            
            for product in test_products:
                logger.info(f"Starting Kogift test scrape for: {product}")
                # Pass the ConfigParser object to scrape_data
                result = await scrape_data(browser, product, config=config, fetch_price_tables=True)
                
                print(f"\n--- Test Scrape Results for '{product}' ---")
                if not result.empty:
                    print(f"Found {len(result)} results.")
                    print(f"First 5 results:")
                    print(result.head())
                    
                    # Print all URLs in the result
                    print(f"\nAll product URLs found ({len(result)}):")
                    for i, (name, link) in enumerate(zip(result['name'], result['link']), 1):
                        print(f"{i}. {name[:30]}... : {link}")
                    
                    print(f"\nAll image URLs found ({len(result)}):")
                    for i, (name, img) in enumerate(zip(result['name'], result['image_url']), 1):
                        print(f"{i}. {name[:30]}... : {img}")
                    
                    # ë‹¤ìš´ë¡œë“œëœ ì´ë¯¸ì§€ ê²½ë¡œ ì¶œë ¥
                    if 'local_image_path' in result.columns:
                        print(f"\nDownloaded images ({result['local_image_path'].notnull().sum()}/{len(result)}):")
                        for i, (name, img_path) in enumerate(zip(result['name'], result['local_image_path']), 1):
                            if pd.notnull(img_path):
                                print(f"{i}. {name[:30]}... : {img_path}")
                    
                    # ë‹¨ê°€í‘œ ì •ë³´ ì¶œë ¥ (ìˆëŠ” ê²½ìš°)
                    if 'price_table' in result.columns:
                        print(f"\nìˆ˜ëŸ‰-ë‹¨ê°€ ì •ë³´ ì¶”ì¶œ ê²°ê³¼:")
                        price_tables_found = 0
                        for idx, price_table in result['price_table'].items():
                            if isinstance(price_table, pd.DataFrame) and not price_table.empty:
                                price_tables_found += 1
                                product_name = result.loc[idx, 'name']
                                print(f"\nìƒí’ˆ: {product_name[:30]}...")
                                print(price_table)
                        
                        print(f"\nì´ {price_tables_found}ê°œ ìƒí’ˆì—ì„œ ë‹¨ê°€í‘œ ì •ë³´ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
                else:
                    print("No results found.")
                
                print(f"Total results: {len(result)}")
                print("-------------------------\n")
            
            await browser.close()
    
    # Run tests based on command-line arguments
    print(f"Running Kogift scraper tests (mode: {args.test_type})...")
    
    if args.test_type in ['all', 'images']:
        print("\n1. Testing image download functionality")
        test_image_download()
    
    if args.test_type in ['all', 'products']:
        print("\n2. Testing product information retrieval")
        asyncio.run(test_product_info())
    
    print("\nAll Kogift tests completed")

if __name__ == "__main__":
    # If this file is run directly, run the test
    if os.path.basename(__file__) == "crowling_kogift.py":
        # Setup basic logging FOR THE TEST ONLY
        # In production, logging is set up by initialize_environment
        logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
        logger.info("Running Kogift scraper test...")
        
        # Run the comprehensive test function
        test_kogift_scraper()