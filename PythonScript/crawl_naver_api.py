import os
import logging
import re
import httpx
import pandas as pd
import asyncio
import time
import hashlib
from urllib.parse import urlparse
import json
import configparser
import pprint
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Union
import aiohttp
import aiofiles
from PIL import Image
from playwright.async_api import async_playwright, Page, Browser, BrowserContext, Error as PlaywrightError
from bs4 import BeautifulSoup
import random
from datetime import datetime
import sys

# Import based on how the file is run
try:
    # When imported as module
    from utils import (
        download_image_async, get_async_httpx_client, generate_keyword_variations, 
        load_config, tokenize_korean, jaccard_similarity, generate_product_name_hash
    )
    from image_utils import remove_background_async
    from crawling_UPrice_v2_naver import extract_quantity_prices, get_quantities_from_excel
except ImportError:
    # When run directly as script
    from utils import (
        download_image_async, get_async_httpx_client, generate_keyword_variations, 
        load_config, tokenize_korean, jaccard_similarity
    )
    from image_utils import remove_background_async
    from crawling_UPrice_v2_naver import extract_quantity_prices, get_quantities_from_excel

# Setup logger for this module
logger = logging.getLogger(__name__) # Use module-specific logger

# Note: CONFIG will be passed as config_dict
MIN_RESULTS_THRESHOLD_NAVER = 5 # Minimum desired results for Naver API

async def crawl_naver(original_query: str, client: httpx.AsyncClient, config: configparser.ConfigParser, max_items: int = 50, reference_price: float = 0) -> List[Dict[str, Any]]:
    """
    Search products using Naver Shopping API, trying multiple keyword variations.

    Args:
        original_query: The original search term.
        client: Async HTTPX client for making requests.
        config: ConfigParser object containing configuration.
        max_items: Maximum number of items to return per keyword attempt.
        reference_price: Reference price for filtering (currently only logged).

    Returns:
        List of product dictionaries from the first keyword variation yielding enough results,
        or the results from the last attempted keyword.
    """
    logger.debug(f"Initiating Naver search for query: '{original_query}', max_items: {max_items}, ref_price: {reference_price}")
    
    # Define the API URL
    api_url = "https://openapi.naver.com/v1/search/shop.json"  # Example API URL, replace with the correct one if needed

    try:
        client_id = config.get('API_Keys', 'naver_client_id', fallback='')
        client_secret = config.get('API_Keys', 'naver_client_secret', fallback='')
    except (configparser.NoSectionError, configparser.NoOptionError):
         logger.error("Naver API keys not found in [API_Keys] section of config. Cannot perform search.")
         return []

    if not client_id or not client_secret:
        logger.error("Missing Naver API client ID or secret in config. Cannot perform search.")
        return []

    client_id_display = (client_id[:4] + '...') if client_id else 'Not Set'
    client_secret_display = (client_secret[:4] + '...') if client_secret else 'Not Set'
    logger.info(f"üü¢ Naver API Credentials: Client ID starts with '{client_id_display}', Secret starts with '{client_secret_display}'")

    # API ÏöîÏ≤≠ ÎîúÎ†àÏù¥ ÏÑ§Ï†ï ÏàòÏ†ï
    api_delay = config.getfloat('ScraperSettings', 'naver_api_delay', fallback=1.5)
    if api_delay < 2.0:
        api_delay = random.uniform(2.0, 4.0)
    logger.info(f"Using API delay: {api_delay:.2f} seconds")

    # Get initial similarity threshold from config
    try:
        # FIXED: Lower default threshold from 0.6 to 0.4 for better matching
        initial_sim_threshold = config.getfloat('Matching', 'naver_initial_similarity_threshold', fallback=0.4)
    except (configparser.Error, ValueError):
        logger.warning("Could not read 'naver_initial_similarity_threshold' from [Matching] config. Using default 0.4.")
        initial_sim_threshold = 0.4
    logger.info(f"Using initial Naver result similarity threshold: {initial_sim_threshold}")

    # Tokenize the original query once
    original_query_tokens = tokenize_korean(original_query)

    # Generate keywords to try
    keywords_to_try = generate_keyword_variations(original_query)
    logger.info(f"üü¢ Generated Naver keywords for '{original_query}': {keywords_to_try}")

    best_result_list: List[Dict[str, Any]] = [] # Store results from the most successful keyword attempt

    for keyword_idx, query in enumerate(keywords_to_try):
        logger.info(f"üü¢ --- Trying Naver keyword variation {keyword_idx+1}/{len(keywords_to_try)}: '{query}' ---")
        current_keyword_results: List[Dict[str, Any]] = []
        processed_api_items = 0

        # API ÏöîÏ≤≠ Ìó§Îçî Í∞úÏÑ†
        headers = {
            "X-Naver-Client-Id": client_id,
            "X-Naver-Client-Secret": client_secret,
            "Accept": "application/json, text/plain, */*",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Cache-Control": "no-cache",
            "Pragma": "no-cache",
            "DNT": "1",
            "Sec-Fetch-Dest": "empty",
            "Sec-Fetch-Mode": "cors",
            "Sec-Fetch-Site": "same-origin",
        }

        # User-Agent Î°úÌÖåÏù¥ÏÖò
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        ]
        headers['User-Agent'] = random.choice(user_agents)

        # Define promotional keywords (could be moved outside loop/function)
        promo_keywords = [
            "ÌåêÏ¥âÎ¨º", "Í∏∞ÌîÑÌä∏", "ÎãµÎ°ÄÌíà", "Í∏∞ÎÖêÌíà", "Ïù∏ÏáÑ", "Í∞ÅÏù∏", "Ï†úÏûë",
            "ÌôçÎ≥¥", "ÎØ∏Ïä§ÌÑ∞Î™ΩÌÇ§", "Ìò∏Í∞±ÌÉàÏ∂ú", "Í≥†Î†§Í∏∞ÌîÑÌä∏", "ÌåêÏ¥â", "Í∏∞ÏóÖÏÑ†Î¨º",
            "Îã®Ï≤¥ÏÑ†Î¨º", "ÌñâÏÇ¨Ïö©Ìíà", "ÌôçÎ≥¥Î¨º", "Í∏∞ÏóÖÌôçÎ≥¥", "Î°úÍ≥†Ïù∏ÏáÑ", "Î°úÍ≥†Í∞ÅÏù∏",
            "Î°úÍ≥†Ï†úÏûë", "Í∏∞ÏóÖÎãµÎ°ÄÌíà", "ÌñâÏÇ¨ÎãµÎ°ÄÌíà", "Í∏∞ÎÖêÌíàÏ†úÏûë", "Í∏∞ÏóÖÍ∏∞ÎÖêÌíà",
        ]

        # Search up to 3 pages for the current keyword
        for page in range(1, 4):
            if len(current_keyword_results) >= max_items:
                logger.debug(f"Reached max_items ({max_items}) limit for keyword '{query}', stopping API calls for this keyword.")
                break

            # Calculate API request parameters
            api_display_count = 100  # Max allowed by Naver API
            start_index = (page - 1) * api_display_count + 1
            # Calculate how many more items we need, respecting max_items overall
            effective_display_count = min(api_display_count, max_items - len(current_keyword_results))
            if effective_display_count <= 0:
                logger.debug(f"Effective display count is zero or less for keyword '{query}', page {page}. Breaking page loop.")
                break

            # Set up request parameters
            params = {
                "query": query,
                "display": effective_display_count,
                "start": start_index,
                "sort": "sim",
                "exclude": "used:rental"
            }
            logger.debug(f"üü¢ Naver API Request (Keyword: '{query}', Page {page}, Sort: 'sim'): Params={params}")

            # Add delay before API call to avoid hitting rate limits
            if page > 1 or keyword_idx > 0:
                # Increase delay between pages and keywords
                current_delay = api_delay * (1.2 if page > 1 else 1.0)  # Longer delay between pages
                logger.debug(f"üü¢ Adding delay of {current_delay:.2f} seconds before Naver API request (Page: {page}, Keyword Attempt: {keyword_idx+1})")
                await asyncio.sleep(current_delay)

            try:
                # Log headers just before the request
                # Mask secret for safety, though it's fetched locally in this function scope.
                headers_to_log = headers.copy()
                if "X-Naver-Client-Secret" in headers_to_log:
                    headers_to_log["X-Naver-Client-Secret"] = headers_to_log["X-Naver-Client-Secret"][:4] + "..."
                # Use pprint for potentially large headers, limit length if necessary
                log_headers_str = pprint.pformat(headers_to_log, width=120)
                logger.debug(f"üü¢ Preparing Naver API request. Headers: {log_headers_str}")

                logger.info(f"üü¢ Sending Naver API request for '{query}' (Page {page})")
                start_time = time.monotonic()
                response = await client.get(api_url, headers=headers, params=params)
                response_time = time.monotonic() - start_time
                status_code = response.status_code
                response_text = response.text # Get text immediately for potential logging

                logger.info(f"üü¢ Naver API response status: {status_code} (took {response_time:.2f}s)")

                # Enhanced error logging: Check status code first
                if status_code != 200:
                    error_text_snippet = response_text[:200] + "..." if len(response_text) > 200 else response_text
                    logger.error(f"üü¢ Naver API error response (Status: {status_code}, Keyword: '{query}', Page: {page}): Snippet: {error_text_snippet}")
                    # Log full text for non-200 errors for detailed debugging
                    logger.debug(f"üü¢ Full Naver API error response text (Status {status_code}): {response_text}")

                    if status_code == 401: # Unauthorized
                         logger.error("Naver API authentication failed (401). Check credentials.")
                         # Stop trying immediately if credentials are bad
                         return [] # Return empty list, signalling fatal auth error
                    elif status_code == 429: # Rate limit
                        wait_time = api_delay * 3
                        logger.error(f"üü¢ Rate limit exceeded (429). Waiting {wait_time:.2f} seconds before next request.")
                        await asyncio.sleep(wait_time)
                    elif status_code == 404: # Not Found (e.g., invalid API endpoint, unlikely here)
                         logger.error("Naver API endpoint not found (404). Check API URL.")
                    # Continue to next page or keyword for other errors for now
                    continue

                # --- Status code IS 200 ---
                # Now try to parse JSON and check for embedded errors
                try:
                    data = response.json()
                except json.JSONDecodeError as json_err:
                    # Handle cases where status is 200 but body is not valid JSON
                    logger.error(f"üü¢ Failed to decode JSON from Naver API (Status 200, Keyword: '{query}', Page: {page}): {json_err}")
                    logger.error(f"üü¢ Full Naver API response text (Status 200, JSON decode failed): {response_text}")
                    # Check if the raw text contains the specific auth error message
                    if "Not Exist Client ID" in response_text:
                         logger.error("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
                         logger.error("!!! Detected 'Not Exist Client ID' error in response body despite Status 200!")
                         logger.error("!!! This indicates an API inconsistency or header issue. Check credentials/headers.")
                         logger.error("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
                         # Consider stopping if auth error appears even with 200 status
                         # return [] # Option: Treat as fatal auth error
                    break # Stop processing pages for this keyword due to bad response

                # --- JSON decoded successfully (Status 200) ---
                total_items_api = data.get('total', 0)
                api_items_on_page = len(data.get('items', []))
                logger.info(f"üü¢ Naver API Response (Keyword: '{query}', Page {page}): Found {total_items_api} total items, received {api_items_on_page} on this page.")

                # Check for 'errorMessage' key within the successfully decoded JSON
                if 'errorMessage' in data:
                    api_error_message = data.get('errorMessage')
                    logger.error(f"üü¢ Naver API error message found in JSON (Status 200): {api_error_message}")
                    if "Not Exist Client ID" in str(api_error_message):
                         logger.error("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
                         logger.error("!!! Detected 'Not Exist Client ID' error in JSON response body despite Status 200!")
                         logger.error("!!! Check API Key/Secret and API application settings in Naver Developer portal.")
                         logger.error("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
                         # Consider stopping if auth error appears even with 200 status
                         # return [] # Option: Treat as fatal auth error
                    # If there's an error message, even if items exist, treat page as problematic
                    break # Stop fetching pages for this keyword

                if 'items' not in data or not data.get('items'):
                    # Logged 'errorMessage' above if present. Now handle case of no items AND no error message.
                    logger.warning(f"üü¢ Naver API returned Status 200 but no items for '{query}' (Page {page}).")
                    # Log the full response for debugging if no items found
                    logger.debug(f"üü¢ Full Naver API response (Status 200, no items/no error msg): {json.dumps(data, ensure_ascii=False)[:500]}")
                    break # No items on this page, stop fetching for this keyword

            except httpx.TimeoutException as timeout_err:
                 wait_time = api_delay * 3  # Increase wait time on timeout
                 logger.error(f"üü¢ Timeout during Naver API request (Keyword: '{query}', Page {page}): {timeout_err}. Waiting {wait_time:.2f}s.")
                 await asyncio.sleep(wait_time) # Wait longer on timeout
                 continue # Retry this page/keyword after delay
            except httpx.RequestError as req_err:
                 logger.error(f"üü¢ HTTPX Request Error during Naver API request (Keyword: '{query}', Page {page}): {req_err}", exc_info=True)
                 # Log response text if available
                 try:
                     if response and response.text:
                         logger.error(f"üü¢ Response text during HTTPX Request Error: {response.text[:500]}...")
                         # Check for rate limit error in response
                         if "rate limit" in response.text.lower() or "429" in response.text:
                             wait_time = api_delay * 4  # Even longer wait on rate limit
                             logger.error(f"üü¢ Detected rate limit error. Waiting {wait_time:.2f}s before retry.")
                             await asyncio.sleep(wait_time)
                             continue  # Retry after longer wait
                 except NameError: pass # response might not be defined
                 await asyncio.sleep(api_delay) # Basic delay and continue
                 break # Assume persistent issue with this keyword/page
            # except json.JSONDecodeError handled above for status 200 case
            except Exception as e:
                logger.error(f"üü¢ Unexpected error processing Naver API response (Keyword: '{query}', Page {page}): {e}", exc_info=True)
                # Log response text if available
                try:
                     if response and response.text:
                         logger.error(f"üü¢ Response text during unexpected error: {response.text[:500]}...")
                except NameError: pass # response might not be defined

                if isinstance(e, RuntimeError) and "client has been closed" in str(e):
                    logger.error(f"üü¢ HTTPX client has been closed. Cannot continue with API requests.")
                    return best_result_list # Return whatever we have so far
                await asyncio.sleep(api_delay) # Generic delay for unexpected errors
                break # Stop processing for this keyword on unexpected error

            items_added_this_page = 0
            for item_idx, item in enumerate(data.get('items', [])):
                if len(current_keyword_results) >= max_items:
                    break
                processed_api_items += 1
                try:
                    title = re.sub(r"<.*?>", "", item.get("title", "")).strip()
                    price_str = item.get("lprice", "0")
                    price = int(price_str) if price_str.isdigit() else 0

                    if price <= 0:
                        logger.debug(f"üü¢ Skipping item #{item_idx+1} (Keyword: '{query}') due to zero/invalid price: '{title}' (Price String: '{price_str}')")
                        continue

                    # --- Enhanced Similarity Check ---
                    title_tokens = tokenize_korean(title)
                    similarity = jaccard_similarity(original_query_tokens, title_tokens)
                    
                    # FIXED: Improved similarity calculation with more weight on exact matches
                    # Enhanced weighting based on token length and exact matches
                    weight = 1.0
                    common_tokens = set(original_query_tokens) & set(title_tokens)
                    
                    # Add weight for longer common tokens (more significant matches)
                    for token in common_tokens:
                        if len(token) >= 4:  # 4+ character tokens get more weight
                            weight += 0.15
                        elif len(token) >= 3: # 3 character tokens get some weight
                            weight += 0.1
                            
                    # Check for exact word matches (higher confidence)
                    original_words = ' '.join(original_query_tokens).split()
                    title_words = ' '.join(title_tokens).split()
                    exact_word_matches = set(original_words) & set(title_words)
                    
                    # Add weight for exact word matches
                    weight += len(exact_word_matches) * 0.2
                    
                    # Apply weight to similarity
                    weighted_similarity = similarity * weight
                    
                    # FIXED: Log more useful information about similarity calculation
                    if weighted_similarity >= initial_sim_threshold * 0.8:  # Log near-matches too
                        logger.debug(f"üü¢ Item #{item_idx+1} similarity: Jaccard={similarity:.2f}, Weight={weight:.2f}, " +
                                   f"Final={weighted_similarity:.2f}, Threshold={initial_sim_threshold:.2f}, " +
                                   f"Common={len(common_tokens)}, ExactWords={len(exact_word_matches)}")
                    
                    # FIXED: Use stricter threshold
                    if weighted_similarity < initial_sim_threshold:
                        logger.debug(f"üü¢ Skipping item #{item_idx+1} (Keyword: '{query}') due to low weighted similarity ({weighted_similarity:.2f} < {initial_sim_threshold}): '{title}'")
                        continue
                    # --- End Enhanced Similarity Check ---

                    seller = item.get("mallName", "")
                    link = item.get("link", "")
                    image_url = item.get("image", "")
                    mall_product_url = item.get("productUrl", link) # Use link if productUrl missing

                    # Í≥µÍ∏âÏÇ¨ Î∂ÑÎ•ò (Kogift Î∞©Ïãù Ï∞∏Í≥†)
                    supplier_type = "ÏùºÎ∞ò"
                    
                    # Ï£ºÏöî Í≥µÍ∏âÏÇ¨ ÌôïÏù∏
                    if "ÎÑ§Ïù¥Î≤Ñ" in seller or "Ïä§ÎßàÌä∏Ïä§ÌÜ†Ïñ¥" in seller:
                        supplier_type = "ÎÑ§Ïù¥Î≤Ñ"
                    elif "Ïø†Ìå°" in seller:
                        supplier_type = "Ïø†Ìå°"
                    elif "11Î≤àÍ∞Ä" in seller:
                        supplier_type = "11Î≤àÍ∞Ä"
                    elif "GÎßàÏºì" in seller or "ÏßÄÎßàÏºì" in seller:
                        supplier_type = "GÎßàÏºì"
                    elif "Ïò•ÏÖò" in seller:
                        supplier_type = "Ïò•ÏÖò"
                    elif "Ïù∏ÌÑ∞ÌååÌÅ¨" in seller:
                        supplier_type = "Ïù∏ÌÑ∞ÌååÌÅ¨"
                    elif "ÏúÑÎ©îÌîÑ" in seller:
                        supplier_type = "ÏúÑÎ©îÌîÑ"
                    elif "Ìã∞Î™¨" in seller:
                        supplier_type = "Ìã∞Î™¨"
                    
                    # Enhanced check for promotional items - better filtering
                    is_promotional = False
                    for promo in promo_keywords:
                        if promo.lower() in title.lower() or promo.lower() in seller.lower():
                            is_promotional = True
                            logger.debug(f"üü¢ Detected promotional keyword '{promo}' in '{title}' or '{seller}'")
                            break
                            
                    if is_promotional:
                        logger.debug(f"üü¢ Skipping promotional item #{item_idx+1} (Keyword: '{query}'): '{title}' (Seller: '{seller}')")
                        continue

                    # --- Data Extraction ---
                    product = {
                        'name': title,
                        'price': price,
                        'link': link,
                        'image_url': image_url,  # Make sure this is explicitly set
                        'quantity': "1",
                        'mallName': seller,
                        'mallProductUrl': mall_product_url,
                        'initial_similarity': round(weighted_similarity, 3),  # Store weighted similarity
                        'supplier': supplier_type,  # Í≥µÍ∏âÏÇ¨ Ïú†Ìòï Ï∂îÍ∞Ä
                        'source': 'naver',  # Ï∂úÏ≤ò Î™ÖÏãú
                        'seller_name': seller,  # Add seller name for easier access
                        'seller_link': mall_product_url  # Add seller link for easier access
                    }
                    # --- End Data Extraction ---

                    # Optional: Reference price check (only logging for now)
                    if reference_price > 0:
                        price_diff_percent = ((price - reference_price) / reference_price) * 100
                        logger.debug(f"üü¢ Price difference for '{title[:30]}...': {price_diff_percent:.2f}% (Item: {price}, Ref: {reference_price})")

                    current_keyword_results.append(product)
                    items_added_this_page += 1
                    logger.debug(f"  -> Added item #{item_idx+1} (Sim: {weighted_similarity:.2f}): '{title[:50]}...' (Price: {price}, Seller: '{seller}')")

                except Exception as e:
                    logger.error(f"üü¢ Error processing Naver item #{item_idx+1} (Keyword: '{query}'): {e}. Data: {item}", exc_info=True)
                    continue # Skip this item on error

            logger.debug(f"üü¢ Processed {items_added_this_page}/{api_items_on_page} items from Naver page {page} for keyword '{query}'. Total results for keyword: {len(current_keyword_results)}/{max_items}")

            # Check if API says there are no more results or we fetched less than requested
            total_api_results = data.get("total", 0)
            current_start = params.get("start", 1)
            items_received_this_page = len(data.get("items", [])) # Use the actual count received
            if current_start + items_received_this_page > total_api_results or items_received_this_page < effective_display_count:
                 logger.debug(f"üü¢ Stopping page loop for keyword '{query}': API indicates no more results or page returned fewer items ({items_received_this_page}) than requested ({effective_display_count}). (Start: {current_start}, Total API: {total_api_results})")
                 break # Stop fetching pages for this keyword

        # --- End of page loop for the current keyword ---
        logger.info(f"üü¢ Finished API search for keyword '{query}'. Found {len(current_keyword_results)} relevant products.")

        # Update the best result list found so far
        if len(current_keyword_results) > len(best_result_list):
            logger.debug(f"üü¢ Updating best Naver result set with {len(current_keyword_results)} items from keyword '{query}'.")
            best_result_list = current_keyword_results
        elif len(current_keyword_results) > 0 and not best_result_list:
             # If the first keyword gave some results, keep them even if subsequent keywords give more later (unless threshold met)
             logger.debug(f"üü¢ Keeping first set of {len(current_keyword_results)} results from keyword '{query}' as initial best.")
             best_result_list = current_keyword_results

        # Check if we found enough results with this keyword
        # MODIFICATION: Comment out the early break to ensure all keyword variations are tried.
        # if len(best_result_list) >= MIN_RESULTS_THRESHOLD_NAVER:
        #     logger.info(f"üü¢ Found sufficient results ({len(best_result_list)} >= {MIN_RESULTS_THRESHOLD_NAVER}) with keyword '{query}'. Stopping keyword variations.")
        #     break # Stop trying other keywords

    # --- End of keyword loop ---
    if not best_result_list:
        logger.warning(f"üü¢ No Naver results found for '{original_query}' after trying all keyword variations.")
    elif len(best_result_list) < MIN_RESULTS_THRESHOLD_NAVER:
         logger.warning(f"üü¢ Could not find sufficient Naver results ({MIN_RESULTS_THRESHOLD_NAVER} needed) for '{original_query}' after trying {len(keywords_to_try)} variations. Max found: {len(best_result_list)} items.")
    else:
         logger.info(f"üü¢ Naver API search finished for '{original_query}'. Final result count: {len(best_result_list)} items.")

    # FIXED: Sort results by similarity score before returning
    if best_result_list:
        best_result_list.sort(key=lambda x: x.get('initial_similarity', 0), reverse=True)
        logger.info(f"Sorted {len(best_result_list)} Naver results by similarity score (highest first)")
        
        # Log the top match for debugging
        if best_result_list:
            top_match = best_result_list[0]
            logger.info(f"Top Naver match for '{original_query}': '{top_match.get('name', '')}' with similarity {top_match.get('initial_similarity', 0):.3f}")

    return best_result_list


async def download_naver_image(url: str, save_dir: str, product_name: str, config: configparser.ConfigParser) -> Optional[str]:
    """
    Download a single Naver image to the specified directory with enhanced processing.
    
    Args:
        url (str): The image URL to download.
        save_dir (str): The directory to save the image in.
        product_name (str): The product name for generating the filename.
        config (configparser.ConfigParser): ConfigParser object containing configuration.
        
    Returns:
        Optional[str]: The local path to the downloaded image, or None if download failed.
    """
    if not url or not save_dir:
        logger.warning("Empty URL or save directory provided to download_naver_image.")
        return None

    try:
        # Ensure URL is properly encoded and valid
        if not (url.startswith('http://') or url.startswith('https://')):
            logger.warning(f"Invalid URL format: {url}")
            return None
            
        # Handle URL encoding
        if '%' not in url and ' ' in url:
            url = url.replace(' ', '%20')

        # Create all necessary directories
        try:
            # Create RPA base directory
            rpa_base = os.path.join('C:', 'RPA')
            os.makedirs(rpa_base, exist_ok=True)
            
            # Create Output directory
            output_dir = os.path.join(rpa_base, 'Output')
            os.makedirs(output_dir, exist_ok=True)
            
            # Create Image directory
            image_dir = os.path.join(rpa_base, 'Image')
            os.makedirs(image_dir, exist_ok=True)
            
            # Create Main directory
            main_dir = os.path.join(image_dir, 'Main')
            os.makedirs(main_dir, exist_ok=True)
            
            # Create save directory
            os.makedirs(save_dir, exist_ok=True)
            
            # Log directory creation
            logger.info(f"Created/verified directories: {rpa_base}, {output_dir}, {image_dir}, {main_dir}, {save_dir}")
        except Exception as e:
            logger.error(f"Failed to create directories: {e}")
            return None
        
        # Always add Naver subdirectory unless it already exists in the path
        if not save_dir.endswith('Naver'):
            # Normalize path separators and handle Korean characters
            save_dir_normalized = os.path.normpath(save_dir)
            
            if 'Naver' not in save_dir_normalized.split(os.sep):
                # Create the Naver subdirectory (using proper capitalization)
                naver_dir = os.path.join(save_dir, 'Naver')
                os.makedirs(naver_dir, exist_ok=True)
                save_dir = naver_dir
                logger.debug(f"Using Naver subdirectory: {save_dir}")
        
        # Sanitize product name more carefully - Kogift Î∞©ÏãùÍ≥º Ïú†ÏÇ¨ÌïòÍ≤å Ï≤òÎ¶¨
        if product_name is None:
            sanitized_name = "unknown_product"
        else:
            # ÌïúÍ∏Ä Î¨∏ÏûêÍ∞Ä Ìè¨Ìï®Îêú Í≤ΩÏö∞ Ìï¥Ïãú Í∏∞Î∞ò Ïù¥Î¶Ñ ÏÇ¨Ïö© (Íπ®Ïßê Î∞©ÏßÄ)
            if any('\uAC00' <= c <= '\uD7A3' for c in product_name):
                # ÌïúÍ∏ÄÏù¥ Ìè¨Ìï®Îêú ÏÉÅÌíàÎ™ÖÏùÄ Ìï¥ÏãúÎ°ú Ï≤òÎ¶¨
                sanitized_name = hashlib.md5(product_name.encode('utf-8', errors='ignore')).hexdigest()[:16]
                logger.debug(f"Using hash-based name for Korean product name: {sanitized_name}")
            else:
                # ÏòÅÎ¨∏/Ïà´ÏûêÎ°úÎßå Íµ¨ÏÑ±Îêú ÏÉÅÌíàÎ™ÖÏùÄ Ï†ÅÏ†àÌûà Ï†ïÎ¶¨
                sanitized_name = re.sub(r'[^\w\d-]', '_', product_name)[:30]
                # ÏùºÍ¥ÄÎêú Í∏∏Ïù¥Î•º ÏúÑÌï¥ Ìå®Îî© Ï∂îÍ∞Ä
                sanitized_name = sanitized_name.ljust(30, '_')
        
        # ÎûúÎç§ Ìï¥Ïãú ÏÉùÏÑ± (8ÏûêÎ°ú ÌÜµÏùº) - URL Ìï¥Ïãú ÎåÄÏã† ÎûúÎç§ ÏÇ¨Ïö©
        # import secrets
        # random_hash = secrets.token_hex(4)  # 8ÏûêÎ¶¨ ÎûúÎç§ Ìï¥Ïãú ÏÉùÏÑ±
        
        # ÏÉÅÌíàÎ™Ö Ìï¥ÏãúÍ∞í ÏÉùÏÑ± (MD5) - 16ÏûêÎ°ú ÌÜµÏùº
        try:
            name_hash = generate_product_name_hash(product_name)
        except ImportError:
            logging.warning("Could not import generate_product_name_hash, using fallback method")
            # ÏÉÅÌíàÎ™Ö Ï†ïÍ∑úÌôî (Í≥µÎ∞± Ï†úÍ±∞, ÏÜåÎ¨∏Ïûê Î≥ÄÌôò)
            normalized_name = ''.join(product_name.split()).lower()
            name_hash = hashlib.md5(normalized_name.encode('utf-8')).hexdigest()[:16]
        
        # Îëê Î≤àÏß∏ Ìï¥ÏãúÍ∞íÎèÑ ÏÉÅÌíàÎ™Ö Í∏∞Î∞òÏúºÎ°ú ÏÉùÏÑ± (ÏùºÍ¥ÄÏÑ±ÏùÑ ÏúÑÌï¥)
        normalized_name = ''.join(product_name.split()).lower()
        second_hash = hashlib.md5(normalized_name.encode('utf-8')).hexdigest()[16:24]
        
        # URLÏóêÏÑú ÌååÏùº ÌôïÏû•Ïûê Ï∂îÏ∂ú
        parsed_url = urlparse(url)
        file_ext = os.path.splitext(parsed_url.path)[1].lower()
        # ÌôïÏû•ÏûêÍ∞Ä ÏóÜÍ±∞ÎÇò Ïú†Ìö®ÌïòÏßÄ ÏïäÏùÄ Í≤ΩÏö∞ Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©
        if not file_ext or file_ext not in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp']:
            file_ext = '.jpg'
        
        # ÏÉàÎ°úÏö¥ ÌòïÏãùÏúºÎ°ú ÌååÏùºÎ™Ö ÏÉùÏÑ± (ÏÇ¨Ïù¥Ìä∏Ïù¥Î¶Ñ_ÏÉÅÌíàÎ™ÖÌï¥Ïãú_Í≥†Ïú†ÏãùÎ≥ÑÏûê)
        filename = f"naver_{name_hash}_{second_hash}{file_ext}"
        local_path = os.path.join(save_dir, filename)
        final_image_path = local_path
        
        # Ïù¥ÎØ∏ ÌååÏùºÏù¥ Ï°¥Ïû¨ÌïòÎäî Í≤ΩÏö∞ Ï§ëÎ≥µ Îã§Ïö¥Î°úÎìú Î∞©ÏßÄ
        if os.path.exists(local_path) and os.path.getsize(local_path) > 0:
            logger.debug(f"Image already exists: {local_path}")
            
            # Create consistent image data structure
            image_data = {
                'url': url,
                'local_path': os.path.abspath(local_path),
                'source': 'naver',
                'product_name': product_name
            }
            
            # Î∞∞Í≤Ω Ï†úÍ±∞ Î≤ÑÏ†ÑÏù¥ Ïù¥ÎØ∏ ÏûàÎäîÏßÄ ÌôïÏù∏
            try:
                use_bg_removal = config.getboolean('Matching', 'use_background_removal', fallback=True)
                if use_bg_removal:
                    bg_removed_path = local_path.replace('.', '_nobg.', 1)
                    if os.path.exists(bg_removed_path) and os.path.getsize(bg_removed_path) > 0:
                        final_image_path = bg_removed_path
                        image_data['local_path'] = os.path.abspath(bg_removed_path)
                        logger.debug(f"Using existing background-removed image: {final_image_path}")
                    else:
                        # Î∞∞Í≤Ω Ï†úÍ±∞ Î≤ÑÏ†ÑÏù¥ ÏóÜÏúºÎ©¥ ÏÉùÏÑ± ÏãúÎèÑ
                        try:
                            from image_utils import remove_background
                            if remove_background(local_path, bg_removed_path):
                                final_image_path = bg_removed_path
                                image_data['local_path'] = os.path.abspath(bg_removed_path)
                                logger.debug(f"Background removed for existing Naver image: {final_image_path}")
                            else:
                                logger.warning(f"Failed to remove background for Naver image {local_path}. Using original.")
                        except Exception as bg_err:
                            logger.warning(f"Error during background removal: {bg_err}. Using original image.")
            except Exception as config_err:
                logger.warning(f"Error reading background removal config: {config_err}. Using original image.")
            
            # Make sure the path exists and absolute (fixing a common issue)
            final_image_path = os.path.abspath(final_image_path)
            logger.debug(f"Using absolute path for existing image: {final_image_path}")
            
            return final_image_path

        # ÎÑ§Ìä∏ÏõåÌÅ¨ ÏöîÏ≤≠ Ìó§Îçî ÏÑ§Ï†ï (ÌïúÍµ≠ ÏÇ¨Ïù¥Ìä∏ Ìò∏ÌôòÏÑ± ÏúÑÌïú ÏÇ¨Ïö©Ïûê ÏóêÏù¥Ï†ÑÌä∏ Îì± Ï∂îÍ∞Ä)
        headers = {
            'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': 'https://shopping.naver.com/',
            'Connection': 'keep-alive',
            'Cache-Control': 'max-age=0'
        }

        # Ïû¨ÏãúÎèÑ Î°úÏßÅÏúºÎ°ú Îã§Ïö¥Î°úÎìú
        max_retries = config.getint('Network', 'max_retries', fallback=3)
        # Îçî ÎßéÏùÄ Ïû¨ÏãúÎèÑ (ÏµúÏÜå 3Î≤à)
        if max_retries < 3:
            max_retries = 3
            
        # Îã§ÏñëÌïú Ïù¥ÎØ∏ÏßÄ URL Ìè¨Îß∑ ÏãúÎèÑ (Naver ÏáºÌïë Ïù¥ÎØ∏ÏßÄ URLÏùÄ Ïó¨Îü¨ Ìè¨Îß∑Ïù¥ ÏûàÏùå)
        url_variants = [url]
        
        # Í∏∞Î≥∏ URL Ìè¨Îß∑ Î≥ÄÌôò ÏãúÎèÑ
        if "main_" in url and not "main/" in url:
            url_variants.append(url.replace("main_", "main/"))
        if ".jpg" in url and not "20240101" in url:
            url_variants.append(url.replace(".jpg", ".20240101010101.jpg"))
        if "/main_" in url:
            product_id_match = re.search(r'/main_(\d+)/', url)
            if product_id_match:
                product_id = product_id_match.group(1)
                url_variants.append(f"https://shopping-phinf.pstatic.net/main_{product_id}/{product_id}.jpg")
        
        # Handle connection closed errors by creating a fallback empty image
        use_fallback_on_failure = True
        created_placeholder_image = False

        # Î™®Îì† URL Î≥ÄÌòïÏóê ÎåÄÌï¥ Ïû¨ÏãúÎèÑ
        for variant_idx, current_url in enumerate(url_variants):
            for attempt in range(max_retries):
                try:
                    # Ïù¥ÎØ∏ÏßÄ Îã§Ïö¥Î°úÎìú
                    async with aiohttp.ClientSession(trust_env=True, timeout=aiohttp.ClientTimeout(total=60)) as session:
                        # ÌÉÄÏûÑÏïÑÏõÉ Ï¶ùÍ∞Ä Î∞è Ïû¨ÏãúÎèÑÎßàÎã§ Ï¶ùÍ∞Ä
                        timeout = 30 + (attempt * 15)
                        logger.debug(f"Downloading image: URL variant {variant_idx+1}/{len(url_variants)}, attempt {attempt+1}/{max_retries}, timeout {timeout}s: {current_url}")
                        
                        # Add a random delay between attempts to avoid rate limiting
                        if attempt > 0 or variant_idx > 0:
                            delay = 2 + attempt * 2 + random.uniform(0, 2)
                            logger.debug(f"Adding delay of {delay:.2f}s before retry")
                            await asyncio.sleep(delay)
                        
                        # Use a context manager with timeout for the request
                        try:
                            async with session.get(
                                current_url, 
                                headers=headers, 
                                timeout=timeout,
                                ssl=False  # Disable SSL verification to avoid some connection issues
                            ) as response:
                                if response.status != 200:
                                    logger.warning(f"Failed to download image: {current_url}, status: {response.status}, attempt {attempt+1}/{max_retries}")
                                    if attempt < max_retries - 1:
                                        await asyncio.sleep(1 + attempt)  # Ïû¨ÏãúÎèÑ Ï†Ñ ÎåÄÍ∏∞ (Ï¶ùÍ∞Ä)
                                        continue
                                    # ÌòÑÏû¨ URL Î≥ÄÌòïÏóê ÎåÄÌïú Î™®Îì† ÏãúÎèÑ Ïã§Ìå®, Îã§Ïùå URL Î≥ÄÌòïÏúºÎ°ú Ïù¥Îèô
                                    break
                                
                                # Read response data
                                response_data = await response.read()
                                
                                # ÏûÑÏãú ÌååÏùºÏóê Ï†ÄÏû•
                                temp_path = f"{local_path}.{time.time_ns()}.tmp"
                                try:
                                    async with aiofiles.open(temp_path, 'wb') as f:
                                        await f.write(response_data)
                                    
                                    # Ïù¥ÎØ∏ÏßÄ Í≤ÄÏ¶ù
                                    try:
                                        with Image.open(temp_path) as img:
                                            img.verify()
                                        with Image.open(temp_path) as img:
                                            if img.mode in ('RGBA', 'LA'):
                                                img = img.convert('RGB')
                                                img.save(temp_path, 'JPEG', quality=85)
                                        
                                        # ÏûÑÏãú ÌååÏùºÏùÑ ÏµúÏ¢Ö ÏúÑÏπòÎ°ú Ïù¥Îèô
                                        if os.path.exists(local_path):
                                            os.remove(local_path)
                                        os.rename(temp_path, local_path)
                                        logger.info(f"Successfully downloaded image: {current_url} -> {local_path}")
                                        
                                        # Create base image data
                                        image_data = {
                                            'url': current_url,
                                            'local_path': os.path.abspath(local_path),
                                            'original_path': os.path.abspath(local_path),
                                            'source': 'naver',
                                            'product_name': product_name
                                        }
                                        
                                        # ÌïÑÏöîÏãú Î∞∞Í≤Ω Ï†úÍ±∞ ÏãúÎèÑ
                                        try:
                                            use_bg_removal = config.getboolean('Matching', 'use_background_removal', fallback=True)
                                            if use_bg_removal:
                                                from image_utils import remove_background
                                                bg_removed_path = local_path.replace('.', '_nobg.', 1)
                                                if remove_background(local_path, bg_removed_path):
                                                    final_image_path = bg_removed_path
                                                    image_data['local_path'] = os.path.abspath(bg_removed_path)
                                                    logger.debug(f"Background removed for downloaded Naver image: {final_image_path}")
                                                else:
                                                    logger.warning(f"Failed to remove background for Naver image {local_path}. Using original.")
                                        except Exception as bg_err:
                                            logger.warning(f"Error during background removal: {bg_err}. Using original image.")
                                            
                                        # Convert to absolute path and update the data
                                        final_image_path = os.path.abspath(final_image_path)
                                        image_data['local_path'] = final_image_path
                                        
                                        return final_image_path
                                    except Exception as img_err:
                                        logger.warning(f"Downloaded image validation error: {img_err}. Cleaning up temp file.")
                                        if os.path.exists(temp_path):
                                            try:
                                                os.remove(temp_path)
                                            except:
                                                pass
                                        # Continue to next attempt or URL variant
                                        if attempt < max_retries - 1:
                                            await asyncio.sleep(1 + attempt)
                                            continue
                                except Exception as e:
                                    logger.error(f"Error processing image {current_url}: {e}")
                                    if os.path.exists(temp_path):
                                        try:
                                            os.remove(temp_path)
                                        except:
                                            pass
                                    if attempt < max_retries - 1:
                                        await asyncio.sleep(1 + attempt)  # Ïû¨ÏãúÎèÑ Ï†Ñ ÎåÄÍ∏∞ (Ï¶ùÍ∞Ä)
                                        continue
                        except (asyncio.TimeoutError, aiohttp.ClientError) as e:
                            logger.warning(f"Network timeout or client error: {e} (attempt {attempt+1})")
                            await asyncio.sleep(1 + attempt)
                            continue
                            
                except Exception as e:
                    logger.error(f"Network error downloading image {current_url}: {e}")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(1 + attempt)  # Ïû¨ÏãúÎèÑ Ï†Ñ ÎåÄÍ∏∞ (Ï¶ùÍ∞Ä)
                        continue
        
        # If all download attempts fail, create an empty placeholder image
        if use_fallback_on_failure and not created_placeholder_image:
            try:
                logger.warning(f"All download attempts failed. Creating placeholder image for {product_name}")
                
                # Create a small white placeholder image
                placeholder_img = Image.new('RGB', (100, 100), color=(255, 255, 255))
                placeholder_img.save(local_path)
                
                logger.info(f"Created placeholder image at {local_path}")
                created_placeholder_image = True
                
                # Create base image data for placeholder
                image_data = {
                    'url': url,
                    'local_path': os.path.abspath(local_path),
                    'original_path': os.path.abspath(local_path),
                    'source': 'naver',
                    'product_name': product_name,
                    'is_placeholder': True
                }
                
                # Return the placeholder path
                return os.path.abspath(local_path)
            except Exception as e:
                logger.error(f"Error creating placeholder image: {e}")
                return None
                
        return None
                
    except Exception as e:
        logger.error(f"Error downloading image {url}: {e}")
        return None

async def extract_quantity_price_from_naver_link(page: Page, product_url: str) -> Dict[str, Any]:
    """
    Extracts quantity-based pricing information from a Naver product page.
    Returns all available quantity-price combinations from the table.
    
    Args:
        page: Playwright Page object
        product_url: URL of the product page
        
    Returns:
        Dictionary containing pricing information and whether it's a promotional site
    """
    result = {
        "is_promotional_site": False,
        "has_quantity_pricing": False,
        "quantity_prices": {},
        "vat_included": False,
        "supplier_name": "",
        "supplier_url": "",
        "price_table": None,
        "raw_price_table": None,  # Store the raw price table data
        "has_captcha": False
    }
    
    try:
        # Set random viewport size
        viewport_sizes = [
            {"width": 1366, "height": 768},
            {"width": 1920, "height": 1080},
            {"width": 1440, "height": 900},
            {"width": 1536, "height": 864}
        ]
        await page.set_viewport_size(random.choice(viewport_sizes))
        
        # Set random user agent and headers
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Edge/92.0.902.84 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0'
        ]
        
        await page.set_extra_http_headers({
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0'
        })
        
        # Add anti-detection JavaScript
        await page.evaluate('''() => {
            // Hide automation flags
            Object.defineProperty(navigator, 'webdriver', { get: () => false });
            Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5] });
            Object.defineProperty(navigator, 'languages', { get: () => ['ko-KR', 'ko', 'en-US', 'en'] });
            
            // Add random mouse movements
            const randomMove = () => {
                const event = new MouseEvent('mousemove', {
                    bubbles: true,
                    cancelable: true,
                    clientX: Math.floor(Math.random() * window.innerWidth),
                    clientY: Math.floor(Math.random() * window.innerHeight)
                });
                document.dispatchEvent(event);
            };
            setInterval(randomMove, Math.random() * 2000 + 1000);
        }''')
        
        # Navigate to the product page with random delay
        logger.info(f"Navigating to Naver product page: {product_url}")
        await asyncio.sleep(random.uniform(1, 3))
        await page.goto(product_url, wait_until='networkidle', timeout=30000)
        
        # Check for captcha
        captcha_selectors = [
            'form#captcha_form', 
            'img[alt*="captcha"]',
            'div.captcha_wrap',
            'input[name="captchaBotKey"]',
            'div[class*="captcha"]',
            'iframe[src*="captcha"]',
            'div[class*="bot-check"]',
            'div[class*="security-check"]'
        ]
        
        for selector in captcha_selectors:
            if await page.locator(selector).count() > 0:
                logger.info(f"CAPTCHA detected on page: {product_url}")
                result["has_captcha"] = True
                return result
        
        # Get supplier name with enhanced selectors
        supplier_selectors = [
            'div.basicInfo_mall_title__3IDPK a',
            'a.seller_name',
            'span.mall_txt',
            'div.shop_info a.txt',
            'div[class*="mall_title"] a',
            'div[class*="seller"] a',
            'a[class*="mall-name"]'
        ]
        
        for selector in supplier_selectors:
            if await page.locator(selector).count() > 0:
                result["supplier_name"] = await page.locator(selector).text_content()
                result["supplier_name"] = result["supplier_name"].strip()
                
                # Get supplier URL
                supplier_url = await page.locator(selector).get_attribute('href') or ""
                if supplier_url and not supplier_url.startswith('http'):
                    supplier_url = f"https://shopping.naver.com{supplier_url}"
                result["supplier_url"] = supplier_url
                break
        
        # Enhanced lowest price button handling for Naver sellers
        if "ÎÑ§Ïù¥Î≤Ñ" in result["supplier_name"]:
            lowest_price_selectors = [
                '//div[contains(@class, "lowestPrice_btn_box")]/div[contains(@class, "buyButton_compare_wrap")]/a[text()="ÏµúÏ†ÄÍ∞Ä ÏÇ¨Îü¨Í∞ÄÍ∏∞"]',
                '//a[contains(text(), "ÏµúÏ†ÄÍ∞Ä ÏÇ¨Îü¨Í∞ÄÍ∏∞")]',
                '//a[contains(text(), "ÏµúÏ†ÄÍ∞Ä")]',
                '//a[contains(@class, "lowest_price")]',
                '//button[contains(text(), "ÏµúÏ†ÄÍ∞Ä")]',
                '//div[contains(@class, "lowest")]/a',
                '//div[contains(@class, "price_compare")]/a',
                '//a[contains(@class, "price_compare")]',
                '//div[contains(@class, "compare")]/a[contains(@class, "link")]',
                '//a[contains(@href, "search/gate")]'
            ]
            
            max_retries = 3
            retry_count = 0
            button_found = False
            
            while retry_count < max_retries and not button_found:
                for selector in lowest_price_selectors:
                    try:
                        # Wait for selector with timeout
                        await page.wait_for_selector(selector, timeout=5000)
                        element = page.locator(selector).first
                        
                        if await element.is_visible():
                            logger.info(f"Found lowest price button with selector: {selector}")
                            
                            # Get button position and add slight random offset
                            box = await element.bounding_box()
                            if box:
                                x = box['x'] + box['width'] / 2 + random.uniform(-5, 5)
                                y = box['y'] + box['height'] / 2 + random.uniform(-5, 5)
                                
                                # Move mouse naturally to button
                                await page.mouse.move(x, y, steps=random.randint(5, 10))
                                await asyncio.sleep(random.uniform(0.1, 0.3))
                            
                            # Try to get href first
                            href = await element.get_attribute('href')
                            if href:
                                logger.info(f"Navigating to lowest price URL: {href}")
                                await page.goto(href, wait_until='networkidle', timeout=30000)
                            else:
                                logger.info("Clicking lowest price button")
                                await element.click()
                                await page.wait_for_load_state('networkidle', timeout=30000)
                            
                            # Add random delay after click
                            await asyncio.sleep(random.uniform(2, 4))
                            
                            button_found = True
                            break
                    except Exception as e:
                        logger.warning(f"Error with lowest price selector {selector} (attempt {retry_count + 1}): {e}")
                        continue
                
                if not button_found:
                    retry_count += 1
                    if retry_count < max_retries:
                        # Add increasing delay between retries
                        await asyncio.sleep(random.uniform(2, 4) * retry_count)
                        # Reload page before retry
                        await page.reload(wait_until='networkidle', timeout=30000)
            
            if not button_found:
                logger.warning("Could not find lowest price button after all attempts")
        
        # Visit the seller's site to check for quantity-based pricing
        visit_store_selector = 'a.btn_link__dHQPb, a.go_mall, a.link_btn[href*="mall"]'
        if await page.locator(visit_store_selector).count() > 0:
            seller_site_url = await page.locator(visit_store_selector).get_attribute('href') or ""
            logger.info(f"Found seller's site URL: {seller_site_url}")
            
            if seller_site_url:
                try:
                    # Create a new context for visiting the seller's site
                    browser = page.context.browser
                    seller_context = await browser.new_context(
                        user_agent=random.choice(user_agents),
                        viewport=random.choice(viewport_sizes)
                    )
                    seller_page = await seller_context.new_page()
                    
                    # Add anti-detection measures to the new context
                    await seller_page.evaluate('''() => {
                        Object.defineProperty(navigator, 'webdriver', { get: () => false });
                        Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5] });
                        Object.defineProperty(navigator, 'languages', { get: () => ['ko-KR', 'ko', 'en-US', 'en'] });
                    }''')
                    
                    # Navigate to the seller's site with random delay
                    await asyncio.sleep(random.uniform(1, 3))
                    await seller_page.goto(seller_site_url, wait_until='domcontentloaded', timeout=30000)
                    
                    # Check for quantity-based pricing tables
                    # Look for table with keywords like "ÏàòÎüâ" and "Îã®Í∞Ä"
                    quantity_table_selectors = [
                        'table:has(th:has-text("ÏàòÎüâ")):has(th:has-text("Îã®Í∞Ä"))',
                        'table:has(th:has-text("ÏàòÎüâ")):has(th:has-text("Í∞ÄÍ≤©"))',
                        'table.quantity_price__table',
                        'div.price-box table',
                        'div.quantity_discount table',
                        'div.quantity_pricing table',
                        'table.price_by_quantity'
                    ]
                    
                    for selector in quantity_table_selectors:
                        if await seller_page.locator(selector).count() > 0:
                            logger.info(f"Found quantity-based pricing table with selector: {selector}")
                            result["has_quantity_pricing"] = True
                            result["is_promotional_site"] = True
                            
                            # Extract table HTML
                            table_html = await seller_page.locator(selector).evaluate('el => el.outerHTML')
                            
                            # Use pandas to extract the table data
                            try:
                                tables = pd.read_html(table_html)
                                if tables and len(tables) > 0:
                                    df = tables[0]
                                    
                                    # Find quantity and price columns
                                    qty_col = None
                                    price_col = None
                                    
                                    for i, col in enumerate(df.columns):
                                        col_str = str(col).lower()
                                        if any(k in col_str for k in ['ÏàòÎüâ', 'qty', 'quantity']):
                                            qty_col = i
                                        elif any(k in col_str for k in ['Îã®Í∞Ä', 'Í∞ÄÍ≤©', 'price']):
                                            price_col = i
                                    
                                    if qty_col is not None and price_col is not None:
                                        # Extract price table
                                        price_table = []
                                        for _, row in df.iterrows():
                                            try:
                                                qty = row.iloc[qty_col]
                                                price = row.iloc[price_col]
                                                
                                                # Clean data
                                                if isinstance(qty, str):
                                                    qty = ''.join(filter(str.isdigit, qty.replace(',', '')))
                                                    qty = int(qty) if qty else 0
                                                
                                                if isinstance(price, str):
                                                    price = ''.join(filter(str.isdigit, price.replace(',', '')))
                                                    price = int(price) if price else 0
                                                
                                                if qty and price:
                                                    price_table.append({"quantity": int(qty), "price": int(price)})
                                            except Exception as e:
                                                logger.warning(f"Error extracting row from price table: {e}")
                                                continue
                                        
                                        if price_table:
                                            # Sort price table by quantity
                                            price_table.sort(key=lambda x: x["quantity"])
                                            result["price_table"] = price_table
                                            
                                            # Store raw table data
                                            result["raw_price_table"] = {
                                                "quantities": [item["quantity"] for item in price_table],
                                                "prices": [item["price"] for item in price_table]
                                            }
                                            
                                            # Check for VAT info on the page
                                            vat_text_selectors = [
                                                'div:has-text("Î∂ÄÍ∞ÄÏÑ∏")',
                                                'div:has-text("VAT")',
                                                'p:has-text("Î∂ÄÍ∞ÄÏÑ∏")',
                                                'p:has-text("VAT")'
                                            ]
                                            
                                            for vat_selector in vat_text_selectors:
                                                if await seller_page.locator(vat_selector).count() > 0:
                                                    vat_text = await seller_page.locator(vat_selector).text_content()
                                                    if 'Î≥ÑÎèÑ' in vat_text or 'ÎØ∏Ìè¨Ìï®' in vat_text:
                                                        result["vat_included"] = False
                                                        logger.info(f"VAT not included based on text: {vat_text}")
                                                        break
                                                    elif 'Ìè¨Ìï®' in vat_text:
                                                        result["vat_included"] = True
                                                        logger.info(f"VAT included based on text: {vat_text}")
                                                        break
                                            
                                            # Store all quantity prices
                                            for item in price_table:
                                                qty = item["quantity"]
                                                price = item["price"]
                                                price_with_vat = price if result["vat_included"] else round(price * 1.1)
                                                result["quantity_prices"][qty] = {
                                                    "price": price,
                                                    "price_with_vat": price_with_vat,
                                                    "exact_match": True
                                                }
                                            
                                            logger.info(f"Extracted {len(result['quantity_prices'])} quantity-price pairs")
                                            break
                            except Exception as e:
                                logger.warning(f"Error parsing quantity price table: {e}")
                    
                    # Close the seller page context
                    await seller_context.close()
                    
                except Exception as e:
                    logger.error(f"Error visiting seller site: {e}")
        
        return result
        
    except Exception as e:
        logger.error(f"Error extracting quantity pricing from {product_url}: {e}")
        return result

async def crawl_naver_products(product_rows: pd.DataFrame, config: configparser.ConfigParser, browser=None) -> list:
    """
    Crawl product information from Naver Shopping using API asynchronously for multiple product rows,
    including image downloading, optional background removal, and quantity-based pricing.
    """
    if product_rows is None or len(product_rows) == 0:
        logger.info("üü¢ Naver crawl: Input product_rows is empty or None. Skipping.")
        return []  # Return empty list

    total_products = len(product_rows)
    logger.info(f"üü¢ --- Starting Naver product crawl for {total_products} products (Async) ---")

    # Get config values
    try:
        base_image_dir = config.get('Paths', 'image_main_dir', fallback='C:\\RPA\\Image\\Main')
        naver_image_dir = os.path.join(base_image_dir, 'Naver')
        os.makedirs(naver_image_dir, exist_ok=True)
        
        use_bg_removal = config.getboolean('Matching', 'use_background_removal', fallback=True)
        naver_scrape_limit = config.getint('ScraperSettings', 'naver_scrape_limit', fallback=50)
        max_concurrent_api = config.getint('ScraperSettings', 'naver_max_concurrent_api', fallback=3)
        
        # Default target quantities from ScraperSettings
        target_quantities_str = config.get('ScraperSettings', 'target_quantities', fallback='300,500,1000,2000')
        target_quantities = [int(qty.strip()) for qty in target_quantities_str.split(',') if qty.strip().isdigit()]
        if not target_quantities: # Fallback if parsing failed or empty
            target_quantities = [300, 500, 1000, 2000]

        # Check if we should prioritize quantities from Excel
        use_excel_quantities = config.getboolean('ScraperSettings', 'use_excel_quantities', fallback=True)
        if use_excel_quantities:
            try:
                if config.has_section('Input') and config.has_option('Input', 'input_file'):
                    excel_file_path = config.get('Input', 'input_file')
                    if excel_file_path and os.path.exists(excel_file_path):
                        logger.info(f"Attempting to load quantities from Excel file: {excel_file_path} based on 'use_excel_quantities' flag.")
                        excel_quantities = get_quantities_from_excel(excel_file_path) # Ensure this function is correctly imported and used
                        if excel_quantities:
                            target_quantities = excel_quantities
                            logger.info(f"Successfully loaded quantities from Excel: {target_quantities}")
                        else:
                            logger.warning(f"Failed to load quantities from Excel or no quantities found. Falling back to ScraperSettings: {target_quantities}")
                    else:
                        logger.warning(f"Excel file path not found ('{excel_file_path}') or does not exist. Falling back to ScraperSettings for target_quantities.")
                else:
                    logger.warning("'Input' section or 'input_file' option not found in config. Falling back to ScraperSettings for target_quantities.")
            except Exception as e_excel_qty:
                logger.error(f"Error reading quantities from Excel: {e_excel_qty}. Falling back to ScraperSettings.")
        else:
            logger.info("Configuration 'use_excel_quantities' is false. Using target_quantities from ScraperSettings.")
            
        visit_seller_sites = config.getboolean('ScraperSettings', 'naver_visit_seller_sites', fallback=True)
        
        logger.info(f"üü¢ Naver API Configuration: Limit={naver_scrape_limit}, Max Concurrent API={max_concurrent_api}, "
                    f"BG Removal={use_bg_removal}, Image Dir={naver_image_dir}, "
                    f"Target Quantities={target_quantities}, Visit Seller Sites={visit_seller_sites}")
    except Exception as e:
        logger.error(f"Error reading config: {e}")
        return []

    # Create semaphore for concurrent API requests
    api_semaphore = asyncio.Semaphore(max_concurrent_api)
    
    # Initialize Playwright browser if we're visiting seller sites and no browser was provided
    playwright = None
    if visit_seller_sites and not browser:
        try:
            playwright = await async_playwright().start()
            browser = await playwright.chromium.launch(headless=False)
            logger.info("Created new browser instance for seller site visits")
        except Exception as e:
            logger.error(f"Failed to create browser instance: {e}")
            browser = None
            visit_seller_sites = False

    try:
        # Create HTTP client
        async with get_async_httpx_client(config=config) as client:
            # Process each row concurrently
            tasks = []
            for idx, row in product_rows.iterrows():
                task = _process_single_naver_row(
                    idx=idx,
                    row=row,
                    config=config,
                    client=client,
                    api_semaphore=api_semaphore,
                    naver_scrape_limit=naver_scrape_limit,
                    naver_image_dir=naver_image_dir,
                    browser=browser,
                    target_quantities=target_quantities,
                    visit_seller_sites=visit_seller_sites
                )
                tasks.append(task)

            # Wait for all tasks to complete
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Filter out None results and exceptions
            valid_results = []
            for result in results:
                if isinstance(result, Exception):
                    logger.error(f"Error processing product: {result}")
                elif result is not None:
                    valid_results.append(result)

            return valid_results

    except Exception as e:
        logger.error(f"Error in crawl_naver_products: {e}")
        return []
    finally:
        # Clean up browser if we created it
        if playwright:
            try:
                await browser.close()
                await playwright.stop()
                logger.info("Closed browser and playwright instance")
            except Exception as e:
                logger.error(f"Error closing browser: {e}")

async def _process_single_naver_row(idx, row, config, client, api_semaphore, naver_scrape_limit, naver_image_dir, browser=None, target_quantities=None, visit_seller_sites=False):
    """Processes a single product row for Naver API search and image download."""
    product_name = row.get('ÏÉÅÌíàÎ™Ö', '')
    if not product_name or pd.isna(product_name):
        logger.debug(f"Skipping row {idx} due to missing product name.")
        return None

    # Get reference price
    reference_price = 0.0
    if 'ÌåêÎß§Îã®Í∞Ä(VÌè¨Ìï®)' in row and pd.notna(row['ÌåêÎß§Îã®Í∞Ä(VÌè¨Ìï®)']):
        try:
            reference_price = float(str(row['ÌåêÎß§Îã®Í∞Ä(VÌè¨Ìï®)']).replace(',', ''))
        except:
            pass

    # Search Naver API
    async with api_semaphore:
        naver_data = await crawl_naver(
            original_query=product_name,
            client=client,
            config=config,
            max_items=naver_scrape_limit,
            reference_price=reference_price
        )

    if not naver_data:
        logger.warning(f"üü¢ No Naver results found for '{product_name}' after trying all keyword variations.")
        return None

    # Get the threshold from config or use a default
    try:
        min_similarity = config.getfloat('Matching', 'naver_minimum_similarity', fallback=0.15)
    except:
        min_similarity = 0.15
    
    # Check the first item's similarity
    first_item = naver_data[0]
    similarity = first_item.get('initial_similarity', 0)
    
    if similarity < min_similarity:
        logger.warning(f"üü¢ Skipping Naver result for '{product_name}' due to low similarity score: {similarity:.3f} < {min_similarity:.3f}")
        return None
    
    # Return the first Naver result with the original product name
    result_data = {
        'original_product_name': product_name,
        'name': first_item.get('name'),
        'price': first_item.get('price'),
        'seller_name': first_item.get('mallName'),
        'link': first_item.get('link'),
        'seller_link': first_item.get('mallProductUrl'),
        'source': 'naver',
        'initial_similarity': similarity,
        'is_naver_site': False,
        'has_quantity_pricing': False,  # Default values for when we skip crawling
        'quantity_prices': {},
        'vat_included': False,
        'has_captcha': False,
        'api_seller_name': first_item.get('mallName', 'N/A'), # Store API seller name
        'promo_detection_details': "Not analyzed",
        'attempted_quantity_price_scrape': False,
        'quantity_price_scrape_reason': "Not attempted"
    }
    
    # ÎÑ§Ïù¥Î≤Ñ ÏÇ¨Ïù¥Ìä∏Ïù∏ÏßÄ Ï≤¥ÌÅ¨
    product_url = result_data['link']
    # seller_name = result_data['seller_name'] # Use api_seller_name for consistency
    
    # Í≥µÍ∏âÏÇ¨Î™ÖÏù¥ 'ÎÑ§Ïù¥Î≤Ñ'Ïù∏ Í≤ΩÏö∞ ÌäπÎ≥Ñ Ï≤òÎ¶¨
    is_naver_seller = result_data['api_seller_name'] == "ÎÑ§Ïù¥Î≤Ñ" or "ÎÑ§Ïù¥Î≤Ñ" in result_data['api_seller_name']
    is_naver_domain = product_url and ("naver.com" in product_url or "shopping.naver.com" in product_url)
    
    result_data['is_naver_site'] = is_naver_domain
    result_data['is_naver_seller'] = is_naver_seller

    # ÌåêÏ¥âÎ¨º ÏÇ¨Ïù¥Ìä∏ Í∞êÏßÄ ÌÇ§ÏõåÎìú ÌôïÏû•
    promo_keywords = [
        'Ïò®Ïò§ÌîÑÎßàÏºì', 'ÎãµÎ°ÄÌíà', 'Í∏∞ÌîÑÌä∏', 'ÌåêÏ¥â', 'Í∏∞ÎÖêÌíà', 'Ïù∏ÏáÑ', 'Í∞ÅÏù∏', 'Ï†úÏûë', 
        'ÎØ∏Ïä§ÌÑ∞Î™ΩÌÇ§', 'ÌôçÎ≥¥', 'Ìò∏Í∞±ÌÉàÏ∂ú', 'Îã§Ï°∞ÏïÑ', 'Í∏∞ÏóÖÌåêÏ¥â', 'Îã®Ï≤¥ÏÑ†Î¨º', 'ÏÇ¨ÏùÄÌíà',
        'ÌôçÎ≥¥Î¨º', 'ÌåêÏ¥âÎ¨º', 'Í∏∞ÏóÖÍ∏∞ÎÖêÌíà', 'Îã®Ï≤¥Ï£ºÎ¨∏', 'ÎåÄÎüâÍµ¨Îß§', 'ÎåÄÎüâÏ£ºÎ¨∏', 'ÎßûÏ∂§Ï†úÏûë',
        'Î°úÍ≥†Ïù∏ÏáÑ', 'Î°úÍ≥†Í∞ÅÏù∏', 'Ï£ºÎ¨∏Ï†úÏûë', 'Ï†úÌíàÌôçÎ≥¥', 'Í∏∞ÏóÖÌôçÎ≥¥', 'Îã®Ï≤¥Íµ¨Îß§'
    ]

    # ÌåêÏ¥âÎ¨º ÏÇ¨Ïù¥Ìä∏ Í∞êÏßÄ Î°úÏßÅ Í∞ïÌôî - Ïô∏Î∂Ä ÏÇ¨Ïù¥Ìä∏Îßå Ï≤¥ÌÅ¨
    is_promotional = False
    matching_keywords_details = []

    # ÏÉÅÌíàÎ™Ö, ÌåêÎß§ÏûêÎ™Ö, ÎßÅÌÅ¨ URLÏóêÏÑú ÌÇ§ÏõåÎìú Í≤ÄÏÇ¨
    for keyword in promo_keywords:
        if keyword.lower() in product_name.lower(): # Check original product name
            is_promotional = True
            matching_keywords_details.append(f"ÏÉÅÌíàÎ™Ö: '{keyword}'")
        if result_data['api_seller_name'] and keyword.lower() in result_data['api_seller_name'].lower():
            is_promotional = True
            matching_keywords_details.append(f"ÌåêÎß§ÏûêÎ™Ö(API): '{keyword}' in '{result_data['api_seller_name']}'")
        if result_data['link'] and keyword.lower() in result_data['link'].lower():
            is_promotional = True
            matching_keywords_details.append(f"ÎßÅÌÅ¨: '{keyword}'")

    if matching_keywords_details:
        result_data['promo_detection_details'] = f"Promotional keywords matched: {', '.join(matching_keywords_details)}"
        logger.info(f"ÌåêÏ¥âÎ¨º ÏÇ¨Ïù¥Ìä∏ Í∞êÏßÄ ({product_name}) - Îß§Ïπ≠Îêú ÌÇ§ÏõåÎìú: {', '.join(matching_keywords_details)}")
    else:
        result_data['promo_detection_details'] = "No promotional keywords matched in product name, API seller name, or link."
    
    result_data['is_promotional_site'] = is_promotional

    # Visit seller site to check for quantity-based pricing only if we have a browser and visit_seller_sites is True
    # AND it's a promotional item
    if visit_seller_sites and browser and first_item.get('link'):
        if is_promotional:
            result_data['attempted_quantity_price_scrape'] = True
            result_data['quantity_price_scrape_reason'] = "Promotional site detected by keywords, attempting quantity price extraction."
            page = None
            try:
                page = await browser.new_page()
                await page.set_viewport_size({"width": 1366, "height": 768})
                
                # First check for CAPTCHA - Navigate to the link from API (first_item.get('link'))
                # This link might be a Naver search/catalog link or a direct seller link
                logger.info(f"Navigating to initial link for CAPTCHA check: {first_item.get('link')}")
                await page.goto(first_item.get('link'), wait_until='networkidle', timeout=30000)
                
                # Check for CAPTCHA using direct detection
                has_captcha_on_initial_page = False
                captcha_selectors = [
                    'form#captcha_form', 
                    'img[alt*="captcha"]',
                    'div.captcha_wrap',
                    'input[name="captchaBotKey"]',
                    'div[class*="captcha"]',
                    'iframe[src*="captcha"]',
                    'div[class*="bot-check"]',
                    'div[class*="security-check"]'
                ]
                for selector in captcha_selectors:
                    if await page.locator(selector).count() > 0:
                        logger.info(f"CAPTCHA detected on initial page: {page.url} for product '{product_name}'")
                        has_captcha_on_initial_page = True
                        break
                
                result_data['has_captcha'] = has_captcha_on_initial_page # Store if captcha was found on initial link

                if has_captcha_on_initial_page:
                    logger.info(f"CAPTCHA detected for '{product_name}'. Skipping further crawling and using API data only.")
                    result_data['quantity_price_scrape_reason'] = f"CAPTCHA detected on initial page ({page.url}), quantity price scrape aborted."
                    # Don't attempt any further crawling, just use the API data
                else:
                    # No CAPTCHA on initial page, proceed with normal crawling
                    # The logic for Naver sellers (clicking "ÏµúÏ†ÄÍ∞Ä") needs to be handled carefully.
                    # The `extract_quantity_prices` function itself handles navigation and further checks.
                    # The URL passed to `extract_quantity_prices` should be the one to scrape.
                    # If it's a Naver seller, `extract_quantity_prices` will try to click the "ÏµúÏ†ÄÍ∞Ä" button.
                    
                    # Determine the URL to pass to extract_quantity_prices
                    url_to_scrape = page.url # Start with the current URL after initial navigation

                    logger.info(f"Calling extract_quantity_prices for '{product_name}' with link: {url_to_scrape}")
                    print(f"Checking quantity prices for '{product_name}'. Please observe the browser window...")
                    
                    quantity_pricing = await extract_quantity_prices(page, url_to_scrape, target_quantities)
                    await asyncio.sleep(1) # Shorter sleep
                    print(f"Completed quantity price check for '{product_name}'")

                    # Update result data with quantity pricing information
                    result_data.update({
                        'has_quantity_pricing': quantity_pricing.get('has_quantity_pricing', False),
                        'quantity_prices': quantity_pricing.get('quantity_prices', {}),
                        'vat_included': quantity_pricing.get('vat_included', False),
                        # 'is_naver_site': quantity_pricing.get('is_naver_site', result_data['is_naver_site']), # Preserve initial detection
                        'has_captcha': result_data['has_captcha'] or quantity_pricing.get('has_captcha', False), # Combine CAPTCHA flags
                        'is_sold_out': quantity_pricing.get('is_sold_out', False),
                        'price_inquiry_needed': quantity_pricing.get('price_inquiry_needed', False)
                    })
                    
                    if result_data['has_captcha'] and not has_captcha_on_initial_page : # CAPTCHA detected by extract_quantity_prices
                         result_data['quantity_price_scrape_reason'] = f"CAPTCHA detected by extract_quantity_prices on {page.url}, scrape might be incomplete."


                    # If sold out or price inquiry needed, log and potentially return None or a modified result_data
                    if result_data['is_sold_out']:
                        logger.info(f"ÏÉÅÌíà '{product_name}' (URL: {page.url})ÏùÄ ÌíàÏ†à ÏÉÅÌÉúÏûÖÎãàÎã§. Í≤∞Í≥ºÎ•º Î∞òÌôòÌïòÏßÄ ÏïäÏäµÎãàÎã§.")
                        result_data['quantity_price_scrape_reason'] = f"Product sold out on {page.url}, quantity price scrape aborted."
                        if page and not page.is_closed(): await page.close()
                        # return None # Skip this product entirely (Re-evaluate if this is desired for testing)
                    
                    elif result_data['price_inquiry_needed']:
                        logger.info(f"ÏÉÅÌíà '{product_name}' (URL: {page.url})ÏùÄ Í∞ÄÍ≤© Î¨∏ÏùòÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§. Í≤∞Í≥ºÎ•º Î∞òÌôòÌïòÏßÄ ÏïäÏäµÎãàÎã§.")
                        result_data['quantity_price_scrape_reason'] = f"Price inquiry needed on {page.url}, quantity price scrape aborted."
                        if page and not page.is_closed(): await page.close()
                        # return None # Skip this product entirely (Re-evaluate if this is desired for testing)

                    # Update promotional site status based on quantity pricing
                    if quantity_pricing.get('has_quantity_pricing'):
                        result_data['is_promotional_site'] = True # Override if quantity pricing found
                        result_data['promo_detection_details'] += " | Also confirmed as promotional due to quantity pricing table."
                        logger.info(f"ÏàòÎüâÎ≥Ñ Í∞ÄÍ≤©Ìëú Î∞úÍ≤¨ÏúºÎ°ú ÌåêÏ¥âÎ¨º ÏÇ¨Ïù¥Ìä∏Î°ú ÏµúÏ¢Ö ÌåêÎã®: {product_name}")
                
            except Exception as e:
                logger.error(f"Error visiting seller site for '{product_name}': {e}")
                result_data['quantity_price_scrape_reason'] = f"Error during seller site visit: {str(e)[:100]}"
            finally:
                if page and not page.is_closed(): # Ensure page is closed only if it was opened
                    try:
                        await page.close()
                    except Exception as e:
                        logger.error(f"Error closing page: {e}")
        elif visit_seller_sites and browser and first_item.get('link') and not is_promotional:
            result_data['attempted_quantity_price_scrape'] = False
            result_data['quantity_price_scrape_reason'] = "Not a promotional site (no keywords matched), seller site visit and quantity price scrape skipped."
            logger.info(f"ÌåêÏ¥âÎ¨ºÏù¥ ÏïÑÎãàÎØÄÎ°ú ÌåêÎß§Ïûê ÏÇ¨Ïù¥Ìä∏ Î∞©Î¨∏ Í±¥ÎÑàÎúÄ: {product_name}")
        elif not visit_seller_sites:
            result_data['attempted_quantity_price_scrape'] = False
            result_data['quantity_price_scrape_reason'] = "Configuration 'naver_visit_seller_sites' is False."
        elif not browser:
            result_data['attempted_quantity_price_scrape'] = False
            result_data['quantity_price_scrape_reason'] = "Browser instance not available for seller site visit."
        elif not first_item.get('link'):
            result_data['attempted_quantity_price_scrape'] = False
            result_data['quantity_price_scrape_reason'] = "No link available from API to visit seller site."


    # Process image if available
    image_api_url = first_item.get('image_url')
    
    # Make sure we have an image URL - if 'image_url' is not present, try 'image' field
    if not image_api_url:
        image_api_url = first_item.get('image')
    
    # If still no image URL, log a warning
    if not image_api_url:
        logger.warning(f"No image URL found for product '{product_name}'. Using default placeholder URL.")
        # Use a default placeholder URL
        image_api_url = "https://via.placeholder.com/300"
    
    # Initialize paths
    local_path = None
    abs_local_path = None
    
    # Try to download the image
    try:
        # Only attempt download if we have a valid URL
        if image_api_url and image_api_url.startswith(('http://', 'https://')):
            local_path = await download_naver_image(image_api_url, naver_image_dir, product_name, config)
            
            if local_path:
                abs_local_path = os.path.abspath(local_path)
                logger.info(f"Successfully downloaded image for '{product_name}' to {abs_local_path}")
            else:
                logger.warning(f"Failed to download image for '{product_name}' from {image_api_url}")
    except Exception as e:
        logger.error(f"Error processing image for '{product_name}': {e}")
    
    # Always include image information in result data, even if download failed
    image_data_for_df = {
        'url': image_api_url,
        'local_path': abs_local_path,
        'original_path': abs_local_path,
        'source': 'naver',
        'product_name': product_name,
        'similarity': max(similarity, 0.1),  # ÏµúÏÜå Ïú†ÏÇ¨ÎèÑ Î≥¥Ïû• (ÌïÑÌÑ∞ÎßÅ Î∞©ÏßÄ)
        'type': 'naver',
        'product_id': first_item.get('productId')
    }
    
    # Ensure we have image data even if download failed
    result_data['image_data'] = image_data_for_df
    result_data['naver_image_data'] = image_data_for_df
    result_data['image_url'] = image_api_url
    result_data['image_path'] = abs_local_path
    
    # Include additional links
    result_data['ÎÑ§Ïù¥Î≤Ñ ÏáºÌïë ÎßÅÌÅ¨'] = first_item.get('link', '')
    result_data['Í≥µÍ∏âÏÇ¨ ÏÉÅÌíàÎßÅÌÅ¨'] = first_item.get('mallProductUrl', first_item.get('link', ''))
    
    # Add Í≥µÍ∏âÏÇ¨Î™Ö (supplier name) explicitly to ensure it's properly propagated
    result_data['Í≥µÍ∏âÏÇ¨Î™Ö'] = first_item.get('mallName', first_item.get('seller_name', ''))
    
    # Create ÎÑ§Ïù¥Î≤Ñ Ïù¥ÎØ∏ÏßÄ entry
    result_data['ÎÑ§Ïù¥Î≤Ñ Ïù¥ÎØ∏ÏßÄ'] = {
        'url': image_api_url,
        'local_path': abs_local_path,
        'source': 'naver',
        'score': similarity,
        'similarity': max(similarity, 0.1),  # ÏµúÏÜå Ïú†ÏÇ¨ÎèÑ Î≥¥Ïû• (ÌïÑÌÑ∞ÎßÅ Î∞©ÏßÄ)
        'product_id': first_item.get('productId'),
        'original_path': abs_local_path
    }
    
    return result_data


async def _run_single_naver_search(idx: int, row: pd.Series, product_name: str, product_type: str, reference_price: float, client: httpx.AsyncClient, config: configparser.ConfigParser, naver_scrape_limit: int, api_semaphore: asyncio.Semaphore) -> Tuple[int, pd.Series, str, Optional[List[Dict]]]:
    """ Helper coroutine to handle the logic for a single product's Naver API search. """
    task_id = f"Task-{idx}" # Use index for task identification
    logger.debug(f"[{task_id}] Acquiring semaphore for Naver search: '{product_name}'...")
    async with api_semaphore:
        logger.debug(f"[{task_id}] Semaphore acquired. Starting Naver search for '{product_name}'.")
        start_time = time.monotonic()
        try:
            api_results = await crawl_naver(
                original_query=product_name,
                client=client,
                config=config,
                max_items=naver_scrape_limit,
                reference_price=reference_price
            )
            elapsed_time = time.monotonic() - start_time
            logger.debug(f"[{task_id}] Completed Naver search for '{product_name}' in {elapsed_time:.2f}s. Found {len(api_results) if api_results else 0} items.")
            # Return index, original row data, type, and the API results list
            return idx, row, product_type, api_results
        except Exception as e:
            elapsed_time = time.monotonic() - start_time
            logger.error(f"üü¢ [{task_id}] Error during Naver search task for '{product_name}' after {elapsed_time:.2f}s: {e}", exc_info=True)
            # Ensure we return the index and row even on failure, but with None for results
            # This allows the main function to know which task failed.
            return idx, row, product_type, None
        finally:
             logger.debug(f"[{task_id}] Releasing semaphore for '{product_name}'.")


# --- Test block Updated for Async ---
async def _test_main():
    """Test Naver API and price crawling functionality"""
    # Setup basic logging for the test
    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s:%(name)s:%(lineno)d - %(message)s')
    logging.getLogger("httpx").setLevel(logging.WARNING)
    print("--- Running Naver API Test with Promotional Items ---")
    logger.info("--- Running Naver API Test with Promotional Items (Async) ---")

    # Load config
    script_dir = os.path.dirname(os.path.abspath(__file__))
    config_path = os.path.join(script_dir, '..', 'config.ini')

    try:
        config = load_config(config_path)
        print(f"Config loaded from: {config_path}")
    except Exception as e:
        print(f"ERROR loading config: {e}")
        return

    # Test products including promotional items
    test_data = {
        'Íµ¨Î∂Ñ': ['A', 'A', 'A', 'A'],  # Adjusted for four products
        'Îã¥ÎãπÏûê': ['ÌÖåÏä§Ìä∏', 'ÌÖåÏä§Ìä∏', 'ÌÖåÏä§Ìä∏', 'ÌÖåÏä§Ìä∏'],
        'ÏóÖÏ≤¥Î™Ö': ['ÌÖåÏä§Ìä∏ÏóÖÏ≤¥', 'ÌÖåÏä§Ìä∏ÏóÖÏ≤¥', 'ÌÖåÏä§Ìä∏ÏóÖÏ≤¥', 'ÌÖåÏä§Ìä∏ÏóÖÏ≤¥'],
        'ÏóÖÏ≤¥ÏΩîÎìú': ['T001', 'T001', 'T001', 'T001'],
        'Code': ['CODE001', 'CODE002', 'CODE003', 'CODE004'],
        'Ï§ëÎ∂ÑÎ•òÏπ¥ÌÖåÍ≥†Î¶¨': ['ÌÖåÏä§Ìä∏Ïπ¥ÌÖåÍ≥†Î¶¨', 'ÌÖåÏä§Ìä∏Ïπ¥ÌÖåÍ≥†Î¶¨', 'ÌÖåÏä§Ìä∏Ïπ¥ÌÖåÍ≥†Î¶¨', 'ÌÖåÏä§Ìä∏Ïπ¥ÌÖåÍ≥†Î¶¨'],
        'ÏÉÅÌíàÎ™Ö': [
            'Î©ÄÌã∞ ÏïÑÏø†ÏïÑ Ïø®ÌÜ†Ïãú',  # Test product 1
            'ÏõêÌòï ÎØ∏ÎãàÍ±∞Ïö∏ 3TMM007',  # Test product 2
            'Î©îÏâ¨Í∞ÄÎ∞© ÎåÄÌòï ÎπÑÏπòÎ∞± ÎßùÏÇ¨Í∞ÄÎ∞© ÎπÑÏπòÍ∞ÄÎ∞© 43X39X20',  # Test product 3
            'ÏóêÌÅ¥Î¶¨Ï¶à Í≥†Í∏â ÏΩîÌåÖ Î∂ÄÏßÅÌè¨ ÏáºÌïëÎ∞± ÎåÄÌòï'  # Test product 4 (promotional site)
        ],
        'Í∏∞Î≥∏ÏàòÎüâ(1)': [300, 500, 1000, 1000],  # Adjusted quantities for testing
        'ÌåêÎß§Îã®Í∞Ä(VÌè¨Ìï®)': [15000, 20000, 25000, 30000],
        'Î≥∏ÏÇ¨ÏÉÅÌíàÎßÅÌÅ¨': ['', '', '', '']
    }
    test_df = pd.DataFrame(test_data)
    
    print("\n=== Testing with promotional items ===")
    print("Test products:")
    for idx, name in enumerate(test_df['ÏÉÅÌíàÎ™Ö'], 1):
        print(f"{idx}. {name}")

    try:
        # Initialize Playwright
        async with async_playwright() as playwright:
            try:
                # Switch to non-headless mode to observe the crawling process
                browser = await playwright.chromium.launch(headless=False)
                print(f"Browser launched successfully: {browser}")
                
                print("\n--- Testing crawl_naver_products with promotional items ---")
                result_list = await crawl_naver_products(test_df, config, browser=browser)
                
                # Print results
                for idx, result in enumerate(result_list, 1):
                    print(f"\nProduct {idx}: {result.get('original_product_name', 'Unknown')}")
                    print("-" * 50)
                    
                    # Basic info
                    print(f"Matched name: {result.get('name', 'No match')}")
                    print(f"Price: {result.get('price', 'N/A'):,}Ïõê")
                    # print(f"Seller: {result.get('seller_name', 'Unknown')}") # Original seller_name from API
                    print(f"API Seller Name: {result.get('api_seller_name', 'N/A')}")
                    print(f"Similarity score: {result.get('initial_similarity', 0):.3f}")
                    
                    # Links
                    print(f"ÎÑ§Ïù¥Î≤Ñ ÏáºÌïë ÎßÅÌÅ¨: {result.get('link', 'No link')}")
                    print(f"Í≥µÍ∏âÏÇ¨ ÏÉÅÌíàÎßÅÌÅ¨: {result.get('seller_link', 'No link')}")
                    
                    # Promotional site check
                    is_promo = result.get('is_promotional_site', False)
                    has_qty_pricing = result.get('has_quantity_pricing', False)
                    print(f"Is promotional site: {'Yes' if is_promo else 'No'}")
                    print(f"Promotional Detection Details: {result.get('promo_detection_details', 'N/A')}")
                    
                    # Quantity price scraping attempt details
                    print(f"Attempted Quantity Price Scrape: {'Yes' if result.get('attempted_quantity_price_scrape') else 'No'}")
                    print(f"Quantity Price Scrape Reason: {result.get('quantity_price_scrape_reason', 'N/A')}")
                    print(f"Has quantity pricing (from scrape): {'Yes' if has_qty_pricing else 'No'}")
                    print(f"CAPTCHA Detected: {'Yes' if result.get('has_captcha', False) else 'No'}")
                    print(f"Sold Out: {'Yes' if result.get('is_sold_out', False) else 'No'}")
                    print(f"Price Inquiry Needed: {'Yes' if result.get('price_inquiry_needed', False) else 'No'}")

                    # Add error information if any
                    if result.get('error'):
                        print(f"Error: {result.get('error')}")
                    
                    # Quantity prices if available
                    qty_prices = result.get('quantity_prices', {})
                    if qty_prices:
                        print("\nQuantity-based prices:")
                        print("-" * 50)
                        print("| {:^8} | {:^12} | {:^12} | {:^20} |".format(
                            "ÏàòÎüâ", "Îã®Í∞Ä", "VATÌè¨Ìï®", "ÎπÑÍ≥†"))
                        print("-" * 50)
                        
                        # Ensure sorted display of quantities (smallest to largest)
                        sorted_quantities = sorted(qty_prices.keys())
                        
                        for qty in sorted_quantities:
                            price_info = qty_prices[qty]
                            
                            # Ensure numeric types for proper formatting
                            try:
                                qty_num = int(qty)
                                price = int(price_info.get('price', 0))
                                price_vat = int(price_info.get('price_with_vat', 0))
                                note = "Ï†ïÌôïÌïú ÏàòÎüâ" if price_info.get('exact_match', False) else "Í∑ºÏÇ¨Ïπò"
                                
                                print("| {:>8,d} | {:>12,} | {:>12,} | {:<20} |".format(
                                    qty_num, price, price_vat, note))
                            except (ValueError, TypeError) as e:
                                print(f"Error formatting quantity {qty}: {e}")
                        print("-" * 50)
                    
                    # Image information
                    if 'image_data' in result:
                        img_data = result['image_data']
                        print("\nImage information:")
                        print(f"URL: {img_data.get('url', 'No URL')}")
                        local_path = img_data.get('local_path', 'No local path')
                        print(f"Local path: {local_path}")
                        
                        if local_path and os.path.exists(local_path):
                            size = os.path.getsize(local_path)
                            print(f"Image file size: {size:,} bytes")
                        else:
                            print("Warning: Image file not found locally")
            except Exception as browser_error:
                print(f"\n‚õî BROWSER ERROR: {browser_error}")
                logger.error(f"Browser error: {browser_error}", exc_info=True)
            finally:
                if browser:
                    await browser.close()

            # Summary
            print("\n=== Test Summary ===")
            total_results = len(result_list)
            promo_sites = sum(1 for r in result_list if r.get('is_promotional_site', False))
            with_qty_pricing = sum(1 for r in result_list if r.get('has_quantity_pricing', False))
            with_images = sum(1 for r in result_list if 'image_data' in r)
            
            print(f"Total products processed: {total_results}")
            print(f"Promotional sites detected: {promo_sites}")
            print(f"Products with quantity pricing: {with_qty_pricing}")
            print(f"Products with images: {with_images}")
            
            if total_results == 0:
                print("\n‚õî TEST FAILED: No results returned")
            else:
                print("\n‚úÖ TEST COMPLETED SUCCESSFULLY")
    
    except Exception as e:
        print(f"\n‚õî TEST ERROR: {e}")
        logger.error(f"Test error: {e}", exc_info=True)
    
    print("\n--- Naver API Test Finished ---")

def generate_output_filename(base_name: str, timestamp: str = None) -> str:
    """
    Generate a properly formatted output filename.
    
    Args:
        base_name: Base name for the file
        timestamp: Optional timestamp string
        
    Returns:
        Properly formatted filename
    """
    # Clean the base name
    base_name = base_name.strip()
    
    # Generate timestamp if not provided
    if not timestamp:
        now = datetime.now()
        timestamp = now.strftime("%Y%m%d_%H%M%S")
    
    # Create filename with proper path handling
    filename = f"{base_name}-{timestamp}.xlsx"
    
    # Normalize the path to handle Korean characters
    output_dir = os.path.normpath(os.path.join('C:', 'RPA', 'Output'))
    full_path = os.path.normpath(os.path.join(output_dir, filename))
    
    # Ensure the directory exists
    os.makedirs(output_dir, exist_ok=True)
    
    return full_path

def save_results_to_excel(df: pd.DataFrame, base_name: str = "ÏäπÏù∏Í¥ÄÎ¶¨", timestamp: str = None) -> str:
    """
    Save results to Excel file with proper path handling.
    
    Args:
        df: DataFrame to save
        base_name: Base name for the output file
        timestamp: Optional timestamp string
        
    Returns:
        Path to the saved file
    """
    try:
        # Generate proper output path
        output_path = generate_output_filename(base_name, timestamp)
        
        # Ensure the directory exists
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # Save with encoding specification
        df.to_excel(output_path, index=False, engine='openpyxl')
        logger.info(f"Successfully saved results to: {output_path}")
        
        return output_path
    except Exception as e:
        logger.error(f"Error saving results to Excel: {e}")
        raise

async def process_matching(df: pd.DataFrame, config: configparser.ConfigParser, browser=None) -> Tuple[pd.DataFrame, str]:
    """Process product matching with proper output handling."""
    try:
        # ... existing processing code ...
        
        # Generate timestamp once for consistent naming
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save results with proper path handling
        if len(df) <= 10:
            base_name = f"ÏÑ±ÏùÄ({len(df)}Í∞ú)-ÏäπÏù∏Í¥ÄÎ¶¨"
        else:
            base_name = f"ÏäπÏù∏Í¥ÄÎ¶¨"
        
        output_path = save_results_to_excel(df, base_name, timestamp)
        
        return df, output_path
    except Exception as e:
        logger.error(f"Error in process_matching: {e}")
        raise

async def main(input_file: str = None, output_file: str = None, config_file: str = None):
    """
    Main entry point for the Naver crawler.
    
    Args:
        input_file: Path to input Excel file
        output_file: Path to output Excel file (optional)
        config_file: Path to config file (optional)
    """
    try:
        # Set up logging
        setup_logging()
        
        # Load configuration
        config = load_config(config_file)
        
        # Read input file
        df = pd.read_excel(input_file) if input_file else create_test_dataframe()
        
        # Process matching
        result_df, output_path = await process_matching(df, config)
        
        logger.info(f"Processing completed successfully. Output saved to: {output_path}")
        return output_path
        
    except Exception as e:
        logger.error(f"Error in main function: {e}")
        raise

def setup_logging():
    """Configure logging with proper encoding for Korean characters."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler('naver_crawler.log', encoding='utf-8')
        ]
    )

def load_config(config_file: str = None) -> configparser.ConfigParser:
    """Load configuration with proper encoding."""
    config = configparser.ConfigParser()
    
    if config_file and os.path.exists(config_file):
        config.read(config_file, encoding='utf-8')
    
    return config

def create_test_dataframe() -> pd.DataFrame:
    """Creates a sample DataFrame for testing purposes."""
    logger.info("Creating a sample DataFrame for testing as no input file was provided.")
    test_data = {
        'Íµ¨Î∂Ñ': ['A', 'A', 'A'],
        'Îã¥ÎãπÏûê': ['ÌÖåÏä§Ìä∏', 'ÌÖåÏä§Ìä∏', 'ÌÖåÏä§Ìä∏'],
        'ÏóÖÏ≤¥Î™Ö': ['ÌÖåÏä§Ìä∏ÏóÖÏ≤¥', 'ÌÖåÏä§Ìä∏ÏóÖÏ≤¥', 'ÌÖåÏä§Ìä∏ÏóÖÏ≤¥'],
        'ÏóÖÏ≤¥ÏΩîÎìú': ['T001', 'T001', 'T001'],
        'Code': ['CODE001', 'CODE002', 'CODE003'],
        'Ï§ëÎ∂ÑÎ•òÏπ¥ÌÖåÍ≥†Î¶¨': ['ÌÖåÏä§Ìä∏Ïπ¥ÌÖåÍ≥†Î¶¨', 'ÌÖåÏä§Ìä∏Ïπ¥ÌÖåÍ≥†Î¶¨', 'ÌÖåÏä§Ìä∏Ïπ¥ÌÖåÍ≥†Î¶¨'],
        'ÏÉÅÌíàÎ™Ö': [
            'Î©ÄÌã∞ ÏïÑÏø†ÏïÑ Ïø®ÌÜ†Ïãú',
            'ÏõêÌòï ÎØ∏ÎãàÍ±∞Ïö∏ 3TMM007',
            'Î©îÏâ¨Í∞ÄÎ∞© ÎåÄÌòï ÎπÑÏπòÎ∞± ÎßùÏÇ¨Í∞ÄÎ∞© ÎπÑÏπòÍ∞ÄÎ∞© 43X39X20'
        ],
        'Í∏∞Î≥∏ÏàòÎüâ(1)': [300, 500, 1000],
        'ÌåêÎß§Îã®Í∞Ä(VÌè¨Ìï®)': [15000, 20000, 25000],
        'Î≥∏ÏÇ¨ÏÉÅÌíàÎßÅÌÅ¨': ['', '', '']
    }
    return pd.DataFrame(test_data)

if __name__ == "__main__":
    # Set up logging
    logging.basicConfig(level=logging.INFO)
    logging.getLogger("httpx").setLevel(logging.WARNING)
    
    print("Running Naver API test with promotional items...")
    asyncio.run(_test_main())
