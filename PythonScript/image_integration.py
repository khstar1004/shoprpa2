import os
import logging
import pandas as pd
from pathlib import Path
import configparser
from typing import Dict, Any, Optional, List, Tuple, Set, Union
from openpyxl import Workbook
from openpyxl.drawing.image import Image
from openpyxl.utils import get_column_letter
import shutil
import sys
import re
import hashlib
from datetime import datetime
import glob
import time
import json
import numpy as np
import tqdm
import traceback
import openpyxl
import PIL

# Add the parent directory to sys.path to allow imports from PythonScript
current_dir = Path(__file__).resolve().parent
parent_dir = current_dir.parent
if str(parent_dir) not in sys.path:
    sys.path.insert(0, str(parent_dir))

# Import common utilities first
try:
    from .utils import generate_product_name_hash, extract_product_hash_from_filename
    from .tokenize_product_names import tokenize_product_name, extract_meaningful_keywords
    logging.info("‚úÖ Í≥µÌÜµ Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§ÏùÑ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú importÌñàÏäµÎãàÎã§.")
except ImportError:
    try:
        from utils import generate_product_name_hash, extract_product_hash_from_filename
        from tokenize_product_names import tokenize_product_name, extract_meaningful_keywords
        logging.info("‚úÖ Í≥µÌÜµ Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§ÏùÑ ÏßÅÏ†ë importÌñàÏäµÎãàÎã§.")
    except ImportError as e:
        logging.error(f"‚ùå Í≥µÌÜµ Ïú†Ìã∏Î¶¨Ìã∞ Ìï®Ïàò import Ïã§Ìå®: {e}")
        # Fallback implementations
        def generate_product_name_hash(product_name: str) -> str:
            """Fallback hash generation"""
            try:
                normalized = ''.join(product_name.split()).lower()
                return hashlib.md5(normalized.encode('utf-8')).hexdigest()[:16]
            except Exception:
                return ""
        
        def extract_product_hash_from_filename(filename: str) -> Optional[str]:
            """Fallback hash extraction"""
            try:
                name = os.path.splitext(os.path.basename(filename))[0]
                parts = name.split('_')
                if len(parts) >= 2 and len(parts[1]) == 16:
                    return parts[1].lower()
                return None
            except Exception:
                return None

# Initialize logger
logger = logging.getLogger(__name__)

# Import enhanced image matcher with improved error handling
try:
    from .enhanced_image_matcher import EnhancedImageMatcher, check_gpu_status
    ENHANCED_MATCHER_AVAILABLE = True
    logging.info("‚úÖ Í≥†Í∏â Ïù¥ÎØ∏ÏßÄ Îß§Ï≤òÎ•º ÏÑ±Í≥µÏ†ÅÏúºÎ°ú importÌñàÏäµÎãàÎã§.")
except ImportError:
    try:
        from enhanced_image_matcher import EnhancedImageMatcher, check_gpu_status
        ENHANCED_MATCHER_AVAILABLE = True
        logging.info("‚úÖ Í≥†Í∏â Ïù¥ÎØ∏ÏßÄ Îß§Ï≤òÎ•º ÏßÅÏ†ë importÌñàÏäµÎãàÎã§.")
    except ImportError:
        ENHANCED_MATCHER_AVAILABLE = False
        logging.warning("‚ö†Ô∏è Í≥†Í∏â Ïù¥ÎØ∏ÏßÄ Îß§Ï≤òÎ•º ÏÇ¨Ïö©Ìï† Ïàò ÏóÜÏäµÎãàÎã§. Í∏∞Î≥∏ ÌÖçÏä§Ìä∏ Í∏∞Î∞ò Îß§Ïπ≠ÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§.")

def prepare_image_metadata(image_dir: Path, prefix: str, prefer_original: bool = True, prefer_jpg: bool = True) -> Dict[str, Dict]:
    """
    Ïù¥ÎØ∏ÏßÄ ÎîîÎ†âÌÜ†Î¶¨ÏóêÏÑú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Î•º Ï∂îÏ∂úÌï¥ Ïù∏Îç±Ïä§Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§.
    
    Args:
        image_dir: Ïù¥ÎØ∏ÏßÄ ÎîîÎ†âÌÜ†Î¶¨ Í≤ΩÎ°ú
        prefix: Ïù¥ÎØ∏ÏßÄ ÏÜåÏä§ Íµ¨Î∂ÑÏö© Ï†ëÎëêÏÇ¨ (Ïòà: 'haereum', 'kogift', 'naver')
        prefer_original: _nobgÍ∞Ä ÏïÑÎãå ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Ïö∞ÏÑ†ÏãúÌï†ÏßÄ Ïó¨Î∂Ä
        prefer_jpg: PNGÎ≥¥Îã§ JPG ÌååÏùºÏùÑ Ïö∞ÏÑ†ÏãúÌï†ÏßÄ Ïó¨Î∂Ä
        
    Returns:
        Ïù¥ÎØ∏ÏßÄ Í≤ΩÎ°úÎ•º ÌÇ§Î°ú, Ïù¥ÎØ∏ÏßÄ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Î•º Í∞íÏúºÎ°ú ÌïòÎäî ÎîïÏÖîÎÑàÎ¶¨
    """
    image_info = {}
    
    # Normalize path to use forward slashes
    abs_image_dir = os.path.abspath(str(image_dir)).replace('\\', '/')
    logging.info(f"Preparing image metadata from directory: {abs_image_dir} (prefix: {prefix}, prefer_original: {prefer_original}, prefer_jpg: {prefer_jpg})")
    
    # Handle case where directory doesn't exist
    if not os.path.exists(abs_image_dir):
        logging.warning(f"Image directory does not exist: {abs_image_dir}")
        return {}
    
    # First look for image files in the directory
    all_image_files = []
    valid_extensions = ('.jpg', '.jpeg', '.png', '.gif')
    
    try:
        # Get all image files with normalized paths
        for root, _, files in os.walk(abs_image_dir):
            root = root.replace('\\', '/')  # Normalize path
            for file in files:
                if file.lower().endswith(valid_extensions):
                    full_path = os.path.join(root, file).replace('\\', '/')
                    all_image_files.append(full_path)
        
        logging.info(f"Found {len(all_image_files)} total images in {abs_image_dir}")
        
        # Group images by base name (without _nobg suffix)
        image_groups = {}
        
        for img_path in all_image_files:
            filename = os.path.basename(img_path)
            file_root, file_ext = os.path.splitext(filename)
            
            # Handle _nobg suffix
            base_name = file_root
            is_nobg = False
            if file_root.endswith('_nobg'):
                base_name = file_root[:-5]  # Remove _nobg suffix
                is_nobg = True
                
            # Group by base name
            if base_name not in image_groups:
                image_groups[base_name] = {'original_jpg': None, 'original_png': None, 'nobg_png': None, 'nobg_jpg': None}
                
            # Store the path based on type and extension
            if is_nobg:
                if file_ext.lower() in ['.jpg', '.jpeg']:
                    image_groups[base_name]['nobg_jpg'] = img_path
                else:
                    image_groups[base_name]['nobg_png'] = img_path
            else:
                if file_ext.lower() in ['.jpg', '.jpeg']:
                    image_groups[base_name]['original_jpg'] = img_path
                else:
                    image_groups[base_name]['original_png'] = img_path
        
        # Process each group and prioritize files according to preferences
        for base_name, paths in image_groups.items():
            try:
                # Select the best path based on preferences
                img_path = None
                
                # Priority order based on preferences:
                if prefer_original and prefer_jpg:
                    # 1. Original JPG
                    # 2. Original PNG
                    # 3. Nobg JPG
                    # 4. Nobg PNG
                    if paths['original_jpg']:
                        img_path = paths['original_jpg']
                    elif paths['original_png']:
                        img_path = paths['original_png']
                    elif paths['nobg_jpg']:
                        img_path = paths['nobg_jpg']
                    elif paths['nobg_png']:
                        img_path = paths['nobg_png']
                elif prefer_original and not prefer_jpg:
                    # 1. Original PNG
                    # 2. Original JPG
                    # 3. Nobg PNG
                    # 4. Nobg JPG
                    if paths['original_png']:
                        img_path = paths['original_png']
                    elif paths['original_jpg']:
                        img_path = paths['original_jpg']
                    elif paths['nobg_png']:
                        img_path = paths['nobg_png']
                    elif paths['nobg_jpg']:
                        img_path = paths['nobg_jpg']
                elif not prefer_original and prefer_jpg:
                    # 1. Nobg JPG
                    # 2. Original JPG
                    # 3. Nobg PNG
                    # 4. Original PNG
                    if paths['nobg_jpg']:
                        img_path = paths['nobg_jpg']
                    elif paths['original_jpg']:
                        img_path = paths['original_jpg']
                    elif paths['nobg_png']:
                        img_path = paths['nobg_png']
                    elif paths['original_png']:
                        img_path = paths['original_png']
                else:
                    # not prefer_original and not prefer_jpg
                    # 1. Nobg PNG
                    # 2. Original PNG
                    # 3. Nobg JPG
                    # 4. Original JPG
                    if paths['nobg_png']:
                        img_path = paths['nobg_png']
                    elif paths['original_png']:
                        img_path = paths['original_png']
                    elif paths['nobg_jpg']:
                        img_path = paths['nobg_jpg']
                    elif paths['original_jpg']:
                        img_path = paths['original_jpg']
                
                # Skip if no image found
                if not img_path:
                    continue
                
                # Always store all available paths for reference
                original_jpg_path = paths['original_jpg']
                original_png_path = paths['original_png']
                nobg_png_path = paths['nobg_png']
                nobg_jpg_path = paths['nobg_jpg']
                
                # Extract metadata
                filename = os.path.basename(img_path)
                file_root, file_ext = os.path.splitext(filename)
                
                # Create metadata dictionary
                metadata = {
                    'path': img_path,
                    'filename': filename,
                    'extension': file_ext.lower(),
                    'is_original': not file_root.endswith('_nobg'),
                    'original_jpg_path': original_jpg_path,
                    'original_png_path': original_png_path,
                    'nobg_jpg_path': nobg_jpg_path,
                    'nobg_png_path': nobg_png_path,
                    'source': prefix.rstrip('_'),
                    'base_name': base_name
                }
                
                # Store metadata in image_info dictionary
                image_info[img_path] = metadata
                
                # Debug some sample entries
                if len(image_info) <= 2 or len(image_info) % 50 == 0:
                    logging.debug(f"Image metadata sample: {img_path} -> {metadata}")
            
            except Exception as e:
                import traceback
                stack_trace = traceback.format_exc()
                logging.error(f"Error processing image file {base_name}: {e}")
                logging.debug(f"Exception traceback: {stack_trace}")
                continue  # Skip this image but continue processing others
        
        # Log summary
        logging.info(f"Processed {len(image_info)} {prefix} images")
        
        # Additional debug information
        if image_info:
            logging.debug(f"First 3 image keys in {prefix} image_info: {list(image_info.keys())[:3]}")
        else:
            logging.warning(f"No images were successfully processed for {prefix}")
        
        return image_info
        
    except Exception as e:
        import traceback
        stack_trace = traceback.format_exc()
        logging.error(f"Error preparing image metadata from {abs_image_dir}: {e}")
        logging.debug(f"Exception traceback: {stack_trace}")
        return {}

def calculate_similarity(product_tokens: List[str], image_tokens: List[str]) -> float:
    """
    ÏÉÅÌíàÎ™ÖÍ≥º Ïù¥ÎØ∏ÏßÄ Ïù¥Î¶Ñ Í∞ÑÏùò Ïú†ÏÇ¨ÎèÑÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§.
    
    Args:
        product_tokens: ÏÉÅÌíàÎ™ÖÏóêÏÑú Ï∂îÏ∂úÌïú ÌÜ†ÌÅ∞ Î™©Î°ù
        image_tokens: Ïù¥ÎØ∏ÏßÄ Ïù¥Î¶ÑÏóêÏÑú Ï∂îÏ∂úÌïú ÌÜ†ÌÅ∞ Î™©Î°ù
        
    Returns:
        Ïú†ÏÇ¨ÎèÑ Ï†êÏàò (0.0 ~ 1.0)
    """
    # ÌÜ†ÌÅ∞ Í∏∞Î∞ò Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
    common_tokens = set(product_tokens) & set(image_tokens)
    
    # Îçî Ï†ïÌôïÌïú Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞ - ÌÜ†ÌÅ∞Ïùò Í∏∏Ïù¥ÏôÄ ÏàòÎ•º Í≥†Î†§
    total_tokens = len(set(product_tokens) | set(image_tokens))
    if total_tokens == 0:
        return 0.0
        
    similarity = len(common_tokens) / total_tokens
    
    # Îçî Í∏¥ ÌÜ†ÌÅ∞Ïù¥ Îß§Ïπ≠ÎêòÎ©¥ Í∞ÄÏ§ëÏπò Î∂ÄÏó¨
    weight = 1.0
    for token in common_tokens:
        if len(token) >= 4:  # 4Í∏ÄÏûê Ïù¥ÏÉÅ ÌÜ†ÌÅ∞Ïóê Í∞ÄÏ§ëÏπò
            weight += 0.1
    
    return min(similarity * weight, 1.0) # Ensure score doesn't exceed 1.0

def find_best_image_matches(product_names: List[str], 
                           haereum_images: Dict[str, Dict], 
                           kogift_images: Dict[str, Dict], 
                           naver_images: Dict[str, Dict],
                           similarity_threshold: float = 0.8,  # Îçî ÏóÑÍ≤©Ìïú ÏûÑÍ≥ÑÍ∞íÏúºÎ°ú Î≥ÄÍ≤Ω
                           config: Optional[configparser.ConfigParser] = None,
                           df: Optional[pd.DataFrame] = None) -> List[Tuple[Optional[str], Optional[str], Optional[str]]]:
    """
    Í∞úÏÑ†Îêú 2Îã®Í≥Ñ ÏÉÅÌíà Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠ ÏãúÏä§ÌÖú
    
    Îã®Í≥Ñ 1: Ìï¥Ïãú Í∏∞Î∞ò Ï†ïÌôïÌïú Îß§Ïπ≠ (MD5 Ìï¥Ïãú ÎπÑÍµê)
    Îã®Í≥Ñ 2: Ïù¥ÎØ∏ÏßÄ Ïú†ÏÇ¨ÎèÑ Í≤ÄÏ¶ù (0.8 ÏûÑÍ≥ÑÍ∞í)
    
    Args:
        product_names: Îß§Ïπ≠Ìï† ÏÉÅÌíàÎ™Ö Î¶¨Ïä§Ìä∏
        haereum_images: Ìï¥Ïò§Î¶Ñ Ïù¥ÎØ∏ÏßÄ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÎîïÏÖîÎÑàÎ¶¨
        kogift_images: Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÎîïÏÖîÎÑàÎ¶¨
        naver_images: ÎÑ§Ïù¥Î≤Ñ Ïù¥ÎØ∏ÏßÄ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÎîïÏÖîÎÑàÎ¶¨
        similarity_threshold: Ïù¥ÎØ∏ÏßÄ Ïú†ÏÇ¨ÎèÑ ÏûÑÍ≥ÑÍ∞í (Í∏∞Î≥∏: 0.8)
        config: ÏÑ§Ï†ï Í∞ùÏ≤¥
        df: ÏÉÅÌíà Ï†ïÎ≥¥ DataFrame
        
    Returns:
        Í∞Å ÏÉÅÌíàÏóê ÎåÄÌïú (haereum_match, kogift_match, naver_match) ÌäúÌîå Î¶¨Ïä§Ìä∏
    """
    
    logging.info("üöÄ Í∞úÏÑ†Îêú 2Îã®Í≥Ñ Îß§Ïπ≠ ÏãúÏä§ÌÖú ÏãúÏûë")
    logging.info(f"üìä ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞: ÏÉÅÌíà {len(product_names)}Í∞ú, Ìï¥Ïò§Î¶Ñ {len(haereum_images)}Í∞ú, "
                f"Í≥†Î†§Í∏∞ÌîÑÌä∏ {len(kogift_images)}Í∞ú, ÎÑ§Ïù¥Î≤Ñ {len(naver_images)}Í∞ú")
    
    # ÏÑ§Ï†ïÍ∞í Î°úÎìú
    if config:
        try:
            similarity_threshold = config.getfloat('ImageMatching', 'similarity_threshold', fallback=similarity_threshold)
        except (configparser.Error, ValueError):
            logging.warning(f"ÏÑ§Ï†ïÏóêÏÑú similarity_thresholdÎ•º ÏùΩÏùÑ Ïàò ÏóÜÏùå, Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©: {similarity_threshold}")
    
    # Í≥†Í∏â Ïù¥ÎØ∏ÏßÄ Îß§Ï≤ò Ï¥àÍ∏∞Ìôî
    enhanced_matcher = None
    try:
        from enhanced_image_matcher import EnhancedImageMatcher
        enhanced_matcher = EnhancedImageMatcher(config)
        use_gpu = getattr(enhanced_matcher, 'use_gpu', False)
        logging.info(f"‚úÖ Í≥†Í∏â Ïù¥ÎØ∏ÏßÄ Îß§Ï≤ò Ï¥àÍ∏∞Ìôî ÏôÑÎ£å (GPU: {use_gpu})")
    except Exception as e:
        logging.error(f"‚ùå Í≥†Í∏â Ïù¥ÎØ∏ÏßÄ Îß§Ï≤ò Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
        logging.warning("Í∏∞Î≥∏ Îß§Ïπ≠ Î∞©ÏãùÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§.")
    
    # Îß§Ïπ≠ Í≤∞Í≥º Î∞è ÏÇ¨Ïö©Îêú Ïù¥ÎØ∏ÏßÄ Ï∂îÏ†Å
    best_matches = []
    used_haereum = set()
    used_kogift = set()
    used_naver = set()
    
    # ÌÜµÍ≥Ñ Î≥ÄÏàò
    hash_matches = 0
    image_verified = 0
    no_matches = 0
    
    # Í∞Å ÏÉÅÌíàÏóê ÎåÄÌï¥ Îß§Ïπ≠ ÏàòÌñâ
    for idx, product_name in enumerate(product_names):
        if (idx + 1) % 10 == 0:
            logging.info(f"ÏßÑÌñâ ÏÉÅÌô©: {idx + 1}/{len(product_names)} Ï≤òÎ¶¨ Ï§ë...")
        
        logging.debug(f"\nüì¶ ÏÉÅÌíà '{product_name}' Îß§Ïπ≠ ÏãúÏûë")
        
        # === Îã®Í≥Ñ 1: Ìï¥Ïãú Í∏∞Î∞ò Ï†ïÌôïÌïú Îß§Ïπ≠ ===
        product_hash = generate_product_name_hash(product_name)
        hash_candidates = {
            'haereum': [],
            'kogift': [],
            'naver': []
        }
        
        if product_hash:
            logging.debug(f"üîë ÏÉùÏÑ±Îêú Ìï¥Ïãú: {product_hash}")
            
            # Í∞Å ÏÜåÏä§ÏóêÏÑú Ìï¥Ïãú Îß§Ïπ≠ ÌõÑÎ≥¥ Ï∞æÍ∏∞
            for h_path, h_info in haereum_images.items():
                if h_path not in used_haereum:
                    img_hash = h_info.get('product_hash')
                    if img_hash and img_hash == product_hash:
                        hash_candidates['haereum'].append((h_path, h_info))
            
            for k_path, k_info in kogift_images.items():
                if k_path not in used_kogift:
                    img_hash = k_info.get('product_hash')
                    if img_hash and img_hash == product_hash:
                        hash_candidates['kogift'].append((k_path, k_info))
            
            for n_path, n_info in naver_images.items():
                if n_path not in used_naver:
                    img_hash = n_info.get('product_hash')
                    if img_hash and img_hash == product_hash:
                        hash_candidates['naver'].append((n_path, n_info))
            
            total_hash_candidates = (len(hash_candidates['haereum']) + 
                                   len(hash_candidates['kogift']) + 
                                   len(hash_candidates['naver']))
            
            logging.debug(f"üéØ Ìï¥Ïãú Îß§Ïπ≠ ÌõÑÎ≥¥: Ìï¥Ïò§Î¶Ñ {len(hash_candidates['haereum'])}Í∞ú, "
                         f"Í≥†Î†§Í∏∞ÌîÑÌä∏ {len(hash_candidates['kogift'])}Í∞ú, ÎÑ§Ïù¥Î≤Ñ {len(hash_candidates['naver'])}Í∞ú")
            
            # === Îã®Í≥Ñ 2: Ïù¥ÎØ∏ÏßÄ Ïú†ÏÇ¨ÎèÑ Í≤ÄÏ¶ù (Ìï¥Ïãú Îß§Ïπ≠ ÌõÑÎ≥¥Í∞Ä ÏûàÏùÑ ÎïåÎßå) ===
            final_matches = {'haereum': None, 'kogift': None, 'naver': None}
            
            if total_hash_candidates > 0 and enhanced_matcher:
                logging.debug(f"üîç Ïù¥ÎØ∏ÏßÄ Ïú†ÏÇ¨ÎèÑ Í≤ÄÏ¶ù ÏãúÏûë (ÏûÑÍ≥ÑÍ∞í: {similarity_threshold})")
                
                # Í∏∞Ï§Ä Ïù¥ÎØ∏ÏßÄ ÏÑ†ÌÉù (Ìï¥Ïò§Î¶Ñ > Í≥†Î†§Í∏∞ÌîÑÌä∏ > ÎÑ§Ïù¥Î≤Ñ Ïàú)
                reference_path = None
                reference_source = None
                
                if hash_candidates['haereum']:
                    ref_path, ref_info = hash_candidates['haereum'][0]
                    reference_path = ref_info.get('path', ref_path)
                    reference_source = 'haereum'
                elif hash_candidates['kogift']:
                    ref_path, ref_info = hash_candidates['kogift'][0]
                    reference_path = ref_info.get('path', ref_path)
                    reference_source = 'kogift'
                elif hash_candidates['naver']:
                    ref_path, ref_info = hash_candidates['naver'][0]
                    reference_path = ref_info.get('path', ref_path)
                    reference_source = 'naver'
                
                if reference_path and os.path.exists(reference_path):
                    logging.debug(f"üìç Í∏∞Ï§Ä Ïù¥ÎØ∏ÏßÄ: {reference_source} - {os.path.basename(reference_path)}")
                    
                    # Í∏∞Ï§Ä Ïù¥ÎØ∏ÏßÄÏùò Îß§Ïπ≠ ÌôïÏ†ï
                    if reference_source == 'haereum':
                        final_matches['haereum'] = hash_candidates['haereum'][0]
                        used_haereum.add(hash_candidates['haereum'][0][0])
                    elif reference_source == 'kogift':
                        final_matches['kogift'] = hash_candidates['kogift'][0]
                        used_kogift.add(hash_candidates['kogift'][0][0])
                    elif reference_source == 'naver':
                        final_matches['naver'] = hash_candidates['naver'][0]
                        used_naver.add(hash_candidates['naver'][0][0])
                    
                    # Îã§Î•∏ ÏÜåÏä§Îì§Í≥º Ïù¥ÎØ∏ÏßÄ Ïú†ÏÇ¨ÎèÑ Í≤ÄÏ¶ù
                    for source, candidates in hash_candidates.items():
                        if source == reference_source or not candidates:
                            continue
                        
                        for candidate_path, candidate_info in candidates:
                            candidate_img_path = candidate_info.get('path', candidate_path)
                            
                            if os.path.exists(candidate_img_path):
                                try:
                                    similarity = enhanced_matcher.calculate_similarity(reference_path, candidate_img_path)
                                    logging.debug(f"üîç Ïú†ÏÇ¨ÎèÑ Í≤ÄÏÇ¨: {reference_source} vs {source} = {similarity:.3f}")
                                    
                                    if similarity >= similarity_threshold:
                                        final_matches[source] = (candidate_path, candidate_info)
                                        if source == 'haereum':
                                            used_haereum.add(candidate_path)
                                        elif source == 'kogift':
                                            used_kogift.add(candidate_path)
                                        elif source == 'naver':
                                            used_naver.add(candidate_path)
                                        
                                        logging.info(f"‚úÖ {source} Îß§Ïπ≠ ÏÑ±Í≥µ: {os.path.basename(candidate_path)} (Ïú†ÏÇ¨ÎèÑ: {similarity:.3f})")
                                        break
                                    else:
                                        logging.debug(f"‚ùå Ïú†ÏÇ¨ÎèÑ Î∂ÄÏ°±: {source} {similarity:.3f} < {similarity_threshold}")
                                        
                                except Exception as e:
                                    logging.error(f"Ïù¥ÎØ∏ÏßÄ Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞ Ïò§Î•ò: {e}")
                            else:
                                logging.warning(f"Ïù¥ÎØ∏ÏßÄ ÌååÏùº ÏóÜÏùå: {candidate_img_path}")
                
                # Îß§Ïπ≠ Í≤∞Í≥º Ï†ïÎ¶¨
                if any(final_matches.values()):
                    hash_matches += 1
                    if total_hash_candidates > 1:  # 2Í∞ú Ïù¥ÏÉÅ ÏÜåÏä§ÏóêÏÑú Ìï¥Ïãú Îß§Ïπ≠Îêú Í≤ΩÏö∞
                        image_verified += 1
                    
                    logging.info(f"üéâ '{product_name}' Ìï¥Ïãú+Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠ ÏôÑÎ£å")
                    
                    # Í≤∞Í≥º Ï∂îÍ∞Ä
                    best_matches.append((
                        (final_matches['haereum'][0], 0.95) if final_matches['haereum'] else None,
                        (final_matches['kogift'][0], 0.95) if final_matches['kogift'] else None,
                        (final_matches['naver'][0], 0.95) if final_matches['naver'] else None
                    ))
                    continue
        
        # === Ìï¥Ïãú Îß§Ïπ≠ Ïã§Ìå® Ïãú Îß§Ïπ≠ ÏóÜÏùåÏúºÎ°ú Ï≤òÎ¶¨ ===
        logging.debug(f"‚ùå '{product_name}' Ìï¥Ïãú Îß§Ïπ≠ Ïã§Ìå® - Îß§Ïπ≠ ÏóÜÏùåÏúºÎ°ú Ï≤òÎ¶¨")
        no_matches += 1
        
        best_matches.append((None, None, None))
    
    # ÏµúÏ¢Ö ÌÜµÍ≥Ñ Ï∂úÎ†•
    success_rate = (hash_matches / len(product_names) * 100) if product_names else 0
    verification_rate = (image_verified / hash_matches * 100) if hash_matches > 0 else 0
    
    logging.info("\nüìà === Îß§Ïπ≠ ÏôÑÎ£å ÌÜµÍ≥Ñ ===")
    logging.info(f"‚úÖ Ìï¥Ïãú Îß§Ïπ≠ ÏÑ±Í≥µ: {hash_matches}/{len(product_names)} ({success_rate:.1f}%)")
    logging.info(f"üîç Ïù¥ÎØ∏ÏßÄ Í≤ÄÏ¶ù ÏôÑÎ£å: {image_verified}/{hash_matches} ({verification_rate:.1f}%)")
    logging.info(f"‚ùå Îß§Ïπ≠ Ïã§Ìå®: {no_matches}/{len(product_names)} ({100-success_rate:.1f}%)")
    logging.info(f"üèÉ‚Äç‚ôÇÔ∏è ÏÇ¨Ïö©Îêú Ïù¥ÎØ∏ÏßÄ: Ìï¥Ïò§Î¶Ñ {len(used_haereum)}, Í≥†Î†§Í∏∞ÌîÑÌä∏ {len(used_kogift)}, ÎÑ§Ïù¥Î≤Ñ {len(used_naver)}")
    
    # ÏÑ±Îä• ÌÜµÍ≥Ñ
    if len(product_names) > 0:
        efficiency_score = hash_matches / len(product_names)
        if efficiency_score >= 0.8:
            logging.info("üèÜ Îß§Ïπ≠ Ìö®Ïú®ÏÑ±: Ïö∞Ïàò (80% Ïù¥ÏÉÅ)")
        elif efficiency_score >= 0.6:
            logging.info("üëç Îß§Ïπ≠ Ìö®Ïú®ÏÑ±: ÏñëÌò∏ (60% Ïù¥ÏÉÅ)")
        else:
            logging.info("‚ö†Ô∏è Îß§Ïπ≠ Ìö®Ïú®ÏÑ±: Í∞úÏÑ† ÌïÑÏöî (60% ÎØ∏Îßå)")
    
    return best_matches

def find_best_match_for_product(product_tokens: List[str], 
                               image_info: Dict[str, Dict], 
                               used_images: Set[str] = None,
                               similarity_threshold: float = 0.45,  # ÏûÑÍ≥ÑÍ∞í ÏÉÅÌñ• Ï°∞Ï†ï (0.3ÏóêÏÑú 0.45ÏúºÎ°ú)
                               source_name_for_log: str = "UnknownSource",
                               config: Optional[configparser.ConfigParser] = None) -> Optional[Tuple[str, float]]:
    """
    Find the best matching image for a product based on name tokens.
    Updated with higher thresholds for stricter matching.
    
    Args:
        product_tokens: Tokens of the product name
        image_info: Dictionary of image metadata
        used_images: Set of already used image paths
        similarity_threshold: Minimum similarity score for matching
        source_name_for_log: Source name for logging
        config: Configuration object for retrieving settings
        
    Returns:
        Tuple of (best_match_path, similarity_score) or None if no match found
    """
    if not product_tokens:
        return None
        
    if used_images is None:
        used_images = set()
        
    best_match_path = None
    best_match_score = 0
    
    # Use the similarity_threshold passed as argument directly.
    # This threshold is expected to be set by the caller (find_best_image_matches)
    # and should be appropriate for text-based similarity.
    effective_similarity_threshold = similarity_threshold

    # Log which threshold is being used.
    logging.info(f"[{source_name_for_log}] Using text similarity threshold: {effective_similarity_threshold} (passed from caller)")
    
    # Log the number of images we're searching through
    logging.info(f"[{source_name_for_log}] Searching through {len(image_info)} images for a match")

    for img_path, img_data in image_info.items():
        # Skip if already used
        if img_path in used_images:
            continue
            
        # Get the name for matching from metadata
        if 'name_for_matching' in img_data:
            img_name = img_data['name_for_matching']
        elif 'original_name' in img_data:
            img_name = img_data['original_name']
        else:
            # Use the filename if no metadata is available
            img_name = os.path.basename(img_path)
        
        # Convert to string and calculate text similarity
        img_name_str = str(img_name)
        product_name_str = ' '.join(product_tokens)
        
        similarity = calculate_text_similarity(product_name_str, img_name_str)
        
        if similarity > best_match_score:
            best_match_score = similarity
            best_match_path = img_path
    
    # Check threshold - use the minimum threshold for extreme lenience
    if best_match_score >= effective_similarity_threshold:
        if best_match_path:
            img_name = image_info[best_match_path].get('original_name', os.path.basename(best_match_path))
            logging.info(f"{source_name_for_log}: Best match for '{' '.join(product_tokens)}': '{img_name}' with score {best_match_score:.3f}")
            return best_match_path, best_match_score
    elif best_match_path:  # We found a match but score is below threshold
        img_name = image_info[best_match_path].get('original_name', os.path.basename(best_match_path))
        logging.info(f"{source_name_for_log}: Found match below threshold. Product: '{' '.join(product_tokens)}', Image: '{img_name}', Score: {best_match_score:.3f} (threshold: {effective_similarity_threshold})")
    
    # No match found with sufficient similarity
    logging.info(f"No match found above threshold {effective_similarity_threshold} for {source_name_for_log}. Trying basic token matching.")
    
    # Try more basic matching as fallback
    for img_path, img_data in image_info.items():
        # Skip if already used
        if img_path in used_images:
            continue
            
        # Get image name from metadata
        if 'name_for_matching' in img_data:
            img_name = img_data['name_for_matching']
        elif 'original_name' in img_data:
            img_name = img_data['original_name']
        else:
            img_name = os.path.basename(img_path)
            
        # Convert to lowercase for case-insensitive matching
        img_name_lower = str(img_name).lower()
        product_name_lower = ' '.join(product_tokens).lower()
        
        # 1. Ïö∞ÏÑ† Ï†ÑÏ≤¥ ÏÉÅÌíàÎ™ÖÏùò ÏùºÎ∂ÄÍ∞Ä Ïù¥ÎØ∏ÏßÄ Ïù¥Î¶ÑÏóê Ìè¨Ìï®ÎêòÏñ¥ ÏûàÎäîÏßÄ ÌôïÏù∏
        basic_match_score = 0.0
        if len(product_name_lower) >= 4 and product_name_lower[:4] in img_name_lower:
            basic_match_score = 0.4
            logging.info(f"{source_name_for_log}: Product name prefix match found: '{product_name_lower[:4]}' in '{img_name}'")
            return img_path, basic_match_score
        
        # 2. Í∞úÎ≥Ñ ÌÜ†ÌÅ∞ Îß§Ïπ≠ (Í∏∏Ïù¥Í∞Ä 2 Ïù¥ÏÉÅÏù∏ Ï§ëÏöî ÌÜ†ÌÅ∞)
        matched_tokens = []
        for token in product_tokens:
            if len(token) >= 2 and token.lower() in img_name_lower:
                matched_tokens.append(token)
        
        # Îß§Ïπ≠Îêú ÌÜ†ÌÅ∞ ÏàòÏóê Îî∞Îùº Ï†êÏàò Í≥ÑÏÇ∞
        if matched_tokens:
            # ÌÜ†ÌÅ∞ Í∏∏Ïù¥Ïóê Îî∞Îùº Í∞ÄÏ§ëÏπò Ï†ÅÏö©
            token_weight = sum(len(token) for token in matched_tokens) / sum(len(token) for token in product_tokens)
            # ÌÜ†ÌÅ∞ Í∞úÏàòÏóê Îî∞Îùº Í∞ÄÏ§ëÏπò Ï†ÅÏö©
            count_weight = len(matched_tokens) / len(product_tokens)
            # ÏµúÏ¢Ö Ï†êÏàò Í≥ÑÏÇ∞ (Í∏∏Ïù¥ÏôÄ Í∞úÏàòÎ•º Î™®Îëê Í≥†Î†§)
            basic_match_score = 0.3 * token_weight + 0.2 * count_weight
            
            # ÏûÑÍ≥ÑÍ∞íÏùÑ 0.05Î°ú ÏÑ§Ï†ïÌïòÏó¨ Îß§Ïπ≠ÏùÑ ÌóàÏö©
            if basic_match_score >= 0.05:
                logging.info(f"{source_name_for_log}: Basic token match found: '{matched_tokens}' in '{img_name}' with score {basic_match_score:.3f}")
                return img_path, basic_match_score
    
    return None

def find_best_match_with_enhanced_matcher(
    source_img_path: str, 
    target_images: Dict[str, Dict], 
    used_images: Set[str] = None,
    enhanced_matcher: Any = None
) -> Optional[Tuple[str, float]]:
    """
    Enhanced image matching with stricter thresholds for higher quality matches.
    """
    if not enhanced_matcher:
        logging.warning("Enhanced image matcher not available. Falling back to text-based matching.")
        return None
        
    if used_images is None:
        used_images = set()
        
    best_match = None
    best_score = 0
    
    # Using higher thresholds for stricter matching
    high_confidence_threshold = 0.30   # 0.15ÏóêÏÑú 0.30ÏúºÎ°ú ÏÉÅÌñ•
    min_confidence_threshold = 0.15    # 0.00001ÏóêÏÑú 0.15Î°ú ÎåÄÌè≠ ÏÉÅÌñ•
    
    gpu_info = "GPU enabled" if getattr(enhanced_matcher, 'use_gpu', False) else "CPU mode"
    logging.info(f"Running enhanced matching on {len(target_images)} target images against source: {os.path.basename(source_img_path)} ({gpu_info})")
    
    # Check if source image exists
    if not os.path.exists(source_img_path):
        logging.error(f"Source image doesn't exist: {source_img_path}")
        return None
    
    # Track timing for performance analysis
    start_time = time.time()
    matches_checked = 0
    high_conf_matches = 0
    
    # Debug: Log some target image paths for verification
    sample_keys = list(target_images.keys())[:3] if len(target_images) > 3 else list(target_images.keys())
    for key in sample_keys:
        logging.info(f"Sample target image key: {key}")
        if isinstance(target_images[key], dict):
            img_path = target_images[key].get('path', key)
            logging.info(f"  - Path from dict: {img_path}")
            if os.path.exists(img_path):
                logging.info(f"  - This path exists on disk")
            else:
                logging.info(f"  - This path does NOT exist on disk")
        else:
            logging.info(f"  - Value is not a dict: {type(target_images[key])}")
    
    for image_path, image_info in target_images.items():
        # Skip if already used
        if image_path in used_images:
            continue
        
        # Determine the actual path to use
        actual_path = image_path
        if isinstance(image_info, dict):
            if 'path' in image_info:
                actual_path = image_info['path']
            
            # Try to use nobg version if available (for potentially better matching)
            if enhanced_matcher.USE_BACKGROUND_REMOVAL and isinstance(image_info, dict):
                # Check nobg paths
                nobg_png = image_info.get('nobg_png_path')
                nobg_jpg = image_info.get('nobg_jpg_path')
                
                # Prefer PNG version for nobg (usually better quality)
                if nobg_png and os.path.exists(nobg_png):
                    logging.debug(f"Using background-removed PNG version for matching: {os.path.basename(nobg_png)}")
                    actual_path = nobg_png
                elif nobg_jpg and os.path.exists(nobg_jpg):
                    logging.debug(f"Using background-removed JPG version for matching: {os.path.basename(nobg_jpg)}")
                    actual_path = nobg_jpg
            
        # Check if image exists
        if not os.path.exists(actual_path):
            logging.warning(f"Target image doesn't exist: {actual_path} (key: {image_path})")
            continue
        
        try:
            logging.debug(f"Comparing source {os.path.basename(source_img_path)} with target {os.path.basename(actual_path)}")
            # Call the calculate_similarity method directly instead of is_match to get raw similarity
            similarity = enhanced_matcher.calculate_similarity(source_img_path, actual_path)
            
            # Log similarity score for debugging
            logging.debug(f"Raw similarity score: {similarity:.4f} for {os.path.basename(actual_path)}")
            
            matches_checked += 1
            
            if similarity > high_confidence_threshold:
                high_conf_matches += 1
                logging.info(f"High confidence match: {os.path.basename(actual_path)} = {similarity:.4f}")
                
            if similarity > best_score:
                best_score = similarity
                best_match = image_path  # Keep the original key, not the resolved path
                logging.info(f"New best match: {os.path.basename(actual_path)} with score {similarity:.4f}")
                
                # Early exit for very high confidence matches to save processing time
                if similarity > 0.75:
                    logging.info(f"Found very high confidence match ({similarity:.4f}), early exit")
                    break
                    
        except Exception as e:
            logging.error(f"Error comparing images: {e}")
            
    # Compute time spent
    elapsed = time.time() - start_time
    
    # Don't return matches below the absolute minimum threshold
    if best_match and best_score < min_confidence_threshold:
        logging.info(f"Best match score ({best_score:.4f}) below min threshold ({min_confidence_threshold})")
        return None
        
    # Log summary of matching results
    if best_match:
        if isinstance(target_images[best_match], dict) and 'path' in target_images[best_match]:
            best_path = target_images[best_match]['path']
        else:
            best_path = best_match
        logging.info(f"Best image match: {os.path.basename(best_path)} ({best_score:.4f}) [checked {matches_checked} images in {elapsed:.2f}s, {high_conf_matches} high confidence]")
    else:
        logging.info(f"No image match found after checking {matches_checked} images in {elapsed:.2f}s")
    
    return (best_match, best_score) if best_match else None

def verify_image_matches(best_matches, product_names, haereum_images, kogift_images, naver_images):
    """
    Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠ Í≤∞Í≥ºÎ•º Í≤ÄÏ¶ùÌïòÎäî Ìï®ÏàòÏûÖÎãàÎã§.
    ÌîÑÎ°úÎçïÌä∏ Ïù¥Î¶ÑÍ≥º ÌååÏùº Ïù¥Î¶Ñ Í∞ÑÏùò Í≥µÌÜµ ÌÜ†ÌÅ∞ÏùÑ ÌôïÏù∏ÌïòÏó¨ Îß§Ïπ≠ ÌíàÏßàÏùÑ Í≤ÄÏ¶ùÌï©ÎãàÎã§.
    
    Args:
        best_matches: find_best_image_matches Ìï®ÏàòÏùò Í≤∞Í≥º
        product_names: ÏÉÅÌíàÎ™Ö Î™©Î°ù
        haereum_images: Ìï¥Ïò§Î¶Ñ Ïù¥ÎØ∏ÏßÄ Ï†ïÎ≥¥
        kogift_images: Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ Ï†ïÎ≥¥
        naver_images: ÎÑ§Ïù¥Î≤Ñ Ïù¥ÎØ∏ÏßÄ Ï†ïÎ≥¥
        
    Returns:
        Í≤ÄÏ¶ùÎêú Îß§Ïπ≠ Í≤∞Í≥º
    """
    verified_matches = []
    
    # ID Í∏∞Î∞ò Îß§Ïπ≠Ïóê ÏÇ¨Ïö©ÎêòÎäî Ï†ïÍ∑ú ÌëúÌòÑÏãù
    id_pattern = re.compile(r'_([a-f0-9]{10})(?:\.jpg|\.png|_nobg\.png)?$')
    
    for idx, (product_name, match_set) in enumerate(zip(product_names, best_matches)):
        # Handle cases where match_set may not have exactly 3 elements
        # or if elements within are not as expected (e.g. None instead of (path, score) tuple)
        haereum_data, kogift_data, naver_data = None, None, None # Initialize with None

        if match_set and len(match_set) == 3:
            haereum_data, kogift_data, naver_data = match_set
        else:
            logging.warning(f"Invalid match_set for product '{product_name}': {match_set}. Using None values for all sources.")
            # haereum_data, kogift_data, naver_data remain None as initialized

        product_tokens = set(tokenize_product_name(product_name))
        
        # Îß§Ïπ≠ ÌíàÏßà Í∏∞Î°ù
        match_quality = {
            'haereum': {'score': 0, 'match': haereum_data, 'id': None}, # Store data tuple directly
            'kogift': {'score': 0, 'match': kogift_data, 'id': None},  # Store data tuple directly
            'naver': {'score': 0, 'match': naver_data, 'id': None}    # Store data tuple directly
        }
        
        # Ìï¥Ïò§Î¶Ñ Îß§Ïπ≠ Í≤ÄÏ¶ù
        if haereum_data and isinstance(haereum_data, tuple) and len(haereum_data) == 2:
            haereum_path, haereum_score = haereum_data
            match_quality['haereum']['score'] = haereum_score # Use the propagated score
            
            if haereum_path and haereum_path in haereum_images:
                haereum_filename = os.path.basename(haereum_path)
                haereum_id = None
                id_match = id_pattern.search(haereum_filename)
                if id_match:
                    haereum_id = id_match.group(1)
                
                haereum_tokens = set(tokenize_product_name(haereum_images[haereum_path].get('clean_name', 
                                                        haereum_images[haereum_path].get('name_for_matching', ''))))
                common_tokens = product_tokens & haereum_tokens
                token_ratio = len(common_tokens) / max(len(product_tokens), 1)
                
                # Refine score, but start with the propagated score
                match_quality['haereum']['score'] = haereum_score * (1 + token_ratio) 
                match_quality['haereum']['id'] = haereum_id
            else:
                logging.warning(f"Haereum path '{haereum_path}' not found in haereum_images or path is None.")
                match_quality['haereum']['match'] = None # Invalidate if path issue
                match_quality['haereum']['score'] = 0
        elif haereum_data: # Was not None, but not a (path, score) tuple
             logging.warning(f"Unexpected format for haereum_data: {haereum_data}. Clearing Haereum match.")
             match_quality['haereum']['match'] = None
             match_quality['haereum']['score'] = 0
        
        # Í≥†Î†§Í∏∞ÌîÑÌä∏ Îß§Ïπ≠ Í≤ÄÏ¶ù
        if kogift_data and isinstance(kogift_data, tuple) and len(kogift_data) == 2:
            kogift_path, kogift_score = kogift_data
            match_quality['kogift']['score'] = kogift_score # Use the propagated score

            if kogift_path and kogift_path in kogift_images:
                kogift_filename = os.path.basename(kogift_path)
                kogift_id = None
                id_match = id_pattern.search(kogift_filename)
                if id_match:
                    kogift_id = id_match.group(1)
                
                haereum_id_for_comparison = match_quality['haereum'].get('id')
                if haereum_id_for_comparison and haereum_id_for_comparison == kogift_id:
                    match_quality['kogift']['score'] = max(kogift_score, 0.8) * 1.5
                else:
                    kogift_tokens = set(tokenize_product_name(kogift_images[kogift_path].get('clean_name',
                                                         kogift_images[kogift_path].get('name_for_matching', ''))))
                    common_tokens = product_tokens & kogift_tokens
                    token_ratio = len(common_tokens) / max(len(product_tokens), 1)
                    match_quality['kogift']['score'] = kogift_score * (1 + token_ratio)
                match_quality['kogift']['id'] = kogift_id
            else:
                logging.warning(f"Kogift path '{kogift_path}' not found in kogift_images or path is None.")
                match_quality['kogift']['match'] = None # Invalidate if path issue
                match_quality['kogift']['score'] = 0
        elif kogift_data: # Was not None, but not a (path, score) tuple
             logging.warning(f"Unexpected format for kogift_data: {kogift_data}. Clearing Kogift match.")
             match_quality['kogift']['match'] = None
             match_quality['kogift']['score'] = 0

        # ÎÑ§Ïù¥Î≤Ñ Îß§Ïπ≠ Í≤ÄÏ¶ù
        if naver_data and isinstance(naver_data, tuple) and len(naver_data) == 2:
            naver_path, naver_score = naver_data
            match_quality['naver']['score'] = naver_score # Use the propagated score

            if naver_path and naver_path in naver_images:
                naver_filename = os.path.basename(naver_path)
                naver_id = None
                id_match = id_pattern.search(naver_filename)
                if id_match:
                    naver_id = id_match.group(1)
                
                haereum_id_for_comparison = match_quality['haereum'].get('id')
                if haereum_id_for_comparison and haereum_id_for_comparison == naver_id:
                    match_quality['naver']['score'] = max(naver_score, 0.8) * 1.5
                else:
                    naver_tokens = set(tokenize_product_name(naver_images[naver_path].get('clean_name',
                                                        naver_images[naver_path].get('name_for_matching', ''))))
                    common_tokens = product_tokens & naver_tokens
                    token_ratio = len(common_tokens) / max(len(product_tokens), 1)
                    # Refine score, but start with the propagated score
                    match_quality['naver']['score'] = naver_score * (1 + token_ratio) 
                match_quality['naver']['id'] = naver_id
            else:
                logging.warning(f"Naver path '{naver_path}' not found in naver_images or path is None.")
                match_quality['naver']['match'] = None # Invalidate if path issue
                match_quality['naver']['score'] = 0
        elif naver_data: # Was not None, but not a (path, score) tuple
             logging.warning(f"Unexpected format for naver_data: {naver_data}. Clearing Naver match.")
             match_quality['naver']['match'] = None
             match_quality['naver']['score'] = 0
        
        # Í≤ÄÏ¶ù Í≤∞Í≥ºÎ•º Î°úÍ∑∏Î°ú Ï∂úÎ†•
        logging.debug(f"Product: '{product_name}' - Verification scores: Haereum={match_quality['haereum']['score']:.2f}, Kogift={match_quality['kogift']['score']:.2f}, Naver={match_quality['naver']['score']:.2f}")
        
        # ÏµúÏ¢Ö Í≤ÄÏ¶ùÎêú Îß§Ïπ≠ Í≤∞Í≥º Ï∂îÍ∞Ä
        verified_matches.append((
            match_quality['haereum']['match'],
            match_quality['kogift']['match'],
            match_quality['naver']['match']
        ))
    
    return verified_matches

def integrate_images(df: pd.DataFrame, config: configparser.ConfigParser) -> pd.DataFrame:
    """
    ÏÑ∏ Í∞ÄÏßÄ Ïù¥ÎØ∏ÏßÄ ÏÜåÏä§(Ìï¥Ïò§Î¶Ñ, Í≥†Î†§Í∏∞ÌîÑÌä∏, ÎÑ§Ïù¥Î≤Ñ)Ïùò Ïù¥ÎØ∏ÏßÄÎ•º DataFrameÏóê ÌÜµÌï©Ìï©ÎãàÎã§.
    ÏÉÅÌíàÎ≥ÑÎ°ú ÏùºÍ¥ÄÎêú Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠ÏùÑ Î≥¥Ïû•Ìï©ÎãàÎã§.
    
    Note: Haereum images (Î≥∏ÏÇ¨ Ïù¥ÎØ∏ÏßÄ) are ALWAYS included, regardless of matching scores.
    
    Args:
        df: Ï≤òÎ¶¨Ìï† DataFrame (data_processing.pyÏùò format_product_data_for_outputÏùÑ Í±∞Ïπú ÏÉÅÌÉúÏó¨Ïïº Ìï®)
        config: ÏÑ§Ï†ï ÌååÏùº
        
    Returns:
        Ïù¥ÎØ∏ÏßÄÍ∞Ä ÌÜµÌï©Îêú DataFrame
    """
    try:
        logging.info("ÌÜµÌï©: Ïù¥ÎØ∏ÏßÄ ÌÜµÌï© ÌîÑÎ°úÏÑ∏Ïä§ ÏãúÏûë...")
        result_df = df.copy() # dfÎäî Ïù¥ÎØ∏ format_product_data_for_outputÏùÑ Í±∞Ï≥ê Ïù¥ÎØ∏ÏßÄ Ïª¨ÎüºÏóê dictÍ∞Ä ÏûàÏùÑ Í≤ÉÏúºÎ°ú ÏòàÏÉÅ

        # These column names are expected to be in the input df, potentially holding original scraped URLs
        # if they were not already incorporated into the image dictionaries by data_processing.py
        scraped_haereum_url_col_input_df = 'Î≥∏ÏÇ¨Ïù¥ÎØ∏ÏßÄURL' 
        scraped_kogift_url_col_input_df = 'Í≥†Î†§Í∏∞ÌîÑÌä∏Ïù¥ÎØ∏ÏßÄURL' 
        scraped_naver_url_col_input_df = 'ÎÑ§Ïù¥Î≤ÑÏù¥ÎØ∏ÏßÄURL'
        
        # Ïù¥ÎØ∏ÏßÄ ÎîîÎ†âÌÜ†Î¶¨ Í≤ΩÎ°ú
        main_img_dir = Path(config.get('Paths', 'image_main_dir', fallback='C:\\\\RPA\\\\Image\\\\Main'))
        haereum_dir = main_img_dir / 'Haereum'
        kogift_dir = main_img_dir / 'Kogift'
        naver_dir = main_img_dir / 'Naver'
        
        # ÎîîÎ†âÌÜ†Î¶¨ Ï°¥Ïû¨ Ï≤¥ÌÅ¨
        if not haereum_dir.exists():
            logging.warning(f"Ìï¥Ïò§Î¶Ñ Ïù¥ÎØ∏ÏßÄ ÎîîÎ†âÌÜ†Î¶¨Í∞Ä Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§: {haereum_dir}")
        if not kogift_dir.exists():
            logging.warning(f"Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ ÎîîÎ†âÌÜ†Î¶¨Í∞Ä Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§: {kogift_dir}")
        if not naver_dir.exists():
            logging.warning(f"ÎÑ§Ïù¥Î≤Ñ Ïù¥ÎØ∏ÏßÄ ÎîîÎ†âÌÜ†Î¶¨Í∞Ä Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§: {naver_dir}")
        
        # Ïù¥ÎØ∏ÏßÄ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ
        haereum_images = prepare_image_metadata(haereum_dir, 'haereum_', prefer_original=True, prefer_jpg=True)
        kogift_images = prepare_image_metadata(kogift_dir, 'kogift_', prefer_original=True, prefer_jpg=True)
        naver_images = prepare_image_metadata(naver_dir, 'naver_', prefer_original=True, prefer_jpg=True)
        
        # ÌïÑÏöîÌïú Ïó¥ Ï∂îÍ∞Ä
        if 'Î≥∏ÏÇ¨ Ïù¥ÎØ∏ÏßÄ' not in result_df.columns:
            result_df['Î≥∏ÏÇ¨ Ïù¥ÎØ∏ÏßÄ'] = None
        if 'Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ' not in result_df.columns:
            result_df['Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ'] = None
        if 'ÎÑ§Ïù¥Î≤Ñ Ïù¥ÎØ∏ÏßÄ' not in result_df.columns:
            result_df['ÎÑ§Ïù¥Î≤Ñ Ïù¥ÎØ∏ÏßÄ'] = None
        
        # Ensure target columns for image data exist before processing
        # These are the final column names used for output (e.g., in Excel)
        target_cols = ['Î≥∏ÏÇ¨ Ïù¥ÎØ∏ÏßÄ', 'Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ', 'ÎÑ§Ïù¥Î≤Ñ Ïù¥ÎØ∏ÏßÄ']
        for col in target_cols:
            if col not in result_df.columns:
                # Initialize with a suitable default, e.g., None or '-'
                # Using None initially might be better if subsequent logic checks for None
                result_df[col] = None 
                logging.debug(f"Added missing target image column: {col}")

        # ÏÉÅÌíà Î™©Î°ù Ï∂îÏ∂ú
        product_names = result_df['ÏÉÅÌíàÎ™Ö'].tolist()
        
        # Ï†úÌíà ÏàòÏôÄ Ïù¥ÎØ∏ÏßÄ Ïàò Î°úÍπÖ
        logging.info(f"Ï†úÌíà Ïàò: {len(product_names)}Í∞ú")
        logging.info(f"Ìï¥Ïò§Î¶Ñ Ïù¥ÎØ∏ÏßÄ: {len(haereum_images)}Í∞ú")
        logging.info(f"Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ: {len(kogift_images)}Í∞ú")
        logging.info(f"ÎÑ§Ïù¥Î≤Ñ Ïù¥ÎØ∏ÏßÄ: {len(naver_images)}Í∞ú")
        
        # ÏÉÅÌíàÎ™Ö ÏÉòÌîå Î°úÍπÖ
        if product_names:
            sample_products = product_names[:3] if len(product_names) > 3 else product_names
            logging.debug(f"Ï†úÌíà ÏÉòÌîå: {sample_products}")
        
        # Retrieve similarity threshold from config with higher quality defaults
        # 1) Primary key: Matching.text_threshold (higher quality standard)
        # 2) Secondary key: Matching.image_threshold 
        # 3) Tertiary key: ImageMatching.minimum_match_confidence
        # 4) Fallback: 0.4 (high quality default)
        
        # Get thresholds in priority order
        text_threshold = None
        image_threshold = None
        min_match_confidence = None
        
        try:
            if config.has_option('Matching', 'text_threshold'):
                text_threshold = config.getfloat('Matching', 'text_threshold')
                logging.debug(f"Read text_threshold: {text_threshold}")
        except (configparser.Error, ValueError) as e:
            logging.warning(f"Could not read [Matching] text_threshold: {e}")
            
        try:
            if config.has_option('Matching', 'image_threshold'):
                image_threshold = config.getfloat('Matching', 'image_threshold')
                logging.debug(f"Read image_threshold: {image_threshold}")
        except (configparser.Error, ValueError) as e:
            logging.warning(f"Could not read [Matching] image_threshold: {e}")

        try:
            if config.has_option('ImageMatching', 'minimum_match_confidence'):
                min_match_confidence = config.getfloat('ImageMatching', 'minimum_match_confidence')
                logging.debug(f"Read minimum_match_confidence: {min_match_confidence}")
        except (configparser.Error, ValueError) as e:
            logging.warning(f"Could not read [ImageMatching] minimum_match_confidence: {e}")

        # Use the first available threshold, with higher defaults
        if text_threshold is not None:
            similarity_threshold = text_threshold
        elif image_threshold is not None:
            similarity_threshold = image_threshold
        elif min_match_confidence is not None:
            similarity_threshold = min_match_confidence
        else:
            similarity_threshold = 0.4  # Higher quality default
            logging.warning(f"Using higher quality default similarity_threshold of {similarity_threshold} as specific values not found or invalid in config.")
        
        # Set the initial matching threshold
        initial_matching_threshold = similarity_threshold # Use the general similarity_threshold from config or default
        
        logging.info(f"Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠ Ïú†ÏÇ¨ÎèÑ ÏûÑÍ≥ÑÍ∞í (for find_best_image_matches): {initial_matching_threshold}")
        
        # ÏµúÏ†Å Îß§Ïπò Ï∞æÍ∏∞ (ÏùºÍ¥ÄÏÑ± Î≥¥Ïû•)
        best_matches = find_best_image_matches(
            product_names,
            haereum_images,
            kogift_images,
            naver_images,
            similarity_threshold=initial_matching_threshold,  # Use lower threshold for initial matching
            config=config,
            df=result_df  # Pass DataFrame for product code matching
        )
        
        # Îß§Ïπ≠ Í≤∞Í≥º Í≤ÄÏ¶ù
        logging.info(f"Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠ Í≤ÄÏ¶ù Ï§ë...")
        verified_matches = verify_image_matches(
            best_matches,
            product_names,
            haereum_images,
            kogift_images,
            naver_images
        )
        
        # Í≤∞Í≥ºÎ•º DataFrameÏóê Ï†ÅÏö©
        # Map for matching web URL columns with their correct names in the dataframe
        assumed_url_cols = {
            'haereum': 'Î≥∏ÏÇ¨ÏÉÅÌíàÎßÅÌÅ¨',      # Changed from 'Î≥∏ÏÇ¨ÎßÅÌÅ¨'
            'kogift': 'Í≥†Î†§Í∏∞ÌîÑÌä∏ ÏÉÅÌíàÎßÅÌÅ¨', # Changed from 'Í≥†Î†§ ÎßÅÌÅ¨'
            'naver': 'ÎÑ§Ïù¥Î≤Ñ ÏáºÌïë ÎßÅÌÅ¨'     # Changed from 'ÎÑ§Ïù¥Î≤Ñ ÎßÅÌÅ¨'
        }

        # Pre-compute Koreagift product info existence for all rows
        # Will be used to determine if image should be assigned
        kogift_product_info_exists = []
        for idx in range(len(result_df)):
            if idx >= len(result_df):
                kogift_product_info_exists.append(False)
                continue
                
            row_data = result_df.iloc[idx]
            has_kogift_info = False
            
            # Check for Koreagift link
            kogift_link_col = 'Í≥†Î†§Í∏∞ÌîÑÌä∏ ÏÉÅÌíàÎßÅÌÅ¨'
            if kogift_link_col in row_data and row_data[kogift_link_col]:
                if isinstance(row_data[kogift_link_col], str) and row_data[kogift_link_col].strip() not in ['', '-', 'None', None]:
                    has_kogift_info = True
            
            # Check for Koreagift price
            if not has_kogift_info:
                kogift_price_col = 'ÌåêÎß§Í∞Ä(VÌè¨Ìï®)(2)'
                if kogift_price_col in row_data and pd.notna(row_data[kogift_price_col]) and row_data[kogift_price_col] not in [0, '-', '', None]:
                    has_kogift_info = True
                    
            # Check for alternative price column
            if not has_kogift_info:
                alt_kogift_price_col = 'ÌåêÎß§Îã®Í∞Ä(VÌè¨Ìï®)(2)'
                if alt_kogift_price_col in row_data and pd.notna(row_data[alt_kogift_price_col]) and row_data[alt_kogift_price_col] not in [0, '-', '', None]:
                    has_kogift_info = True
            
            kogift_product_info_exists.append(has_kogift_info)
        
        logging.info(f"Pre-computed Koreagift product info existence for {len(kogift_product_info_exists)} rows")
        logging.info(f"Found {sum(kogift_product_info_exists)} rows with Koreagift product info")

        # Apply results to DataFrame
        mismatch_count = 0
        naver_mismatch_count = 0
        
        # Keep track of used Haereum images
        used_haereum_images = set()
        
        for idx, (match_set, product_name) in enumerate(zip(verified_matches, product_names)):
            if idx >= len(result_df):
                continue
                
            if match_set:
                haereum_data, kogift_data, naver_data = match_set
                
                # Process Haereum image
                if haereum_data and isinstance(haereum_data, tuple) and len(haereum_data) >= 2:
                    haereum_path, haereum_score = haereum_data[:2]  # Correctly extract path and score
                    if haereum_path and haereum_path in haereum_images:
                        used_haereum_images.add(haereum_path)
                        haereum_image_info_from_metadata = haereum_images[haereum_path] # Metadata from disk scan
                        
                        # Determine web URL for the Haereum image
                        current_haereum_url = None
                        
                        # 1. Try extracting from existing image dict in the input df
                        if idx < len(df) and 'Î≥∏ÏÇ¨ Ïù¥ÎØ∏ÏßÄ' in df.columns and isinstance(df.iloc[idx].get('Î≥∏ÏÇ¨ Ïù¥ÎØ∏ÏßÄ'), dict):
                            url_from_dict = df.iloc[idx]['Î≥∏ÏÇ¨ Ïù¥ÎØ∏ÏßÄ'].get('url')
                            if isinstance(url_from_dict, str) and url_from_dict.startswith(('http://', 'https://')):
                                current_haereum_url = url_from_dict
                        
                        # 2. Fallback: Check a separate URL column in input df (e.g., 'Î≥∏ÏÇ¨Ïù¥ÎØ∏ÏßÄURL')
                        if not current_haereum_url and idx < len(df) and scraped_haereum_url_col_input_df in df.columns:
                            url_val_from_separate_col = df.iloc[idx].get(scraped_haereum_url_col_input_df)
                            if isinstance(url_val_from_separate_col, str) and url_val_from_separate_col.startswith(('http://', 'https://')):
                                current_haereum_url = url_val_from_separate_col
                        
                        # 3. Fallback: URL from metadata (less likely to be a web URL)
                        if not current_haereum_url:
                            current_haereum_url = haereum_image_info_from_metadata.get('url', '')

                        image_data = {
                            'url': current_haereum_url, 
                            'local_path': haereum_image_info_from_metadata.get('path', haereum_path),
                            'source': 'haereum',
                            'product_name': product_name,
                            'similarity': haereum_score,
                            'original_path': haereum_path
                        }
                        result_df.at[idx, 'Î≥∏ÏÇ¨ Ïù¥ÎØ∏ÏßÄ'] = image_data
                        
                        # Also update the separate URL column in result_df for consistency if it exists
                        if scraped_haereum_url_col_input_df not in result_df.columns:
                            result_df[scraped_haereum_url_col_input_df] = None
                        result_df.at[idx, scraped_haereum_url_col_input_df] = image_data['url']
                
                # Process Kogift image
                if kogift_data and isinstance(kogift_data, tuple) and len(kogift_data) >= 2:
                    kogift_path, kogift_score = kogift_data[:2]
                    if kogift_path and kogift_path in kogift_images:
                        has_kogift_product_info = idx < len(kogift_product_info_exists) and kogift_product_info_exists[idx]
                        if has_kogift_product_info:
                            kogift_image_info_from_metadata = kogift_images[kogift_path]
                            
                            current_kogift_url = None
                            original_url_for_dict = None
                            original_crawled_url_for_dict = None

                            # 1. Check URL from the dictionary in the input df's 'Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ' column
                            input_kogift_dict = None
                            if idx < len(df) and 'Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ' in df.columns and isinstance(df.iloc[idx].get('Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ'), dict):
                                input_kogift_dict = df.iloc[idx]['Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ']
                                url_from_dict = input_kogift_dict.get('url')
                                if isinstance(url_from_dict, str) and url_from_dict.startswith(('http://', 'https://')):
                                    current_kogift_url = url_from_dict
                                # Preserve original_url and original_crawled_url if they exist in the input dict
                                original_url_for_dict = input_kogift_dict.get('original_url')
                                original_crawled_url_for_dict = input_kogift_dict.get('original_crawled_url')
                            
                            # 2. Fallback: Check a separate URL column in input df (e.g., 'Í≥†Î†§Í∏∞ÌîÑÌä∏Ïù¥ÎØ∏ÏßÄURL')
                            if not current_kogift_url and idx < len(df) and scraped_kogift_url_col_input_df in df.columns:
                                url_val_from_separate_col = df.iloc[idx].get(scraped_kogift_url_col_input_df)
                                if isinstance(url_val_from_separate_col, str) and url_val_from_separate_col.startswith(('http://', 'https://')):
                                    current_kogift_url = url_val_from_separate_col
                            
                            # 3. Fallback: URL from metadata (less likely to be a web URL)
                            if not current_kogift_url:
                                current_kogift_url = kogift_image_info_from_metadata.get('url', '')
                            
                            # If original_url/original_crawled_url were not in input_dict, try metadata
                            if not original_url_for_dict:
                                original_url_for_dict = kogift_image_info_from_metadata.get('original_url')
                            if not original_crawled_url_for_dict:
                                original_crawled_url_for_dict = kogift_image_info_from_metadata.get('original_crawled_url')

                            image_data = {
                                'url': current_kogift_url, 
                                'local_path': kogift_image_info_from_metadata.get('path', kogift_path),
                                'source': 'kogift',
                                'product_name': product_name,
                                'similarity': kogift_score,  
                                'original_path': kogift_path,
                                'original_url': original_url_for_dict,
                                'original_crawled_url': original_crawled_url_for_dict
                            }
                            result_df.at[idx, 'Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ'] = image_data
                            logger.info(f"Row {idx} ('{product_name}'): Assigned Kogift image '{os.path.basename(kogift_path)}' with URL: {current_kogift_url}")
                        else:
                            logger.warning(f"Row {idx} ('{product_name}'): Matched Kogift image '{os.path.basename(kogift_path)}' (Score: {kogift_score:.4f}) BUT product info missing. Discarding image.")
                            mismatch_count += 1
                    else:
                        logger.warning(f"Row {idx} ('{product_name}'): Kogift matched path '{kogift_path}' not in kogift_images dict or path is None.")
                elif kogift_data: # Matched but not in expected tuple format
                    logger.warning(f"Row {idx} ('{product_name}'): Kogift image matched but data format unexpected: {kogift_data}")
                else: # No Kogift match found by find_best_image_matches
                    logger.debug(f"Row {idx} ('{product_name}'): No Kogift image matched by find_best_image_matches.")

                # Process Naver image
                if naver_data and isinstance(naver_data, tuple) and len(naver_data) >= 2:
                    naver_path, naver_score = naver_data[:2]  # Correctly extract path and score
                    if naver_path and naver_path in naver_images:
                        # Check if there's Naver product info
                        naver_info_exists = False
                        
                        # Check for Naver link
                        naver_link_col = 'ÎÑ§Ïù¥Î≤Ñ ÏáºÌïë ÎßÅÌÅ¨'
                        if naver_link_col in result_df.columns and pd.notna(result_df.at[idx, naver_link_col]) and result_df.at[idx, naver_link_col] not in ['', '-', 'None', None]:
                            naver_info_exists = True
                            
                        # Check for alternative Naver link column
                        alt_naver_link_col = 'ÎÑ§Ïù¥Î≤Ñ ÎßÅÌÅ¨'
                        if not naver_info_exists and alt_naver_link_col in result_df.columns and pd.notna(result_df.at[idx, alt_naver_link_col]) and result_df.at[idx, alt_naver_link_col] not in ['', '-', 'None', None]:
                            naver_info_exists = True
                            
                        # Check for Naver price
                        naver_price_cols = ['ÌåêÎß§Îã®Í∞Ä(VÌè¨Ìï®)(3)', 'ÎÑ§Ïù¥Î≤Ñ ÌåêÎß§Îã®Í∞Ä']
                        for price_col in naver_price_cols:
                            if not naver_info_exists and price_col in result_df.columns and pd.notna(result_df.at[idx, price_col]) and result_df.at[idx, price_col] not in [0, '-', '', None]:
                                naver_info_exists = True
                                break
                                
                        # For Naver, we're more lenient - still add the image even without product info,
                        # but log the mismatch for tracking
                        if not naver_info_exists:
                            naver_mismatch_count += 1
                            logging.warning(f"Adding Naver image despite missing product info: {product_name}")
                            
                        naver_image_info_from_metadata = naver_images[naver_path]
                        
                        current_naver_image_url = None
                        product_page_url_for_dict = None

                        # 1. Check URL from the dictionary in the input df's 'ÎÑ§Ïù¥Î≤Ñ Ïù¥ÎØ∏ÏßÄ' column
                        input_naver_dict = None
                        if idx < len(df) and 'ÎÑ§Ïù¥Î≤Ñ Ïù¥ÎØ∏ÏßÄ' in df.columns and isinstance(df.iloc[idx].get('ÎÑ§Ïù¥Î≤Ñ Ïù¥ÎØ∏ÏßÄ'), dict):
                            input_naver_dict = df.iloc[idx]['ÎÑ§Ïù¥Î≤Ñ Ïù¥ÎØ∏ÏßÄ']
                            url_from_dict = input_naver_dict.get('url') # This should be direct image URL
                            if isinstance(url_from_dict, str) and (('phinf.pstatic.net' in url_from_dict) or any(url_from_dict.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif'])):
                                current_naver_image_url = url_from_dict
                            # Preserve product_page_url if it exists and is not an image URL
                            prod_page_url = input_naver_dict.get('product_page_url')
                            if isinstance(prod_page_url, str) and prod_page_url.startswith(('http://', 'https://')) and not (('phinf.pstatic.net' in prod_page_url) or any(prod_page_url.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif'])):
                                product_page_url_for_dict = prod_page_url
                        
                        # 2. Fallback: Check a separate URL column in input df for direct image URL
                        if not current_naver_image_url and idx < len(df) and scraped_naver_url_col_input_df in df.columns:
                            url_val_from_separate_col = df.iloc[idx].get(scraped_naver_url_col_input_df)
                            if isinstance(url_val_from_separate_col, str) and (('phinf.pstatic.net' in url_val_from_separate_col) or any(url_val_from_separate_col.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif'])):
                                current_naver_image_url = url_val_from_separate_col
                        
                        # 3. Fallback: URL from metadata (less likely to be a direct image web URL)
                        if not current_naver_image_url:
                            meta_url = naver_image_info_from_metadata.get('url')
                            if isinstance(meta_url, str) and (('phinf.pstatic.net' in meta_url) or any(meta_url.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif'])):
                                current_naver_image_url = meta_url

                        # For product_page_url, if not found in input_dict, try metadata's product_url
                        if not product_page_url_for_dict:
                            meta_prod_page_url = naver_image_info_from_metadata.get('product_url')
                            if isinstance(meta_prod_page_url, str) and meta_prod_page_url.startswith(('http://', 'https://')) and not (('phinf.pstatic.net' in meta_prod_page_url) or any(meta_prod_page_url.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif'])):
                                product_page_url_for_dict = meta_prod_page_url
                        
                        image_data = {
                            'url': current_naver_image_url, 
                            'local_path': naver_image_info_from_metadata.get('path', naver_path),
                            'source': 'naver',
                            'product_name': product_name,
                            'similarity': naver_score,  
                            'original_path': naver_path,
                            'product_page_url': product_page_url_for_dict
                        }
                        result_df.at[idx, 'ÎÑ§Ïù¥Î≤Ñ Ïù¥ÎØ∏ÏßÄ'] = image_data
                        
                        # Logic for 'ÎÑ§Ïù¥Î≤Ñ ÏáºÌïë ÎßÅÌÅ¨' (product page link in result_df)
                        shopping_link_col_in_result_df = 'ÎÑ§Ïù¥Î≤Ñ ÏáºÌïë ÎßÅÌÅ¨'
                        final_product_page_link = None

                        # A. Prioritize product_page_url_for_dict if available from above
                        if product_page_url_for_dict:
                            final_product_page_link = product_page_url_for_dict
                        
                        # B. Fallback to checking a separate 'ÎÑ§Ïù¥Î≤Ñ ÏáºÌïë ÎßÅÌÅ¨' column in the input df
                        if not final_product_page_link and idx < len(df) and shopping_link_col_in_result_df in df.columns:
                             url_val_from_input_shopping_link_col = df.iloc[idx].get(shopping_link_col_in_result_df)
                             if isinstance(url_val_from_input_shopping_link_col, str) and url_val_from_input_shopping_link_col.startswith(('http://', 'https://')) and not (('phinf.pstatic.net' in url_val_from_input_shopping_link_col) or any(url_val_from_input_shopping_link_col.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif'])):
                                 final_product_page_link = url_val_from_input_shopping_link_col
                        
                        # C. Fallback to naver_image_info_from_metadata's product_url or url if it's a page link (already covered by product_page_url_for_dict logic)
                        # No new logic needed here for C as it's incorporated into product_page_url_for_dict

                        if final_product_page_link:
                            if shopping_link_col_in_result_df not in result_df.columns: result_df[shopping_link_col_in_result_df] = None
                            # Set only if current value is empty/placeholder OR if the new link is different and valid
                            current_shopping_link_val = result_df.at[idx, shopping_link_col_in_result_df]
                            if not pd.notna(current_shopping_link_val) or current_shopping_link_val in ['', '-', 'None', None] or current_shopping_link_val != final_product_page_link:
                                result_df.at[idx, shopping_link_col_in_result_df] = final_product_page_link
        
        # Final check: Ensure EVERY row has a Haereum image 
        # If any row is missing a Haereum image, assign a random one
        haereum_count_before = sum(1 for i in range(len(result_df)) if isinstance(result_df.at[i, 'Î≥∏ÏÇ¨ Ïù¥ÎØ∏ÏßÄ'], dict))
        haereum_added_count = 0
        
        # Get available Haereum images that haven't been used yet
        available_haereum = [path for path in haereum_images if path not in used_haereum_images]
        if not available_haereum and haereum_images:
            # If all images are used but we still need more, reset and use them again
            available_haereum = list(haereum_images.keys())
            logging.warning(f"All Haereum images have been used. Reusing images for remaining rows.")
        
        for idx in range(len(result_df)):
            if idx >= len(result_df):
                continue
                
            # Check if this row is missing a Haereum image
            if not isinstance(result_df.at[idx, 'Î≥∏ÏÇ¨ Ïù¥ÎØ∏ÏßÄ'], dict):
                product_name = result_df.at[idx, 'ÏÉÅÌíàÎ™Ö'] if 'ÏÉÅÌíàÎ™Ö' in result_df.columns else f"Row {idx+1}"
                logging.warning(f"Row {idx} ('{product_name}'): Missing Haereum image. Assigning random image.")
                
                # Assign a random Haereum image
                if available_haereum:
                    haereum_path = available_haereum.pop(0)  # Take the first available
                    if not available_haereum and haereum_images:
                        # If we ran out, reset the list
                        available_haereum = list(haereum_images.keys())
                    
                    haereum_image_info_from_metadata = haereum_images[haereum_path]
                    
                    # Create image data
                    image_data = {
                        'url': haereum_image_info_from_metadata.get('url', ''), 
                        'local_path': haereum_image_info_from_metadata.get('path', haereum_path),
                        'source': 'haereum',
                        'product_name': product_name,
                        'similarity': 0.01,  # Very low score to indicate this is a desperate assignment
                        'original_path': haereum_path
                    }
                    result_df.at[idx, 'Î≥∏ÏÇ¨ Ïù¥ÎØ∏ÏßÄ'] = image_data
                    haereum_added_count += 1
        
        # Log the results of ensuring Haereum images
        haereum_count_after = sum(1 for i in range(len(result_df)) if isinstance(result_df.at[i, 'Î≥∏ÏÇ¨ Ïù¥ÎØ∏ÏßÄ'], dict))
        logging.info(f"Haereum image count: {haereum_count_before} -> {haereum_count_after} (Added {haereum_added_count} random images)")
        
        # Ïù¥ÎØ∏ÏßÄ Í≤ΩÎ°ú Î∂àÏùºÏπò ÏàòÏ†ï (Î°úÏª¨ ÌååÏùºÏù¥ Ïù¥ÎèôÎêú Í≤ΩÏö∞)
        for idx in range(len(result_df)):
            if idx >= len(result_df):
                continue
                
            # Check and fix Koreagift image paths
            kogift_img = result_df.at[idx, 'Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ']
            if isinstance(kogift_img, dict) and 'local_path' in kogift_img:
                local_path = kogift_img['local_path']
                if not os.path.exists(local_path):
                    # Try to find the file in the Kogift directory by basename
                    basename = os.path.basename(local_path)
                    for directory in [kogift_dir, main_img_dir]:
                        possible_path = os.path.join(directory, basename)
                        if os.path.exists(possible_path):
                            kogift_img['local_path'] = possible_path
                            result_df.at[idx, 'Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ'] = kogift_img
                            logging.info(f"Fixed Koreagift image path from {local_path} to {possible_path}")
                            break
            
            # Check and fix Naver image paths
            naver_img = result_df.at[idx, 'ÎÑ§Ïù¥Î≤Ñ Ïù¥ÎØ∏ÏßÄ']
            if isinstance(naver_img, dict) and 'local_path' in naver_img:
                local_path = naver_img['local_path']
                if not os.path.exists(local_path):
                    # Try to find the file in the Naver directory by basename or URL hash
                    basename = os.path.basename(local_path)
                    url_hash = None
                    if 'url' in naver_img and naver_img['url']:
                        url_hash = hashlib.md5(naver_img['url'].encode()).hexdigest()[:10]
                    
                    found = False
                    for directory in [naver_dir, main_img_dir]:
                        if os.path.exists(directory):
                            for filename in os.listdir(directory):
                                file_path = os.path.join(directory, filename)
                                if os.path.isfile(file_path) and (
                                    basename == filename or 
                                    (url_hash and url_hash in filename)
                                ):
                                    naver_img['local_path'] = file_path
                                    result_df.at[idx, 'ÎÑ§Ïù¥Î≤Ñ Ïù¥ÎØ∏ÏßÄ'] = naver_img
                                    logging.info(f"Fixed Naver image path from {local_path} to {file_path}")
                                    found = True
                                    break
                        if found:
                            break
        
        # Count final images
        haereum_count = result_df['Î≥∏ÏÇ¨ Ïù¥ÎØ∏ÏßÄ'].apply(lambda x: isinstance(x, dict)).sum()
        kogift_count = result_df['Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ'].apply(lambda x: isinstance(x, dict)).sum()
        naver_count = result_df['ÎÑ§Ïù¥Î≤Ñ Ïù¥ÎØ∏ÏßÄ'].apply(lambda x: isinstance(x, dict)).sum()
        
        logging.info(f"ÌÜµÌï©: Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠ ÏôÑÎ£å - Ìï¥Ïò§Î¶Ñ: {haereum_count}Í∞ú, Í≥†Î†§Í∏∞ÌîÑÌä∏: {kogift_count}Í∞ú, ÎÑ§Ïù¥Î≤Ñ: {naver_count}Í∞ú")
        
        return result_df
    
    except Exception as e:
        logging.error(f"ÌÜµÌï©: Ïù¥ÎØ∏ÏßÄ ÌÜµÌï© Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}", exc_info=True)
        return df

def improved_kogift_image_matching(df: pd.DataFrame) -> pd.DataFrame:
    """
    Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠ Í∞úÏÑ† Ìï®Ïàò
    
    Ïù¥ÎØ∏ÏßÄ URLÏù¥ ÎàÑÎùΩÎêòÏóàÍ±∞ÎÇò ÏûòÎ™ªÎêú Í≤ΩÏö∞ Ïã§Ï†ú ÏÉÅÌíà ÎßÅÌÅ¨Î•º ÌôúÏö©Ìï¥ Ïò¨Î∞îÎ•∏ Ïù¥ÎØ∏ÏßÄ URLÏùÑ Í∞ÄÏ†∏ÏòµÎãàÎã§.
    
    Args:
        df: ÌòÑÏû¨ DataFrame
        
    Returns:
        updated DataFrame with improved Kogift image URLs
    """
    try:
        if 'Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ' not in df.columns or 'Í≥†Î†§Í∏∞ÌîÑÌä∏ ÏÉÅÌíàÎßÅÌÅ¨' not in df.columns:
            logging.warning("ÌïÑÏöîÌïú Ïª¨Îüº(Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ ÎòêÎäî Í≥†Î†§Í∏∞ÌîÑÌä∏ ÏÉÅÌíàÎßÅÌÅ¨)Ïù¥ ÏóÜÏñ¥ Ïù¥ÎØ∏ÏßÄ ÎßÅÌÅ¨ ÏàòÏ†ï Î∂àÍ∞Ä")
            return df
        
        update_count = 0
        result_df = df.copy()
        
        for idx, row in result_df.iterrows():
            # Check if already has a valid URL
            img_data = row.get('Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ')
            if not isinstance(img_data, dict):
                continue
                
            # Check if URL is missing or a placeholder
            url = img_data.get('url')
            if url and isinstance(url, str) and not url.startswith('http://placeholder.url/') and url.startswith(('http://', 'https://')):
                # Ïù¥ÎØ∏ Ïú†Ìö®Ìïú URLÏù¥ ÏûàÎäî Í≤ΩÏö∞
                continue
                
            # Check if we have an original_url - Ï∂îÍ∞ÄÎêú Î∂ÄÎ∂Ñ
            original_url = img_data.get('original_url')
            if original_url and isinstance(original_url, str) and original_url.startswith(('http://', 'https://')):
                # ÏõêÎ≥∏ URL Ï†ïÎ≥¥Í∞Ä ÏûàÏúºÎ©¥ ÏÇ¨Ïö©
                img_data['url'] = original_url
                result_df.at[idx, 'Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ'] = img_data
                update_count += 1
                logging.info(f"Row {idx}: Using original URL {original_url[:50]}... for Kogift image")
                continue
                
            # Check if we have an original_crawled_url
            original_crawled_url = img_data.get('original_crawled_url')
            if original_crawled_url and isinstance(original_crawled_url, str) and original_crawled_url.startswith(('http://', 'https://')):
                # original_crawled_url Ï†ïÎ≥¥Í∞Ä ÏûàÏúºÎ©¥ ÏÇ¨Ïö©
                img_data['url'] = original_crawled_url
                result_df.at[idx, 'Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ'] = img_data
                update_count += 1
                logging.info(f"Row {idx}: Using original crawled URL {original_crawled_url[:50]}... for Kogift image")
                continue
                
            # ÏÉÅÌíà ÎßÅÌÅ¨Í∞Ä ÏûàÎäîÏßÄ ÌôïÏù∏
            product_link = row.get('Í≥†Î†§Í∏∞ÌîÑÌä∏ ÏÉÅÌíàÎßÅÌÅ¨')
            if not product_link or not isinstance(product_link, str) or not product_link.startswith(('http://', 'https://')):
                continue
                
            # Get product code from URL
            try:
                # ÏÉÅÌíà ÏΩîÎìú Ï∂îÏ∂ú (URL Ìå®ÌÑ¥Ïóê Îî∞Îùº Ï°∞Ï†ï ÌïÑÏöî)
                product_code = None
                
                # URLÏóêÏÑú ÏÉÅÌíà ÏΩîÎìú Ï∂îÏ∂ú ÏãúÎèÑ (Ïó¨Îü¨ Ìå®ÌÑ¥ ÏßÄÏõê)
                if 'goods_view.php' in product_link:
                    # goods_view.php?goodsno=12345 Ìå®ÌÑ¥ Ï≤òÎ¶¨
                    parts = product_link.split('goodsno=')
                    if len(parts) > 1:
                        product_code = parts[1].split('&')[0]
                elif '/goods/' in product_link:
                    # /goods/1234 Ìå®ÌÑ¥ Ï≤òÎ¶¨
                    parts = product_link.split('/goods/')
                    if len(parts) > 1:
                        product_code = parts[1].split('/')[0].split('?')[0]
                elif 'goodsDetail' in product_link:
                    # goodsDetail?goodsNo=1234 Ìå®ÌÑ¥ Ï≤òÎ¶¨
                    parts = product_link.split('goodsNo=')
                    if len(parts) > 1:
                        product_code = parts[1].split('&')[0]
                # Í≥†Î†§Í∏∞ÌîÑÌä∏ ÌäπÌôî Ìå®ÌÑ¥
                elif 'no=' in product_link:
                    # Í≥†Î†§Í∏∞ÌîÑÌä∏ URL Ìå®ÌÑ¥ Ï≤òÎ¶¨ (no=12345)
                    parts = product_link.split('no=')
                    if len(parts) > 1:
                        product_code = parts[1].split('&')[0]
                
                if not product_code:
                    logging.warning(f"Row {idx}: ÏÉÅÌíà ÎßÅÌÅ¨ÏóêÏÑú ÏΩîÎìúÎ•º Ï∂îÏ∂úÌï† Ïàò ÏóÜÏùå: {product_link}")
                    # ÏÉÅÌíà ÏΩîÎìúÎ•º Ï∂îÏ∂úÌï† Ïàò ÏóÜÎäî Í≤ΩÏö∞ ÏÉÅÌíà ÎßÅÌÅ¨ ÏûêÏ≤¥Î•º Ïù¥ÎØ∏ÏßÄ URLÎ°ú ÏÇ¨Ïö©
                    img_data['url'] = product_link
                    img_data['original_url'] = product_link  # Ï∂îÍ∞Ä: ÏõêÎ≥∏ URL Ï†ÄÏû•
                    result_df.at[idx, 'Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ'] = img_data
                    update_count += 1
                    logging.info(f"Row {idx}: ÏΩîÎìú Ï∂îÏ∂ú Ïã§Ìå®, ÏÉÅÌíà ÎßÅÌÅ¨Î•º Ïù¥ÎØ∏ÏßÄ URLÎ°ú ÏÇ¨Ïö© - {product_link}")
                    continue
                    
                # ÏÉÅÌíà Ïù¥ÎØ∏ÏßÄ URL ÏÉùÏÑ±
                if 'koreagift.com' in product_link.lower():
                    # Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ Ìå®ÌÑ¥
                    # 1. Í∏∞Î≥∏ Ìå®ÌÑ¥: shop_{product_code}.jpg
                    # Ensure product_code is just the base number (e.g., 1707873892937710, not 1707873892937710_0)
                    image_url = f"https://koreagift.com/ez/upload/mall/shop_{product_code}_0.jpg"
                else:
                    # ÏùºÎ∞òÏ†ÅÏù∏ ÏáºÌïëÎ™∞ Ïù¥ÎØ∏ÏßÄ Ìå®ÌÑ¥
                    domain_parts = product_link.split('/')
                    if len(domain_parts) > 2:
                        base_domain = domain_parts[2]
                        image_url = f"https://{base_domain}/data/item/goods{product_code}/thumb-{product_code}_500x500.jpg"
                    else:
                        # ÎèÑÎ©îÏù∏ÏùÑ Ï∂îÏ∂úÌï† Ïàò ÏóÜÎäî Í≤ΩÏö∞ ÏÉÅÌíà ÎßÅÌÅ¨ ÏûêÏ≤¥Î•º Ïù¥ÎØ∏ÏßÄ URLÎ°ú ÏÇ¨Ïö©
                        image_url = product_link
                        logging.warning(f"Row {idx}: ÏÉÅÌíà ÎßÅÌÅ¨ {product_link}ÏóêÏÑú ÎèÑÎ©îÏù∏ÏùÑ Ï∂îÏ∂úÌï† Ïàò ÏóÜÏñ¥ ÏÉÅÌíà ÎßÅÌÅ¨ ÏûêÏ≤¥Î•º ÏÇ¨Ïö©")
                
                # Í∏∞Ï°¥ Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏
                img_data['url'] = image_url
                img_data['original_url'] = image_url  # Ï∂îÍ∞Ä: ÏõêÎ≥∏ URL Ï†ÄÏû•
                img_data['original_crawled_url'] = image_url  # Ï∂îÍ∞Ä: ÌÅ¨Î°§ÎßÅÎêú URL Ï†ÄÏû•
                img_data['product_id'] = product_code  # Ï∂îÍ∞Ä: ÏÉÅÌíà ÏΩîÎìú Ï†ÄÏû•
                result_df.at[idx, 'Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ'] = img_data
                update_count += 1
                logging.debug(f"Row {idx}: Í≥†Î†§Í∏∞ÌîÑÌä∏ URL Ï∂îÍ∞Ä - {image_url}")
                
            except Exception as e:
                logging.error(f"Row {idx}: Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ URL ÏÉùÏÑ± Ïò§Î•ò - {str(e)}")
                # Ïò§Î•òÍ∞Ä Î∞úÏÉùÌïú Í≤ΩÏö∞ÏóêÎèÑ ÏÉÅÌíà ÎßÅÌÅ¨ ÏûêÏ≤¥Î•º Ïù¥ÎØ∏ÏßÄ URLÎ°ú ÏÇ¨Ïö©
                if product_link and isinstance(product_link, str) and product_link.startswith(('http://', 'https://')):
                    img_data['url'] = product_link
                    img_data['original_url'] = product_link
                    result_df.at[idx, 'Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ'] = img_data
                    update_count += 1
                    logging.info(f"Row {idx}: Ïò§Î•ò Î∞úÏÉù, ÏÉÅÌíà ÎßÅÌÅ¨Î•º Ïù¥ÎØ∏ÏßÄ URLÎ°ú ÏÇ¨Ïö© - {product_link}")
                continue
                
        logging.info(f"improved_kogift_image_matching fixed {update_count} image links")
        return result_df
        
    except Exception as e:
        logging.error(f"Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ ÎßÅÌÅ¨ Í∞úÏÑ† Ï§ë Ïò§Î•ò: {str(e)}")
        return df

def integrate_and_filter_images(df: pd.DataFrame, config: configparser.ConfigParser, 
                            save_excel_output=False) -> pd.DataFrame:
    """
    Integrates and filters images from all sources, applying strict quality controls.
    
    This function performs the following steps:
    1. Integrate images from all sources with higher thresholds
    2. Improve Kogift image matching
    3. Apply strict filtering based on similarity scores
    4. Perform URL validation to reject invalid URLs (especially front/ URLs)
    5. Final quality control check
    
    Args:
        df: DataFrame with product data
        config: Configuration settings
        save_excel_output: Whether to save an Excel output file with images
        
    Returns:
        DataFrame with high-quality integrated and filtered images
    """
    logger.info("Integrating and filtering images with enhanced quality controls")
    
    # Step 1: Integrate images from all sources with higher thresholds
    df_with_images = integrate_images(df, config)
    logger.info(f"Image integration completed. DataFrame shape: {df_with_images.shape}")
    
    # Step 2: Improve Kogift image matching
    logger.info("Improving Kogift image matching with strict quality controls...")
    df_kogift_improved = improved_kogift_image_matching(df_with_images)
    logger.info(f"Kogift image matching improvement completed. DataFrame shape: {df_kogift_improved.shape}")
    
    # Step 3: Apply image filtering based on similarity and URL validity
    df_filtered = filter_images_by_similarity(df_kogift_improved, config)
    logger.info(f"Image filtering completed. DataFrame shape: {df_filtered.shape}")
    
    result_df = df_filtered.copy()
    
    # Ensure essential image columns exist in result_df before further processing and counting
    # This is crucial if any of the upstream functions returned the original df without these columns due to an error.
    required_image_columns = ['Î≥∏ÏÇ¨ Ïù¥ÎØ∏ÏßÄ', 'Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ', 'ÎÑ§Ïù¥Î≤Ñ Ïù¥ÎØ∏ÏßÄ']
    for col_name in required_image_columns:
        if col_name not in result_df.columns:
            logger.warning(f"Column '{col_name}' missing in result_df before final validation and counting. Adding it with None values.")
            result_df[col_name] = None
            
    # Step 4: Final validation - ensure all URLs are valid and reject any problematic images
    # Now with less strict validation for Naver images
    logger.info("Performing final URL validation and quality check...")
    
    # Add an ImageFiltering section to config if it doesn't exist
    if 'ImageFiltering' not in config:
        config.add_section('ImageFiltering')
    
    # Get validation settings from config
    try:
        skip_naver_validation = config.getboolean('ImageFiltering', 'skip_naver_validation', fallback=True)
    except (configparser.NoSectionError, configparser.NoOptionError):
        skip_naver_validation = True  # Default to skipping Naver validation to avoid filtering
    
    for idx in range(len(result_df)):
        product_name = result_df.at[idx, 'ÏÉÅÌíàÎ™Ö'] if 'ÏÉÅÌíàÎ™Ö' in result_df.columns else f"Index {idx}"
        
        # Check Naver image URLs - much more lenient validation
        if 'ÎÑ§Ïù¥Î≤Ñ Ïù¥ÎØ∏ÏßÄ' in result_df.columns:
            naver_data = result_df.at[idx, 'ÎÑ§Ïù¥Î≤Ñ Ïù¥ÎØ∏ÏßÄ']
            if isinstance(naver_data, dict):
                # Check if there's a valid local_path
                local_path = naver_data.get('local_path')
                has_valid_local_path = local_path and os.path.exists(str(local_path))
                
                # Check URL validity but be very lenient
                url = naver_data.get('url')
                has_valid_url = url and isinstance(url, str) and url.startswith(('http://', 'https://'))
                
                # Special case: Keep Naver images with valid local path even without valid URL
                if has_valid_local_path:
                    logger.info(f"Row {idx} (Product: '{product_name}'): Keeping Naver image with invalid URL but valid local path.")
                    
                    # Try to fix/add URL from other available data
                    original_path = naver_data.get('original_path')
                    image_url = naver_data.get('original_url')
                    product_id = naver_data.get('product_id')
                    
                    # Try to find a valid URL but don't clear data if none found
                    if not has_valid_url:
                        if image_url and isinstance(image_url, str) and image_url.startswith(('http://', 'https://')):
                            naver_data['url'] = image_url
                        elif original_path and isinstance(original_path, str) and original_path.startswith(('http://', 'https://')):
                            naver_data['url'] = original_path
                        elif product_id:
                            # Construct URL from product_id as a last resort
                            constructed_url = f"https://shopping-phinf.pstatic.net/main_{product_id}/{product_id}.jpg"
                            naver_data['url'] = constructed_url
                            logger.info(f"Row {idx}: Constructed Naver URL from product_id: {constructed_url}")
                    
                    # Update DataFrame with possibly updated naver_data
                    result_df.at[idx, 'ÎÑ§Ïù¥Î≤Ñ Ïù¥ÎØ∏ÏßÄ'] = naver_data
                elif not has_valid_url and skip_naver_validation:
                    # If we're skipping validation and there's no valid URL, warn but don't remove
                    logger.warning(f"Row {idx}: No valid URL found for Naver image, but keeping data due to skip_naver_validation=True.")
                elif not has_valid_url and not has_valid_local_path:
                    # Only clear if both URL and local file are invalid AND we're not skipping validation
                    logger.warning(f"Row {idx}: No valid URL or local path found for Naver image. Clearing Naver image data.")
                    result_df.at[idx, 'ÎÑ§Ïù¥Î≤Ñ Ïù¥ÎØ∏ÏßÄ'] = None
                else:
                    # For all other cases, keep the data as is
                    pass
    
    # Count valid images after all processing
    naver_count = sum(1 for i in range(len(result_df)) if isinstance(result_df.at[i, 'ÎÑ§Ïù¥Î≤Ñ Ïù¥ÎØ∏ÏßÄ'], dict))
    kogift_count = sum(1 for i in range(len(result_df)) if isinstance(result_df.at[i, 'Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ'], dict))
    haereum_count = sum(1 for i in range(len(result_df)) if isinstance(result_df.at[i, 'Î≥∏ÏÇ¨ Ïù¥ÎØ∏ÏßÄ'], dict))
    
    logger.info(f"Final image counts after validation: Haereum={haereum_count}, Kogift={kogift_count}, Naver={naver_count}")
    
    # Save Excel output if requested
    if save_excel_output:
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            excel_output = f"image_integration_result_{timestamp}.xlsx"
            
            # Create the Excel file with images
            create_excel_with_images(result_df, excel_output)
            logger.info(f"Created Excel output file with images: {excel_output}")
        except Exception as e:
            logger.error(f"Error creating Excel output: {e}")
    
    return result_df

def filter_images_by_similarity(df: pd.DataFrame, config: configparser.ConfigParser) -> pd.DataFrame:
    """
    Filter images based on similarity scores and URL validity.
    
    Args:
        df: DataFrame containing image data
        config: Configuration settings
        
    Returns:
        Filtered DataFrame
    """
    logger.info("Filtering images based on similarity scores...")
    
    # Get similarity threshold from config
    try:
        similarity_threshold = config.getfloat('ImageFiltering', 'similarity_threshold', fallback=0.4)
    except (configparser.NoSectionError, configparser.NoOptionError):
        similarity_threshold = 0.4  # Default threshold
    
    # Create a copy of the DataFrame to avoid modifying the original
    filtered_df = df.copy()
    
    # Process each row
    for idx in range(len(filtered_df)):
        # Check each image source
        for col_name in ['Î≥∏ÏÇ¨ Ïù¥ÎØ∏ÏßÄ', 'Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ', 'ÎÑ§Ïù¥Î≤Ñ Ïù¥ÎØ∏ÏßÄ']:
            if col_name in filtered_df.columns:
                image_data = filtered_df.at[idx, col_name]
                
                # Skip if no image data
                if not isinstance(image_data, dict):
                    continue
                
                # Get similarity score
                score = image_data.get('score', 0.0)
                
                # Filter out low similarity scores
                if score < similarity_threshold:
                    logger.info(f"Filtering out {col_name} for row {idx} due to low similarity score: {score:.3f}")
                    filtered_df.at[idx, col_name] = None
    
    return filtered_df

def create_excel_with_images(df: pd.DataFrame, output_file: str):
    """
    Create an Excel file with embedded images.
    
    Args:
        df: DataFrame containing image data
        output_file: Path to output Excel file
    """
    logger.info(f"Creating Excel file with images: {output_file}")
    
    # Create a new Excel writer
    writer = pd.ExcelWriter(output_file, engine='openpyxl')
    
    # Write the DataFrame to Excel
    df.to_excel(writer, index=False, sheet_name='Images')
    
    # Get the worksheet
    worksheet = writer.sheets['Images']
    
    # Process each row
    for idx in range(len(df)):
        row_num = idx + 2  # Excel rows start at 1, and we have a header row
        
        # Process each image column
        for col_name in ['Î≥∏ÏÇ¨ Ïù¥ÎØ∏ÏßÄ', 'Í≥†Î†§Í∏∞ÌîÑÌä∏ Ïù¥ÎØ∏ÏßÄ', 'ÎÑ§Ïù¥Î≤Ñ Ïù¥ÎØ∏ÏßÄ']:
            if col_name in df.columns:
                image_data = df.at[idx, col_name]
                
                # Skip if no image data
                if not isinstance(image_data, dict):
                    continue
                
                # Get image path
                image_path = image_data.get('local_path')
                if not image_path or not os.path.exists(str(image_path)):
                    continue
                
                # Add image to Excel
                try:
                    img = Image.open(str(image_path))
                    img_width, img_height = img.size
                    
                    # Resize image if too large
                    max_width = 200
                    if img_width > max_width:
                        ratio = max_width / img_width
                        img_width = max_width
                        img_height = int(img_height * ratio)
                    
                    # Create image cell
                    cell = worksheet.cell(row=row_num, column=df.columns.get_loc(col_name) + 1)
                    cell.value = f"Image: {os.path.basename(str(image_path))}"
                    
                    # Add image
                    img = openpyxl.drawing.image.Image(str(image_path))
                    img.width = img_width
                    img.height = img_height
                    worksheet.add_image(img, cell.coordinate)
                    
                except Exception as e:
                    logger.error(f"Error adding image to Excel: {e}")
    
    # Save the Excel file
    writer.close()
    logger.info(f"Excel file created successfully: {output_file}")

def calculate_text_similarity(text1: str, text2: str) -> float:
    """
    Calculate text similarity between two strings.
    Uses a combination of Levenshtein distance, token overlap, and character n-gram matching.
    """
    # Convert to strings if needed
    str1 = str(text1).lower()
    str2 = str(text2).lower()
    
    # Handle empty strings
    if not str1 or not str2:
        return 0.0
        
    try:
        # Try to use Levenshtein distance if available
        try:
            from Levenshtein import ratio
            lev_ratio = ratio(str1, str2)
        except ImportError:
            # Fallback to a basic similarity measure
            lev_ratio = len(set(str1) & set(str2)) / max(len(set(str1)), len(set(str2)))
        
        # Calculate token overlap
        tokens1 = set(str1.split())
        tokens2 = set(str2.split())
        
        # If either set is empty, default to character-based ratio
        if not tokens1 or not tokens2:
            return lev_ratio
            
        # Calculate Jaccard similarity coefficient
        intersection = tokens1.intersection(tokens2)
        union = tokens1.union(tokens2)
        
        if not union:
            return 0.0
            
        jaccard = len(intersection) / len(union)
        
        # Character n-gram matching (Îçî Í¥ÄÎåÄÌïú Îß§Ïπ≠ÏùÑ ÏúÑÌï¥ Ï∂îÍ∞Ä)
        # 2-gramÍ≥º 3-gram Îß§Ïπ≠ Í≥ÑÏÇ∞
        ngram_similarity = 0.0
        
        # 2-gram Îß§Ïπ≠
        ngrams1_2 = set(str1[i:i+2] for i in range(len(str1)-1))
        ngrams2_2 = set(str2[i:i+2] for i in range(len(str2)-1))
        
        if ngrams1_2 and ngrams2_2:
            ngram_intersection_2 = ngrams1_2.intersection(ngrams2_2)
            ngram_union_2 = ngrams1_2.union(ngrams2_2)
            if ngram_union_2:
                ngram2_sim = len(ngram_intersection_2) / len(ngram_union_2)
                ngram_similarity += ngram2_sim
        
        # 3-gram Îß§Ïπ≠ (Îçî Í∏¥ Î¨∏ÏûêÏó¥ Ìå®ÌÑ¥ Îß§Ïπ≠)
        if len(str1) >= 3 and len(str2) >= 3:
            ngrams1_3 = set(str1[i:i+3] for i in range(len(str1)-2))
            ngrams2_3 = set(str2[i:i+3] for i in range(len(str2)-2))
            
            if ngrams1_3 and ngrams2_3:
                ngram_intersection_3 = ngrams1_3.intersection(ngrams2_3)
                ngram_union_3 = ngrams1_3.union(ngrams2_3)
                if ngram_union_3:
                    ngram3_sim = len(ngram_intersection_3) / len(ngram_union_3)
                    ngram_similarity += ngram3_sim
        
        # Normalize n-gram similarity (if both n-grams used)
        ngram_similarity = ngram_similarity / 2 if len(str1) >= 3 and len(str2) >= 3 else ngram_similarity
        
        # Check for exact substring matches (Î∂ÄÎ∂Ñ Î¨∏ÏûêÏó¥ ÏùºÏπò ÌôïÏù∏)
        # Í∏∏Ïù¥Í∞Ä 3 Ïù¥ÏÉÅÏù∏ ÌÜ†ÌÅ∞Ïù¥ Îã§Î•∏ Î¨∏ÏûêÏó¥Ïóê Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏúºÎ©¥ Î≥¥ÎÑàÏä§ Ï†êÏàò
        substring_bonus = 0.0
        for token in tokens1:
            if len(token) >= 3 and token in str2:
                substring_bonus = max(substring_bonus, 0.15)  # ÏµúÎåÄ 0.15 Î≥¥ÎÑàÏä§
                break
                
        for token in tokens2:
            if len(token) >= 3 and token in str1:
                substring_bonus = max(substring_bonus, 0.15)  # ÏµúÎåÄ 0.15 Î≥¥ÎÑàÏä§
                break
        
        # Weighted average of all similarity measures
        # Í∞ÄÏ§ëÏπò Ï°∞Ï†ïÏúºÎ°ú Îçî Í¥ÄÎåÄÌïú Îß§Ïπ≠ ÌóàÏö©
        combined_similarity = 0.2 * lev_ratio + 0.4 * jaccard + 0.25 * ngram_similarity + substring_bonus
        
        # ÎÑàÎ¨¥ ÎÇÆÏùÄ Ï†êÏàòÏùº Í≤ΩÏö∞ ÏµúÏÜåÍ∞íÏúºÎ°ú Ï°∞Ï†ï (ÏôÑÏ†ÑÌûà Í¥ÄÎ†® ÏóÜÎäî Ìï≠Î™©ÎèÑ ÏûàÏùÑ Ïàò ÏûàÏúºÎØÄÎ°ú)
        return max(combined_similarity, 0.01)  # ÏµúÏÜå 0.01Ïùò Ïú†ÏÇ¨ÎèÑ Î∞òÌôò
        
    except Exception as e:
        logging.error(f"Error calculating text similarity: {e}")
        return 0.0

def extract_product_hash_from_filename(filename: str) -> Optional[str]:
    """
    ÌååÏùºÎ™ÖÏóêÏÑú 16ÏûêÎ¶¨ ÏÉÅÌíàÎ™Ö Ìï¥ÏãúÍ∞íÏùÑ Ï∂îÏ∂úÌï©ÎãàÎã§.
        
    ÌååÏùºÎ™Ö Ìå®ÌÑ¥:
    - prefix_[16ÏûêÌï¥Ïãú]_[8ÏûêÎûúÎç§].jpg (Ïòà: haereum_1234567890abcdef_12345678.jpg)
    - prefix_[16ÏûêÌï¥Ïãú].jpg
        
    Args:
        filename: Ïù¥ÎØ∏ÏßÄ ÌååÏùºÎ™Ö
            
    Returns:
        16ÏûêÎ¶¨ ÏÉÅÌíàÎ™Ö Ìï¥ÏãúÍ∞í ÎòêÎäî None
    """
    try:
        # ÌôïÏû•Ïûê Ï†úÍ±∞
        name_without_ext = os.path.splitext(os.path.basename(filename))[0]
        
        # '_'Î°ú Î∂ÑÎ¶¨
        parts = name_without_ext.split('_')
        
        # prefix_hash_random ÎòêÎäî prefix_hash Ìå®ÌÑ¥ ÌôïÏù∏
        if len(parts) >= 2:
            # prefixÎ•º Ï†úÍ±∞ÌïòÍ≥† Îëê Î≤àÏß∏ Î∂ÄÎ∂ÑÏù¥ 16ÏûêÎ¶¨ Ìï¥ÏãúÏù∏ÏßÄ ÌôïÏù∏
            potential_hash = parts[1]
            if len(potential_hash) == 16 and all(c in '0123456789abcdef' for c in potential_hash.lower()):
                return potential_hash.lower()
        
        # Ï†ÑÏ≤¥Í∞Ä 16ÏûêÎ¶¨ Ìï¥ÏãúÏù∏ Í≤ΩÏö∞ÎèÑ ÌôïÏù∏ (prefixÍ∞Ä ÏóÜÎäî Í≤ΩÏö∞)
        if len(name_without_ext) == 16 and all(c in '0123456789abcdef' for c in name_without_ext.lower()):
            return name_without_ext.lower()
                    
        return None
    except Exception as e:
        logging.debug(f"Error extracting hash from filename {filename}: {e}")
        return None

def generate_product_name_hash(product_name: str) -> str:
    """
    ÏÉÅÌíàÎ™ÖÏúºÎ°úÎ∂ÄÌÑ∞ 16ÏûêÎ¶¨ MD5 Ìï¥ÏãúÍ∞íÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.
    
    Ï†ïÍ∑úÌôî Í≥ºÏ†ï:
    1. Í≥µÎ∞± Î¨∏Ïûê Ï†úÍ±∞
    2. ÏÜåÎ¨∏Ïûê Î≥ÄÌôò
    3. ÌäπÏàòÎ¨∏Ïûê Ï†ïÎ¶¨
    4. MD5 Ìï¥ÏãúÏùò Ï≤´ 16ÏûêÎ¶¨ Î∞òÌôò
        
    Args:
        product_name: ÏÉÅÌíàÎ™Ö
            
    Returns:
        16ÏûêÎ¶¨ Ìï¥ÏãúÍ∞í (Ïã§Ìå® Ïãú Îπà Î¨∏ÏûêÏó¥)
    """
    try:
        if not product_name or not isinstance(product_name, str):
            logging.debug(f"ÏûòÎ™ªÎêú ÏÉÅÌíàÎ™Ö ÏûÖÎ†•: {product_name}")
            return ""
        
        # ÏÉÅÌíàÎ™Ö Ï†ïÍ∑úÌôî
        # 1. ÏïûÎí§ Í≥µÎ∞± Ï†úÍ±∞
        normalized = product_name.strip()
        
        # 2. ÎÇ¥Î∂Ä Í≥µÎ∞±Îì§ÏùÑ Î™®Îëê Ï†úÍ±∞
        normalized = ''.join(normalized.split())
        
        # 3. ÏÜåÎ¨∏Ïûê Î≥ÄÌôò
        normalized = normalized.lower()
        
        # 4. ÌïúÍ∏Ä Ïô∏Ïùò ÌäπÏàòÎ¨∏ÏûêÎäî Ïú†ÏßÄ (Î∏åÎûúÎìúÎ™Ö Îì±Ïóê Ìè¨Ìï®Îê† Ïàò ÏûàÏùå)
        # Îã®, ÏùºÍ¥ÄÏÑ±ÏùÑ ÏúÑÌï¥ ÏùºÎ∂Ä ÌäπÏàòÎ¨∏ÏûêÎäî Ï†ïÎ¶¨
        import re
        # Ïó∞ÏÜçÎêú ÌäπÏàòÎ¨∏ÏûêÎäî ÌïòÎÇòÎ°ú ÌÜµÏùº
        normalized = re.sub(r'[^\wÍ∞Ä-Ìû£]+', '', normalized)
        
        if not normalized:
            logging.debug(f"Ï†ïÍ∑úÌôî ÌõÑ Îπà Î¨∏ÏûêÏó¥: '{product_name}'")
            return ""
        
        # MD5 Ìï¥Ïãú ÏÉùÏÑ±
        import hashlib
        hash_obj = hashlib.md5(normalized.encode('utf-8'))
        hash_result = hash_obj.hexdigest()[:16]
        
        logging.debug(f"Ìï¥Ïãú ÏÉùÏÑ± ÏôÑÎ£å: '{product_name}' -> '{normalized}' -> {hash_result}")
        
        return hash_result
        
    except Exception as e:
        logging.error(f"ÏÉÅÌíàÎ™Ö Ìï¥Ïãú ÏÉùÏÑ± Ïò§Î•ò '{product_name}': {e}")
        return ""

# Î™®Îìà ÌÖåÏä§Ìä∏Ïö© ÏΩîÎìú
if __name__ == "__main__":
    # Í∏∞Î≥∏ Î°úÍπÖ ÏÑ§Ï†ï
    logging.basicConfig(
        level=logging.DEBUG, # Change level to DEBUG for testing
        format='%(asctime)s - %(levelname)s - %(name)s - [%(funcName)s:%(lineno)d] - %(message)s',
        handlers=[logging.StreamHandler()]
    )
    
    # ÏÑ§Ï†ï ÌååÏùº Î°úÎìú
    config = configparser.ConfigParser()
    # Assuming config.ini is in the parent directory of PythonScript
    config_path = Path(__file__).resolve().parent.parent / 'config.ini'
    if not config_path.exists():
        print(f"Error: config.ini not found at {config_path}")
        sys.exit(1)
    config.read(config_path, encoding='utf-8')
    
    # Test data setup needs careful handling of image paths
    # Ensure the image paths used for testing actually exist or simulate them.
    # For this example, we'll assume the paths are placeholders.
    
    # Get image dirs from config
    main_img_dir = Path(config.get('Paths', 'image_main_dir', fallback='C:\\\\RPA\\\\Image\\\\Main'))
    haereum_dir = main_img_dir / 'Haereum'
    kogift_dir = main_img_dir / 'Kogift'
    naver_dir = main_img_dir / 'Naver'

    # Create dummy image files for testing if they don't exist
    # (This part might need adjustment based on your actual test environment)
    dummy_haereum_img = haereum_dir / "haereum_test_product_1_dummy.jpg"
    dummy_kogift_img = kogift_dir / "kogift_test_product_2_dummy.jpg"
    dummy_naver_img = naver_dir / "naver_test_product_3_dummy.jpg"
    
    for d in [haereum_dir, kogift_dir, naver_dir]:
        d.mkdir(parents=True, exist_ok=True)
        
    for img_file in [dummy_haereum_img, dummy_kogift_img, dummy_naver_img]:
        if not img_file.exists():
            try:
                img_file.touch() # Create empty file
                print(f"Created dummy image file: {img_file}")
            except Exception as e:
                print(f"Could not create dummy file {img_file}: {e}")

    test_df = pd.DataFrame({
        'Î≤àÌò∏': [1, 2, 3, 4],
        'ÏÉÅÌíàÎ™Ö': ['ÌÖåÏä§Ìä∏ ÏÉÅÌíà 1', 'Test Product 2', 'Ìï¥Ïò§Î¶Ñ ÌÖåÏä§Ìä∏', 'Ï†Ä Ïú†ÏÇ¨ÎèÑ ÏÉÅÌíà'],
        # Use source URL columns from scraping (example names)
        'Ìï¥Ïò§Î¶ÑÏù¥ÎØ∏ÏßÄURL': ['http://example.com/hae1.jpg', None, 'https://www.jclgift.com/upload/product/simg3/DDAC0001000s.jpg', 'http://example.com/hae4.jpg'],
        'Í≥†Î†§Í∏∞ÌîÑÌä∏ URL': [None, 'https://koreagift.com/ez/upload/mall/shop_1707873892937710_0.jpg', None, 'http://example.com/ko4.jpg'],
        'ÎÑ§Ïù¥Î≤ÑÏù¥ÎØ∏ÏßÄ URL': ['https://shop-phinf.pstatic.net/20240101_1/image.jpg', None, None, 'http://example.com/na4.jpg'],
        'Ïù¥ÎØ∏ÏßÄ_Ïú†ÏÇ¨ÎèÑ': [0.6, 0.8, 0.9, 0.2], # This column should now be ignored by filter_images_by_similarity
        # Add other necessary columns from FINAL_COLUMN_ORDER for the test
        'Íµ¨Î∂Ñ': ['A', 'A', 'P', 'A'], 'Îã¥ÎãπÏûê': ['Test']*4, 'ÏóÖÏ≤¥Î™Ö': ['Test']*4, 'ÏóÖÏ≤¥ÏΩîÎìú': ['123']*4, 'Code': ['T01', 'T02', 'T03', 'T04'], 'Ï§ëÎ∂ÑÎ•òÏπ¥ÌÖåÍ≥†Î¶¨': ['Test']*4,
        'Í∏∞Î≥∏ÏàòÎüâ(1)': [100]*4, 'ÌåêÎß§Îã®Í∞Ä(VÌè¨Ìï®)': [1000]*4, 'Î≥∏ÏÇ¨ÏÉÅÌíàÎßÅÌÅ¨': ['http://example.com/1']*4,
        'Í∏∞Î≥∏ÏàòÎüâ(2)': [100]*4, 'ÌåêÎß§Í∞Ä(VÌè¨Ìï®)(2)': [1100]*4, 'Í∞ÄÍ≤©Ï∞®Ïù¥(2)': [100]*4, 'Í∞ÄÍ≤©Ï∞®Ïù¥(2)(%)': [10]*4, 'Í≥†Î†§Í∏∞ÌîÑÌä∏ ÏÉÅÌíàÎßÅÌÅ¨': ['http://example.com/2']*4,
        'Í∏∞Î≥∏ÏàòÎüâ(3)': [100]*4, 'ÌåêÎß§Îã®Í∞Ä(VÌè¨Ìï®)(3)': [900]*4, 'Í∞ÄÍ≤©Ï∞®Ïù¥(3)': [-100]*4, 'Í∞ÄÍ≤©Ï∞®Ïù¥(3)(%)': [-10]*4, 'Í≥µÍ∏âÏÇ¨Î™Ö': ['Test']*4, 'ÎÑ§Ïù¥Î≤Ñ ÏáºÌïë ÎßÅÌÅ¨': ['http://example.com/3']*4, 'Í≥µÍ∏âÏÇ¨ ÏÉÅÌíàÎßÅÌÅ¨': ['http://example.com/supplier']*4
    })
    
    # --- Simulate adding image dicts (as would be done by integrate_images) ---
    # This is crucial for testing filter_images_by_similarity correctly
    # We manually add the 'score' key here based on example values
    test_df['Ìï¥Ïò§Î¶Ñ(Ïù¥ÎØ∏ÏßÄÎßÅÌÅ¨)'] = [
        {'local_path': str(dummy_haereum_img), 'url': 'http://example.com/hae1.jpg', 'source': 'haereum', 'score': 0.85},
        None,
        {'local_path': str(dummy_haereum_img), 'url': 'https://www.jclgift.com/upload/product/simg3/DDAC0001000s.jpg', 'source': 'haereum', 'score': 0.95},
         {'local_path': str(dummy_haereum_img), 'url': 'http://example.com/hae4.jpg', 'source': 'haereum', 'score': 0.90} # High score, should not be filtered
    ]
    test_df['Í≥†Î†§Í∏∞ÌîÑÌä∏(Ïù¥ÎØ∏ÏßÄÎßÅÌÅ¨)'] = [
        None,
        {'local_path': str(dummy_kogift_img), 'url': 'https://koreagift.com/ez/upload/mall/shop_1707873892937710_0.jpg', 'source': 'kogift', 'score': 0.75},
        None,
        {'local_path': str(dummy_kogift_img), 'url': 'http://example.com/ko4.jpg', 'source': 'kogift', 'score': 0.25} # Low score, should be filtered
    ]
    test_df['ÎÑ§Ïù¥Î≤ÑÏáºÌïë(Ïù¥ÎØ∏ÏßÄÎßÅÌÅ¨)'] = [
        {'local_path': str(dummy_naver_img), 'url': 'https://shop-phinf.pstatic.net/20240101_1/image.jpg', 'source': 'naver', 'score': 0.65},
        None,
        None,
        {'local_path': str(dummy_naver_img), 'url': 'http://example.com/na4.jpg', 'source': 'naver', 'score': 0.15} # Low score, should be filtered
    ]
    
    # --- Run only the filtering part for isolated testing ---
    logging.info("--- Testing filter_images_by_similarity ---")
    filtered_df = filter_images_by_similarity(test_df.copy(), config) # Use copy
    
    logging.info(f"Test filter results - DataFrame shape: {filtered_df.shape}")
    logging.info(f"Ìï¥Ïò§Î¶Ñ(Ïù¥ÎØ∏ÏßÄÎßÅÌÅ¨) after filter: {filtered_df['Ìï¥Ïò§Î¶Ñ(Ïù¥ÎØ∏ÏßÄÎßÅÌÅ¨)'].tolist()}")
    logging.info(f"Í≥†Î†§Í∏∞ÌîÑÌä∏(Ïù¥ÎØ∏ÏßÄÎßÅÌÅ¨) after filter: {filtered_df['Í≥†Î†§Í∏∞ÌîÑÌä∏(Ïù¥ÎØ∏ÏßÄÎßÅÌÅ¨)'].tolist()}")
    logging.info(f"ÎÑ§Ïù¥Î≤ÑÏáºÌïë(Ïù¥ÎØ∏ÏßÄÎßÅÌÅ¨) after filter: {filtered_df['ÎÑ§Ïù¥Î≤ÑÏáºÌïë(Ïù¥ÎØ∏ÏßÄÎßÅÌÅ¨)'].tolist()}")
    
    # --- Run the full integrate_and_filter process ---
    logging.info("--- Testing integrate_and_filter_images ---")
    # Use a fresh copy for the full test
    full_result_df = integrate_and_filter_images(test_df.copy(), config, save_excel_output=True) 
    
    # Í≤∞Í≥º Ï∂úÎ†• (using the new final column names)
    logging.info(f"Full process result - DataFrame shape: {full_result_df.shape}")
    logging.info(f"Ìï¥Ïò§Î¶Ñ(Ïù¥ÎØ∏ÏßÄÎßÅÌÅ¨) final data: {full_result_df['Ìï¥Ïò§Î¶Ñ(Ïù¥ÎØ∏ÏßÄÎßÅÌÅ¨)'].tolist()}")
    logging.info(f"Í≥†Î†§Í∏∞ÌîÑÌä∏(Ïù¥ÎØ∏ÏßÄÎßÅÌÅ¨) final data: {full_result_df['Í≥†Î†§Í∏∞ÌîÑÌä∏(Ïù¥ÎØ∏ÏßÄÎßÅÌÅ¨)'].tolist()}")
    logging.info(f"ÎÑ§Ïù¥Î≤ÑÏáºÌïë(Ïù¥ÎØ∏ÏßÄÎßÅÌÅ¨) final data: {full_result_df['ÎÑ§Ïù¥Î≤ÑÏáºÌïë(Ïù¥ÎØ∏ÏßÄÎßÅÌÅ¨)'].tolist()}") 


def get_system_status_summary(config: configparser.ConfigParser = None) -> Dict:
    """
    Í∞úÏÑ†Îêú Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠ ÏãúÏä§ÌÖúÏùò ÌòÑÏû¨ ÏÉÅÌÉúÎ•º ÏöîÏïΩÌï©ÎãàÎã§.
    
    Returns:
        ÏãúÏä§ÌÖú ÏÉÅÌÉú Ï†ïÎ≥¥Í∞Ä Îã¥Í∏¥ ÎîïÏÖîÎÑàÎ¶¨
    """
    
    try:
        import psutil
        import platform
        from datetime import datetime
        
        status = {
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'system_info': {
                'platform': platform.platform(),
                'python_version': platform.python_version(),
                'cpu_count': psutil.cpu_count(),
                'memory_total_gb': round(psutil.virtual_memory().total / (1024**3), 2),
                'memory_available_gb': round(psutil.virtual_memory().available / (1024**3), 2)
            },
            'matching_system': {
                'version': 'Í∞úÏÑ†Îêú Ìï¥Ïãú Í∏∞Î∞ò 2Îã®Í≥Ñ Îß§Ïπ≠',
                'hash_algorithm': 'MD5 (16ÏûêÎ¶¨)',
                'image_similarity_threshold': 0.8,
                'enhanced_matcher_available': False,
                'gpu_enabled': False
            },
            'performance_metrics': {
                'expected_hash_match_rate': '95%+ (ÎèôÏùº ÏÉÅÌíà)',
                'expected_processing_speed': '~80% Ìñ•ÏÉÅ',
                'memory_usage_reduction': '~60% Ï†àÏïΩ',
                'cache_enabled': True
            },
            'improvements': [
                '‚úÖ Ìï¥Ïãú Í∏∞Î∞ò 1Ï∞® Ï†ïÌôïÌïú Îß§Ïπ≠',
                '‚úÖ Ïù¥ÎØ∏ÏßÄ Ïú†ÏÇ¨ÎèÑ 2Ï∞® Í≤ÄÏ¶ù',
                '‚úÖ ÎûúÎç§ Ìï†Îãπ Ï†úÍ±∞',
                '‚úÖ Ï§ëÎ≥µ Ï≤òÎ¶¨ Î∞©ÏßÄ',
                '‚úÖ Î™ÖÌôïÌïú Î°úÍ∑∏ Î∞è ÏÉÅÌÉú ÌëúÏãú',
                '‚úÖ ÏÑ±Îä• ÏµúÏ†ÅÌôî'
            ]
        }
        
        # Enhanced Image Matcher ÏÉÅÌÉú ÌôïÏù∏
        try:
            from enhanced_image_matcher import EnhancedImageMatcher
            enhanced_matcher = EnhancedImageMatcher(config)
            status['matching_system']['enhanced_matcher_available'] = True
            status['matching_system']['gpu_enabled'] = getattr(enhanced_matcher, 'use_gpu', False)
            
            # GPU Ï†ïÎ≥¥ Ï∂îÍ∞Ä
            if status['matching_system']['gpu_enabled']:
                try:
                    import tensorflow as tf
                    gpus = tf.config.list_physical_devices('GPU')
                    status['gpu_info'] = {
                        'gpu_count': len(gpus),
                        'gpu_devices': [str(gpu) for gpu in gpus] if gpus else []
                    }
                except Exception:
                    status['gpu_info'] = {'error': 'GPU Ï†ïÎ≥¥ Ï°∞Ìöå Ïã§Ìå®'}
        except Exception as e:
            status['matching_system']['enhanced_matcher_error'] = str(e)
        
        # ÎîîÎ†âÌÜ†Î¶¨ ÏÉÅÌÉú ÌôïÏù∏
        if config:
            try:
                main_img_dir = Path(config.get('Paths', 'image_main_dir', fallback='C:\\RPA\\Image\\Main'))
                directories = {
                    'haereum': main_img_dir / 'Haereum',
                    'kogift': main_img_dir / 'Kogift', 
                    'naver': main_img_dir / 'Naver'
                }
                
                dir_status = {}
                for name, path in directories.items():
                    if path.exists():
                        image_files = list(path.glob('*.jpg')) + list(path.glob('*.png'))
                        dir_status[name] = {
                            'exists': True,
                            'image_count': len(image_files),
                            'path': str(path)
                        }
                    else:
                        dir_status[name] = {
                            'exists': False,
                            'path': str(path)
                        }
                
                status['image_directories'] = dir_status
            except Exception as e:
                status['image_directories'] = {'error': str(e)}
        
        return status
        
    except Exception as e:
        return {
            'error': f"ÏãúÏä§ÌÖú ÏÉÅÌÉú Ï°∞Ìöå Ïò§Î•ò: {e}",
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }


def print_system_status(config: configparser.ConfigParser = None):
    """ÏãúÏä§ÌÖú ÏÉÅÌÉúÎ•º ÏΩòÏÜîÏóê Ï∂úÎ†•Ìï©ÎãàÎã§."""
    
    status = get_system_status_summary(config)
    
    print("\n" + "="*60)
    print("üöÄ Í∞úÏÑ†Îêú Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠ ÏãúÏä§ÌÖú ÏÉÅÌÉú")
    print("="*60)
    
    if 'error' in status:
        print(f"‚ùå Ïò§Î•ò: {status['error']}")
        return
    
    print(f"üìÖ Ï°∞Ìöå ÏãúÍ∞Ñ: {status['timestamp']}")
    
    # ÏãúÏä§ÌÖú Ï†ïÎ≥¥
    sys_info = status['system_info']
    print(f"\nüíª ÏãúÏä§ÌÖú Ï†ïÎ≥¥:")
    print(f"   OS: {sys_info['platform']}")
    print(f"   Python: {sys_info['python_version']}")
    print(f"   CPU: {sys_info['cpu_count']}ÏΩîÏñ¥")
    print(f"   Î©îÎ™®Î¶¨: {sys_info['memory_available_gb']:.1f}GB / {sys_info['memory_total_gb']:.1f}GB")
    
    # Îß§Ïπ≠ ÏãúÏä§ÌÖú
    match_sys = status['matching_system']
    print(f"\nüéØ Îß§Ïπ≠ ÏãúÏä§ÌÖú:")
    print(f"   Î≤ÑÏ†Ñ: {match_sys['version']}")
    print(f"   Ìï¥Ïãú ÏïåÍ≥†Î¶¨Ï¶ò: {match_sys['hash_algorithm']}")
    print(f"   Ïú†ÏÇ¨ÎèÑ ÏûÑÍ≥ÑÍ∞í: {match_sys['image_similarity_threshold']}")
    print(f"   Í≥†Í∏â Îß§Ï≤ò: {'‚úÖ ÏÇ¨Ïö© Í∞ÄÎä•' if match_sys['enhanced_matcher_available'] else '‚ùå ÏÇ¨Ïö© Î∂àÍ∞Ä'}")
    print(f"   GPU Í∞ÄÏÜç: {'‚úÖ ÌôúÏÑ±Ìôî' if match_sys['gpu_enabled'] else '‚ùå ÎπÑÌôúÏÑ±Ìôî'}")
    
    # GPU Ï†ïÎ≥¥
    if 'gpu_info' in status:
        gpu_info = status['gpu_info']
        if 'error' not in gpu_info:
            print(f"   GPU Í∞úÏàò: {gpu_info['gpu_count']}Í∞ú")
            for i, gpu in enumerate(gpu_info['gpu_devices']):
                print(f"     GPU {i}: {gpu}")
    
    # ÏÑ±Îä• Î©îÌä∏Î¶≠
    perf = status['performance_metrics']
    print(f"\nüìä ÏÑ±Îä• Î©îÌä∏Î¶≠:")
    print(f"   ÏòàÏÉÅ Ìï¥Ïãú Îß§Ïπ≠Î•†: {perf['expected_hash_match_rate']}")
    print(f"   Ï≤òÎ¶¨ ÏÜçÎèÑ Í∞úÏÑ†: {perf['expected_processing_speed']}")
    print(f"   Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ Ï†àÏïΩ: {perf['memory_usage_reduction']}")
    print(f"   Ï∫êÏãú ÏÇ¨Ïö©: {'‚úÖ' if perf['cache_enabled'] else '‚ùå'}")
    
    # Ïù¥ÎØ∏ÏßÄ ÎîîÎ†âÌÜ†Î¶¨ ÏÉÅÌÉú
    if 'image_directories' in status and 'error' not in status['image_directories']:
        print(f"\nüìÅ Ïù¥ÎØ∏ÏßÄ ÎîîÎ†âÌÜ†Î¶¨:")
        for name, info in status['image_directories'].items():
            if info['exists']:
                print(f"   {name}: ‚úÖ {info['image_count']}Í∞ú Ïù¥ÎØ∏ÏßÄ")
            else:
                print(f"   {name}: ‚ùå ÎîîÎ†âÌÜ†Î¶¨ ÏóÜÏùå")
    
    # Í∞úÏÑ†ÏÇ¨Ìï≠
    print(f"\nüéâ Ï£ºÏöî Í∞úÏÑ†ÏÇ¨Ìï≠:")
    for improvement in status['improvements']:
        print(f"   {improvement}")
    
    print("\n" + "="*60)