import os
import logging
import pandas as pd
from pathlib import Path
import configparser
from typing import Dict, Any, Optional, List, Tuple, Set, Union
from openpyxl import Workbook
from openpyxl.drawing.image import Image
from openpyxl.utils import get_column_letter
import shutil
import sys
import re
import hashlib
from datetime import datetime
import glob
import time
import json
import numpy as np
import tqdm
import traceback
import openpyxl
import PIL

# Add the parent directory to sys.path to allow imports from PythonScript
current_dir = Path(__file__).resolve().parent
parent_dir = current_dir.parent
if str(parent_dir) not in sys.path:
    sys.path.insert(0, str(parent_dir))

# Import common utilities first
try:
    from .utils import generate_product_name_hash, extract_product_hash_from_filename
    from .tokenize_product_names import tokenize_product_name, extract_meaningful_keywords
    logging.info("âœ… ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ì„ ì„±ê³µì ìœ¼ë¡œ importí–ˆìŠµë‹ˆë‹¤.")
except ImportError:
    try:
        from utils import generate_product_name_hash, extract_product_hash_from_filename
        from tokenize_product_names import tokenize_product_name, extract_meaningful_keywords
        logging.info("âœ… ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ì„ ì§ì ‘ importí–ˆìŠµë‹ˆë‹¤.")
    except ImportError as e:
        logging.error(f"âŒ ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ import ì‹¤íŒ¨: {e}")
        # Fallback implementations
        def generate_product_name_hash(product_name: str) -> str:
            """Fallback hash generation"""
            try:
                normalized = ''.join(product_name.split()).lower()
                return hashlib.md5(normalized.encode('utf-8')).hexdigest()[:16]
            except Exception:
                return ""
        
        def extract_product_hash_from_filename(filename: str) -> Optional[str]:
            """Fallback hash extraction"""
            try:
                name = os.path.splitext(os.path.basename(filename))[0]
                parts = name.split('_')
                if len(parts) >= 2 and len(parts[1]) == 16:
                    return parts[1].lower()
                return None
            except Exception:
                return None

# Initialize logger
logger = logging.getLogger(__name__)

# Import enhanced image matcher with improved error handling
try:
    from .enhanced_image_matcher import EnhancedImageMatcher, check_gpu_status
    ENHANCED_MATCHER_AVAILABLE = True
    logging.info("âœ… ê³ ê¸‰ ì´ë¯¸ì§€ ë§¤ì²˜ë¥¼ ì„±ê³µì ìœ¼ë¡œ importí–ˆìŠµë‹ˆë‹¤.")
except ImportError:
    try:
        from enhanced_image_matcher import EnhancedImageMatcher, check_gpu_status
        ENHANCED_MATCHER_AVAILABLE = True
        logging.info("âœ… ê³ ê¸‰ ì´ë¯¸ì§€ ë§¤ì²˜ë¥¼ ì§ì ‘ importí–ˆìŠµë‹ˆë‹¤.")
    except ImportError:
        ENHANCED_MATCHER_AVAILABLE = False
        logging.warning("âš ï¸ ê³ ê¸‰ ì´ë¯¸ì§€ ë§¤ì²˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ ë§¤ì¹­ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

def prepare_image_metadata(image_dir: Path, prefix: str, prefer_original: bool = True, prefer_jpg: bool = True) -> Dict[str, Dict]:
    """
    ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ì—ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•´ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        image_dir: ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        prefix: ì´ë¯¸ì§€ ì†ŒìŠ¤ êµ¬ë¶„ìš© ì ‘ë‘ì‚¬ (ì˜ˆ: 'haereum', 'kogift', 'naver')
        prefer_original: _nobgê°€ ì•„ë‹Œ ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ìš°ì„ ì‹œí• ì§€ ì—¬ë¶€
        prefer_jpg: PNGë³´ë‹¤ JPG íŒŒì¼ì„ ìš°ì„ ì‹œí• ì§€ ì—¬ë¶€
        
    Returns:
        ì´ë¯¸ì§€ ê²½ë¡œë¥¼ í‚¤ë¡œ, ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„°ë¥¼ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
    """
    image_info = {}
    
    # Normalize path to use forward slashes
    abs_image_dir = os.path.abspath(str(image_dir)).replace('\\', '/')
    logging.info(f"Preparing image metadata from directory: {abs_image_dir} (prefix: {prefix}, prefer_original: {prefer_original}, prefer_jpg: {prefer_jpg})")
    
    # Handle case where directory doesn't exist
    if not os.path.exists(abs_image_dir):
        logging.warning(f"Image directory does not exist: {abs_image_dir}")
        return {}
    
    # First look for image files in the directory
    all_image_files = []
    valid_extensions = ('.jpg', '.jpeg', '.png', '.gif')
    
    try:
        # Get all image files with normalized paths
        for root, _, files in os.walk(abs_image_dir):
            root = root.replace('\\', '/')  # Normalize path
            for file in files:
                if file.lower().endswith(valid_extensions):
                    full_path = os.path.join(root, file).replace('\\', '/')
                    all_image_files.append(full_path)
        
        logging.info(f"Found {len(all_image_files)} total images in {abs_image_dir}")
        
        # Group images by base name (without _nobg suffix)
        image_groups = {}
        
        for img_path in all_image_files:
            filename = os.path.basename(img_path)
            file_root, file_ext = os.path.splitext(filename)
            
            # Handle _nobg suffix
            base_name = file_root
            is_nobg = False
            if file_root.endswith('_nobg'):
                base_name = file_root[:-5]  # Remove _nobg suffix
                is_nobg = True
                
            # Group by base name
            if base_name not in image_groups:
                image_groups[base_name] = {'original_jpg': None, 'original_png': None, 'nobg_png': None, 'nobg_jpg': None}
                
            # Store the path based on type and extension
            if is_nobg:
                if file_ext.lower() in ['.jpg', '.jpeg']:
                    image_groups[base_name]['nobg_jpg'] = img_path
                else:
                    image_groups[base_name]['nobg_png'] = img_path
            else:
                if file_ext.lower() in ['.jpg', '.jpeg']:
                    image_groups[base_name]['original_jpg'] = img_path
                else:
                    image_groups[base_name]['original_png'] = img_path
        
        # Process each group and prioritize files according to preferences
        for base_name, paths in image_groups.items():
            try:
                # Select the best path based on preferences
                img_path = None
                
                # Priority order based on preferences:
                if prefer_original and prefer_jpg:
                    # 1. Original JPG
                    # 2. Original PNG
                    # 3. Nobg JPG
                    # 4. Nobg PNG
                    if paths['original_jpg']:
                        img_path = paths['original_jpg']
                    elif paths['original_png']:
                        img_path = paths['original_png']
                    elif paths['nobg_jpg']:
                        img_path = paths['nobg_jpg']
                    elif paths['nobg_png']:
                        img_path = paths['nobg_png']
                elif prefer_original and not prefer_jpg:
                    # 1. Original PNG
                    # 2. Original JPG
                    # 3. Nobg PNG
                    # 4. Nobg JPG
                    if paths['original_png']:
                        img_path = paths['original_png']
                    elif paths['original_jpg']:
                        img_path = paths['original_jpg']
                    elif paths['nobg_png']:
                        img_path = paths['nobg_png']
                    elif paths['nobg_jpg']:
                        img_path = paths['nobg_jpg']
                elif not prefer_original and prefer_jpg:
                    # 1. Nobg JPG
                    # 2. Original JPG
                    # 3. Nobg PNG
                    # 4. Original PNG
                    if paths['nobg_jpg']:
                        img_path = paths['nobg_jpg']
                    elif paths['original_jpg']:
                        img_path = paths['original_jpg']
                    elif paths['nobg_png']:
                        img_path = paths['nobg_png']
                    elif paths['original_png']:
                        img_path = paths['original_png']
                else:
                    # not prefer_original and not prefer_jpg
                    # 1. Nobg PNG
                    # 2. Original PNG
                    # 3. Nobg JPG
                    # 4. Original JPG
                    if paths['nobg_png']:
                        img_path = paths['nobg_png']
                    elif paths['original_png']:
                        img_path = paths['original_png']
                    elif paths['nobg_jpg']:
                        img_path = paths['nobg_jpg']
                    elif paths['original_jpg']:
                        img_path = paths['original_jpg']
                
                # Skip if no image found
                if not img_path:
                    continue
                
                # Always store all available paths for reference
                original_jpg_path = paths['original_jpg']
                original_png_path = paths['original_png']
                nobg_png_path = paths['nobg_png']
                nobg_jpg_path = paths['nobg_jpg']
                
                # Extract metadata
                filename = os.path.basename(img_path)
                file_root, file_ext = os.path.splitext(filename)
                
                # Extract product hash from filename
                product_hash = extract_product_hash_from_filename(filename)
                if product_hash:
                    logging.debug(f"Extracted hash '{product_hash}' from filename '{filename}'")
                
                # Create metadata dictionary
                metadata = {
                    'path': img_path,
                    'filename': filename,
                    'extension': file_ext.lower(),
                    'is_original': not file_root.endswith('_nobg'),
                    'original_jpg_path': original_jpg_path,
                    'original_png_path': original_png_path,
                    'nobg_jpg_path': nobg_jpg_path,
                    'nobg_png_path': nobg_png_path,
                    'source': prefix.rstrip('_'),
                    'base_name': base_name,
                    'product_hash': product_hash  # Add product hash to metadata
                }
                
                # Store metadata in image_info dictionary
                image_info[img_path] = metadata
                
                # Debug some sample entries
                if len(image_info) <= 2 or len(image_info) % 50 == 0:
                    logging.debug(f"Image metadata sample: {img_path} -> {metadata}")
            
            except Exception as e:
                import traceback
                stack_trace = traceback.format_exc()
                logging.error(f"Error processing image file {base_name}: {e}")
                logging.debug(f"Exception traceback: {stack_trace}")
                continue  # Skip this image but continue processing others
        
        # Log summary
        logging.info(f"Processed {len(image_info)} {prefix} images")
        
        # Additional debug information
        if image_info:
            logging.debug(f"First 3 image keys in {prefix} image_info: {list(image_info.keys())[:3]}")
        else:
            logging.warning(f"No images were successfully processed for {prefix}")
        
        return image_info
        
    except Exception as e:
        import traceback
        stack_trace = traceback.format_exc()
        logging.error(f"Error preparing image metadata from {abs_image_dir}: {e}")
        logging.debug(f"Exception traceback: {stack_trace}")
        return {}

def calculate_similarity(product_tokens: List[str], image_tokens: List[str]) -> float:
    """
    ìƒí’ˆëª…ê³¼ ì´ë¯¸ì§€ ì´ë¦„ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        product_tokens: ìƒí’ˆëª…ì—ì„œ ì¶”ì¶œí•œ í† í° ëª©ë¡
        image_tokens: ì´ë¯¸ì§€ ì´ë¦„ì—ì„œ ì¶”ì¶œí•œ í† í° ëª©ë¡
        
    Returns:
        ìœ ì‚¬ë„ ì ìˆ˜ (0.0 ~ 1.0)
    """
    # í† í° ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°
    common_tokens = set(product_tokens) & set(image_tokens)
    
    # ë” ì •í™•í•œ ìœ ì‚¬ë„ ê³„ì‚° - í† í°ì˜ ê¸¸ì´ì™€ ìˆ˜ë¥¼ ê³ ë ¤
    total_tokens = len(set(product_tokens) | set(image_tokens))
    if total_tokens == 0:
        return 0.0
        
    similarity = len(common_tokens) / total_tokens
    
    # ë” ê¸´ í† í°ì´ ë§¤ì¹­ë˜ë©´ ê°€ì¤‘ì¹˜ ë¶€ì—¬
    weight = 1.0
    for token in common_tokens:
        if len(token) >= 4:  # 4ê¸€ì ì´ìƒ í† í°ì— ê°€ì¤‘ì¹˜
            weight += 0.1
    
    return min(similarity * weight, 1.0) # Ensure score doesn't exceed 1.0

def find_best_image_matches(product_names: List[str], 
                           haereum_images: Dict[str, Dict], 
                           kogift_images: Dict[str, Dict], 
                           naver_images: Dict[str, Dict],
                           similarity_threshold: float = 0.8,  # ë” ì—„ê²©í•œ ì„ê³„ê°’ìœ¼ë¡œ ë³€ê²½
                           config: Optional[configparser.ConfigParser] = None,
                           df: Optional[pd.DataFrame] = None) -> List[Tuple[Optional[str], Optional[str], Optional[str]]]:
    """
    ê°œì„ ëœ 2ë‹¨ê³„ ìƒí’ˆ ì´ë¯¸ì§€ ë§¤ì¹­ ì‹œìŠ¤í…œ
    
    ë‹¨ê³„ 1: í•´ì‹œ ê¸°ë°˜ ì •í™•í•œ ë§¤ì¹­ (MD5 í•´ì‹œ ë¹„êµ)
    ë‹¨ê³„ 2: ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê²€ì¦ (0.8 ì„ê³„ê°’)
    
    Args:
        product_names: ë§¤ì¹­í•  ìƒí’ˆëª… ë¦¬ìŠ¤íŠ¸
        haereum_images: í•´ì˜¤ë¦„ ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        kogift_images: ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        naver_images: ë„¤ì´ë²„ ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        similarity_threshold: ì´ë¯¸ì§€ ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸: 0.8)
        config: ì„¤ì • ê°ì²´
        df: ìƒí’ˆ ì •ë³´ DataFrame
        
    Returns:
        ê° ìƒí’ˆì— ëŒ€í•œ (haereum_match, kogift_match, naver_match) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
    """
    
    logging.info("ğŸš€ ê°œì„ ëœ 2ë‹¨ê³„ ë§¤ì¹­ ì‹œìŠ¤í…œ ì‹œì‘")
    logging.info(f"ğŸ“Š ì…ë ¥ ë°ì´í„°: ìƒí’ˆ {len(product_names)}ê°œ, í•´ì˜¤ë¦„ {len(haereum_images)}ê°œ, "
                f"ê³ ë ¤ê¸°í”„íŠ¸ {len(kogift_images)}ê°œ, ë„¤ì´ë²„ {len(naver_images)}ê°œ")
    
    # ì„¤ì •ê°’ ë¡œë“œ
    if config:
        try:
            similarity_threshold = config.getfloat('ImageMatching', 'similarity_threshold', fallback=similarity_threshold)
        except (configparser.Error, ValueError):
            logging.warning(f"ì„¤ì •ì—ì„œ similarity_thresholdë¥¼ ì½ì„ ìˆ˜ ì—†ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©: {similarity_threshold}")
    
    # ê³ ê¸‰ ì´ë¯¸ì§€ ë§¤ì²˜ ì´ˆê¸°í™”
    enhanced_matcher = None
    try:
        from enhanced_image_matcher import EnhancedImageMatcher
        enhanced_matcher = EnhancedImageMatcher(config)
        use_gpu = getattr(enhanced_matcher, 'use_gpu', False)
        logging.info(f"âœ… ê³ ê¸‰ ì´ë¯¸ì§€ ë§¤ì²˜ ì´ˆê¸°í™” ì™„ë£Œ (GPU: {use_gpu})")
    except Exception as e:
        logging.error(f"âŒ ê³ ê¸‰ ì´ë¯¸ì§€ ë§¤ì²˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        logging.warning("ê¸°ë³¸ ë§¤ì¹­ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    # ë§¤ì¹­ ê²°ê³¼ ë° ì‚¬ìš©ëœ ì´ë¯¸ì§€ ì¶”ì 
    best_matches = []
    used_haereum = set()
    used_kogift = set()
    used_naver = set()
    
    # í†µê³„ ë³€ìˆ˜
    hash_matches = 0
    image_verified = 0
    no_matches = 0
    
    # ê° ìƒí’ˆì— ëŒ€í•´ ë§¤ì¹­ ìˆ˜í–‰
    for idx, product_name in enumerate(product_names):
        if (idx + 1) % 10 == 0:
            logging.info(f"ì§„í–‰ ìƒí™©: {idx + 1}/{len(product_names)} ì²˜ë¦¬ ì¤‘...")
        
        logging.debug(f"\nğŸ“¦ ìƒí’ˆ '{product_name}' ë§¤ì¹­ ì‹œì‘")
        
        # === ë‹¨ê³„ 1: í•´ì‹œ ê¸°ë°˜ ì •í™•í•œ ë§¤ì¹­ ===
        product_hash = generate_product_name_hash(product_name)
        hash_candidates = {
            'haereum': [],
            'kogift': [],
            'naver': []
        }
        
        if product_hash:
            logging.debug(f"ğŸ”‘ ìƒì„±ëœ í•´ì‹œ: {product_hash}")
            
            # ê° ì†ŒìŠ¤ì—ì„œ í•´ì‹œ ë§¤ì¹­ í›„ë³´ ì°¾ê¸°
            for h_path, h_info in haereum_images.items():
                if h_path not in used_haereum:
                    img_hash = h_info.get('product_hash')
                    if img_hash and img_hash == product_hash:
                        hash_candidates['haereum'].append((h_path, h_info))
            
            for k_path, k_info in kogift_images.items():
                if k_path not in used_kogift:
                    img_hash = k_info.get('product_hash')
                    if img_hash and img_hash == product_hash:
                        hash_candidates['kogift'].append((k_path, k_info))
            
            for n_path, n_info in naver_images.items():
                if n_path not in used_naver:
                    img_hash = n_info.get('product_hash')
                    if img_hash and img_hash == product_hash:
                        hash_candidates['naver'].append((n_path, n_info))
            
            total_hash_candidates = (len(hash_candidates['haereum']) + 
                                   len(hash_candidates['kogift']) + 
                                   len(hash_candidates['naver']))
            
            logging.debug(f"ğŸ¯ í•´ì‹œ ë§¤ì¹­ í›„ë³´: í•´ì˜¤ë¦„ {len(hash_candidates['haereum'])}ê°œ, "
                         f"ê³ ë ¤ê¸°í”„íŠ¸ {len(hash_candidates['kogift'])}ê°œ, ë„¤ì´ë²„ {len(hash_candidates['naver'])}ê°œ")
            
            # === ë‹¨ê³„ 2: ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê²€ì¦ (í•´ì‹œ ë§¤ì¹­ í›„ë³´ê°€ ìˆì„ ë•Œë§Œ) ===
            final_matches = {'haereum': None, 'kogift': None, 'naver': None}
            
            if total_hash_candidates > 0:
                # enhanced_matcherê°€ ì—†ì–´ë„ í•´ì‹œ ë§¤ì¹­ì€ ìˆ˜í–‰
                if enhanced_matcher:
                    logging.debug(f"ğŸ” ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê²€ì¦ ì‹œì‘ (ì„ê³„ê°’: {similarity_threshold})")
                else:
                    logging.debug(f"ğŸ” Enhanced matcher ì—†ìŒ - í•´ì‹œ ë§¤ì¹­ë§Œìœ¼ë¡œ ì§„í–‰")
                
                # ê¸°ì¤€ ì´ë¯¸ì§€ ì„ íƒ (í•´ì˜¤ë¦„ > ê³ ë ¤ê¸°í”„íŠ¸ > ë„¤ì´ë²„ ìˆœ)
                reference_path = None
                reference_source = None
                
                if hash_candidates['haereum']:
                    ref_path, ref_info = hash_candidates['haereum'][0]
                    reference_path = ref_info.get('path', ref_path)
                    reference_source = 'haereum'
                elif hash_candidates['kogift']:
                    ref_path, ref_info = hash_candidates['kogift'][0]
                    reference_path = ref_info.get('path', ref_path)
                    reference_source = 'kogift'
                elif hash_candidates['naver']:
                    ref_path, ref_info = hash_candidates['naver'][0]
                    reference_path = ref_info.get('path', ref_path)
                    reference_source = 'naver'
                
                # Enhanced matcherê°€ ì—†ìœ¼ë©´ í•´ì‹œ ë§¤ì¹­ë§Œìœ¼ë¡œ í™•ì •
                if not enhanced_matcher:
                    # í•´ì‹œê°€ ì¼ì¹˜í•˜ëŠ” ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ë§¤ì¹­ìœ¼ë¡œ í™•ì •
                    for source in ['haereum', 'kogift', 'naver']:
                        if hash_candidates[source]:
                            path, info = hash_candidates[source][0]
                            final_matches[source] = (path, info)
                            if source == 'haereum':
                                used_haereum.add(path)
                            elif source == 'kogift':
                                used_kogift.add(path)
                            elif source == 'naver':
                                used_naver.add(path)
                            logging.info(f"âœ… {source} í•´ì‹œ ë§¤ì¹­ ì„±ê³µ: {os.path.basename(path)}")
                
                elif reference_path and os.path.exists(reference_path):
                    logging.debug(f"ğŸ“ ê¸°ì¤€ ì´ë¯¸ì§€: {reference_source} - {os.path.basename(reference_path)}")
                    
                    # ê¸°ì¤€ ì´ë¯¸ì§€ì˜ ë§¤ì¹­ í™•ì •
                    if reference_source == 'haereum':
                        final_matches['haereum'] = hash_candidates['haereum'][0]
                        used_haereum.add(hash_candidates['haereum'][0][0])
                    elif reference_source == 'kogift':
                        final_matches['kogift'] = hash_candidates['kogift'][0]
                        used_kogift.add(hash_candidates['kogift'][0][0])
                    elif reference_source == 'naver':
                        final_matches['naver'] = hash_candidates['naver'][0]
                        used_naver.add(hash_candidates['naver'][0][0])
                    
                    # ë‹¤ë¥¸ ì†ŒìŠ¤ë“¤ê³¼ ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê²€ì¦
                    for source, candidates in hash_candidates.items():
                        if source == reference_source or not candidates:
                            continue
                        
                        for candidate_path, candidate_info in candidates:
                            candidate_img_path = candidate_info.get('path', candidate_path)
                            
                            if os.path.exists(candidate_img_path):
                                try:
                                    similarity = enhanced_matcher.calculate_similarity(reference_path, candidate_img_path)
                                    logging.debug(f"ğŸ” ìœ ì‚¬ë„ ê²€ì‚¬: {reference_source} vs {source} = {similarity:.3f}")
                                    
                                    if similarity >= similarity_threshold:
                                        final_matches[source] = (candidate_path, candidate_info)
                                        if source == 'haereum':
                                            used_haereum.add(candidate_path)
                                        elif source == 'kogift':
                                            used_kogift.add(candidate_path)
                                        elif source == 'naver':
                                            used_naver.add(candidate_path)
                                        
                                        logging.info(f"âœ… {source} ë§¤ì¹­ ì„±ê³µ: {os.path.basename(candidate_path)} (ìœ ì‚¬ë„: {similarity:.3f})")
                                        break
                                    else:
                                        logging.debug(f"âŒ ìœ ì‚¬ë„ ë¶€ì¡±: {source} {similarity:.3f} < {similarity_threshold}")
                                        
                                except Exception as e:
                                    logging.error(f"ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
                            else:
                                logging.warning(f"ì´ë¯¸ì§€ íŒŒì¼ ì—†ìŒ: {candidate_img_path}")
                
                # ë§¤ì¹­ ê²°ê³¼ ì •ë¦¬
                if any(final_matches.values()):
                    hash_matches += 1
                    if total_hash_candidates > 1:  # 2ê°œ ì´ìƒ ì†ŒìŠ¤ì—ì„œ í•´ì‹œ ë§¤ì¹­ëœ ê²½ìš°
                        image_verified += 1
                    
                    logging.info(f"ğŸ‰ '{product_name}' í•´ì‹œ+ì´ë¯¸ì§€ ë§¤ì¹­ ì™„ë£Œ")
                    
                    # ê²°ê³¼ ì¶”ê°€
                    best_matches.append((
                        (final_matches['haereum'][0], 0.95) if final_matches['haereum'] else None,
                        (final_matches['kogift'][0], 0.95) if final_matches['kogift'] else None,
                        (final_matches['naver'][0], 0.95) if final_matches['naver'] else None
                    ))
                    continue
        
        # === í•´ì‹œ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ë§¤ì¹­ ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬ ===
        logging.debug(f"âŒ '{product_name}' í•´ì‹œ ë§¤ì¹­ ì‹¤íŒ¨ - ë§¤ì¹­ ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬")
        no_matches += 1
        
        best_matches.append((None, None, None))
    
    # ìµœì¢… í†µê³„ ì¶œë ¥
    success_rate = (hash_matches / len(product_names) * 100) if product_names else 0
    verification_rate = (image_verified / hash_matches * 100) if hash_matches > 0 else 0
    
    logging.info("\nğŸ“ˆ === ë§¤ì¹­ ì™„ë£Œ í†µê³„ ===")
    logging.info(f"âœ… í•´ì‹œ ë§¤ì¹­ ì„±ê³µ: {hash_matches}/{len(product_names)} ({success_rate:.1f}%)")
    logging.info(f"ğŸ” ì´ë¯¸ì§€ ê²€ì¦ ì™„ë£Œ: {image_verified}/{hash_matches} ({verification_rate:.1f}%)")
    logging.info(f"âŒ ë§¤ì¹­ ì‹¤íŒ¨: {no_matches}/{len(product_names)} ({100-success_rate:.1f}%)")
    logging.info(f"ğŸƒâ€â™‚ï¸ ì‚¬ìš©ëœ ì´ë¯¸ì§€: í•´ì˜¤ë¦„ {len(used_haereum)}, ê³ ë ¤ê¸°í”„íŠ¸ {len(used_kogift)}, ë„¤ì´ë²„ {len(used_naver)}")
    
    # ì„±ëŠ¥ í†µê³„
    if len(product_names) > 0:
        efficiency_score = hash_matches / len(product_names)
        if efficiency_score >= 0.8:
            logging.info("ğŸ† ë§¤ì¹­ íš¨ìœ¨ì„±: ìš°ìˆ˜ (80% ì´ìƒ)")
        elif efficiency_score >= 0.6:
            logging.info("ğŸ‘ ë§¤ì¹­ íš¨ìœ¨ì„±: ì–‘í˜¸ (60% ì´ìƒ)")
        else:
            logging.info("âš ï¸ ë§¤ì¹­ íš¨ìœ¨ì„±: ê°œì„  í•„ìš” (60% ë¯¸ë§Œ)")
    
    return best_matches

def find_best_match_for_product(product_tokens: List[str], 
                               image_info: Dict[str, Dict], 
                               used_images: Set[str] = None,
                               similarity_threshold: float = 0.45,  # ì„ê³„ê°’ ìƒí–¥ ì¡°ì • (0.3ì—ì„œ 0.45ìœ¼ë¡œ)
                               source_name_for_log: str = "UnknownSource",
                               config: Optional[configparser.ConfigParser] = None) -> Optional[Tuple[str, float]]:
    """
    Find the best matching image for a product based on name tokens.
    Updated with higher thresholds for stricter matching.
    
    Args:
        product_tokens: Tokens of the product name
        image_info: Dictionary of image metadata
        used_images: Set of already used image paths
        similarity_threshold: Minimum similarity score for matching
        source_name_for_log: Source name for logging
        config: Configuration object for retrieving settings
        
    Returns:
        Tuple of (best_match_path, similarity_score) or None if no match found
    """
    if not product_tokens:
        return None
        
    if used_images is None:
        used_images = set()
        
    best_match_path = None
    best_match_score = 0
    
    # Use the similarity_threshold passed as argument directly.
    # This threshold is expected to be set by the caller (find_best_image_matches)
    # and should be appropriate for text-based similarity.
    effective_similarity_threshold = similarity_threshold

    # Log which threshold is being used.
    logging.info(f"[{source_name_for_log}] Using text similarity threshold: {effective_similarity_threshold} (passed from caller)")
    
    # Log the number of images we're searching through
    logging.info(f"[{source_name_for_log}] Searching through {len(image_info)} images for a match")

    for img_path, img_data in image_info.items():
        # Skip if already used
        if img_path in used_images:
            continue
            
        # Get the name for matching from metadata
        if 'name_for_matching' in img_data:
            img_name = img_data['name_for_matching']
        elif 'original_name' in img_data:
            img_name = img_data['original_name']
        else:
            # Use the filename if no metadata is available
            img_name = os.path.basename(img_path)
        
        # Convert to string and calculate text similarity
        img_name_str = str(img_name)
        product_name_str = ' '.join(product_tokens)
        
        similarity = calculate_text_similarity(product_name_str, img_name_str)
        
        if similarity > best_match_score:
            best_match_score = similarity
            best_match_path = img_path
    
    # Check threshold - use the minimum threshold for extreme lenience
    if best_match_score >= effective_similarity_threshold:
        if best_match_path:
            img_name = image_info[best_match_path].get('original_name', os.path.basename(best_match_path))
            logging.info(f"{source_name_for_log}: Best match for '{' '.join(product_tokens)}': '{img_name}' with score {best_match_score:.3f}")
            return best_match_path, best_match_score
    elif best_match_path:  # We found a match but score is below threshold
        img_name = image_info[best_match_path].get('original_name', os.path.basename(best_match_path))
        logging.info(f"{source_name_for_log}: Found match below threshold. Product: '{' '.join(product_tokens)}', Image: '{img_name}', Score: {best_match_score:.3f} (threshold: {effective_similarity_threshold})")
    
    # No match found with sufficient similarity
    logging.info(f"No match found above threshold {effective_similarity_threshold} for {source_name_for_log}. Trying basic token matching.")
    
    # Try more basic matching as fallback
    for img_path, img_data in image_info.items():
        # Skip if already used
        if img_path in used_images:
            continue
            
        # Get image name from metadata
        if 'name_for_matching' in img_data:
            img_name = img_data['name_for_matching']
        elif 'original_name' in img_data:
            img_name = img_data['original_name']
        else:
            img_name = os.path.basename(img_path)
            
        # Convert to lowercase for case-insensitive matching
        img_name_lower = str(img_name).lower()
        product_name_lower = ' '.join(product_tokens).lower()
        
        # 1. ìš°ì„  ì „ì²´ ìƒí’ˆëª…ì˜ ì¼ë¶€ê°€ ì´ë¯¸ì§€ ì´ë¦„ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        basic_match_score = 0.0
        if len(product_name_lower) >= 4 and product_name_lower[:4] in img_name_lower:
            basic_match_score = 0.4
            logging.info(f"{source_name_for_log}: Product name prefix match found: '{product_name_lower[:4]}' in '{img_name}'")
            return img_path, basic_match_score
        
        # 2. ê°œë³„ í† í° ë§¤ì¹­ (ê¸¸ì´ê°€ 2 ì´ìƒì¸ ì¤‘ìš” í† í°)
        matched_tokens = []
        for token in product_tokens:
            if len(token) >= 2 and token.lower() in img_name_lower:
                matched_tokens.append(token)
        
        # ë§¤ì¹­ëœ í† í° ìˆ˜ì— ë”°ë¼ ì ìˆ˜ ê³„ì‚°
        if matched_tokens:
            # í† í° ê¸¸ì´ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ì ìš©
            token_weight = sum(len(token) for token in matched_tokens) / sum(len(token) for token in product_tokens)
            # í† í° ê°œìˆ˜ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ì ìš©
            count_weight = len(matched_tokens) / len(product_tokens)
            # ìµœì¢… ì ìˆ˜ ê³„ì‚° (ê¸¸ì´ì™€ ê°œìˆ˜ë¥¼ ëª¨ë‘ ê³ ë ¤)
            basic_match_score = 0.3 * token_weight + 0.2 * count_weight
            
            # ì„ê³„ê°’ì„ 0.05ë¡œ ì„¤ì •í•˜ì—¬ ë§¤ì¹­ì„ í—ˆìš©
            if basic_match_score >= 0.05:
                logging.info(f"{source_name_for_log}: Basic token match found: '{matched_tokens}' in '{img_name}' with score {basic_match_score:.3f}")
                return img_path, basic_match_score
    
    return None

def find_best_match_with_enhanced_matcher(
    source_img_path: str, 
    target_images: Dict[str, Dict], 
    used_images: Set[str] = None,
    enhanced_matcher: Any = None
) -> Optional[Tuple[str, float]]:
    """
    Enhanced image matching with stricter thresholds for higher quality matches.
    """
    if not enhanced_matcher:
        logging.warning("Enhanced image matcher not available. Falling back to text-based matching.")
        return None
        
    if used_images is None:
        used_images = set()
        
    best_match = None
    best_score = 0
    
    # Using higher thresholds for stricter matching
    high_confidence_threshold = 0.30   # 0.15ì—ì„œ 0.30ìœ¼ë¡œ ìƒí–¥
    min_confidence_threshold = 0.15    # 0.00001ì—ì„œ 0.15ë¡œ ëŒ€í­ ìƒí–¥
    
    gpu_info = "GPU enabled" if getattr(enhanced_matcher, 'use_gpu', False) else "CPU mode"
    logging.info(f"Running enhanced matching on {len(target_images)} target images against source: {os.path.basename(source_img_path)} ({gpu_info})")
    
    # Check if source image exists
    if not os.path.exists(source_img_path):
        logging.error(f"Source image doesn't exist: {source_img_path}")
        return None
    
    # Track timing for performance analysis
    start_time = time.time()
    matches_checked = 0
    high_conf_matches = 0
    
    # Debug: Log some target image paths for verification
    sample_keys = list(target_images.keys())[:3] if len(target_images) > 3 else list(target_images.keys())
    for key in sample_keys:
        logging.info(f"Sample target image key: {key}")
        if isinstance(target_images[key], dict):
            img_path = target_images[key].get('path', key)
            logging.info(f"  - Path from dict: {img_path}")
            if os.path.exists(img_path):
                logging.info(f"  - This path exists on disk")
            else:
                logging.info(f"  - This path does NOT exist on disk")
        else:
            logging.info(f"  - Value is not a dict: {type(target_images[key])}")
    
    for image_path, image_info in target_images.items():
        # Skip if already used
        if image_path in used_images:
            continue
        
        # Determine the actual path to use
        actual_path = image_path
        if isinstance(image_info, dict):
            if 'path' in image_info:
                actual_path = image_info['path']
            
            # Try to use nobg version if available (for potentially better matching)
            if enhanced_matcher.USE_BACKGROUND_REMOVAL and isinstance(image_info, dict):
                # Check nobg paths
                nobg_png = image_info.get('nobg_png_path')
                nobg_jpg = image_info.get('nobg_jpg_path')
                
                # Prefer PNG version for nobg (usually better quality)
                if nobg_png and os.path.exists(nobg_png):
                    logging.debug(f"Using background-removed PNG version for matching: {os.path.basename(nobg_png)}")
                    actual_path = nobg_png
                elif nobg_jpg and os.path.exists(nobg_jpg):
                    logging.debug(f"Using background-removed JPG version for matching: {os.path.basename(nobg_jpg)}")
                    actual_path = nobg_jpg
            
        # Check if image exists
        if not os.path.exists(actual_path):
            logging.warning(f"Target image doesn't exist: {actual_path} (key: {image_path})")
            continue
        
        try:
            logging.debug(f"Comparing source {os.path.basename(source_img_path)} with target {os.path.basename(actual_path)}")
            # Call the calculate_similarity method directly instead of is_match to get raw similarity
            similarity = enhanced_matcher.calculate_similarity(source_img_path, actual_path)
            
            # Log similarity score for debugging
            logging.debug(f"Raw similarity score: {similarity:.4f} for {os.path.basename(actual_path)}")
            
            matches_checked += 1
            
            if similarity > high_confidence_threshold:
                high_conf_matches += 1
                logging.info(f"High confidence match: {os.path.basename(actual_path)} = {similarity:.4f}")
                
            if similarity > best_score:
                best_score = similarity
                best_match = image_path  # Keep the original key, not the resolved path
                logging.info(f"New best match: {os.path.basename(actual_path)} with score {similarity:.4f}")
                
                # Early exit for very high confidence matches to save processing time
                if similarity > 0.75:
                    logging.info(f"Found very high confidence match ({similarity:.4f}), early exit")
                    break
                    
        except Exception as e:
            logging.error(f"Error comparing images: {e}")
            
    # Compute time spent
    elapsed = time.time() - start_time
    
    # Don't return matches below the absolute minimum threshold
    if best_match and best_score < min_confidence_threshold:
        logging.info(f"Best match score ({best_score:.4f}) below min threshold ({min_confidence_threshold})")
        return None
        
    # Log summary of matching results
    if best_match:
        if isinstance(target_images[best_match], dict) and 'path' in target_images[best_match]:
            best_path = target_images[best_match]['path']
        else:
            best_path = best_match
        logging.info(f"Best image match: {os.path.basename(best_path)} ({best_score:.4f}) [checked {matches_checked} images in {elapsed:.2f}s, {high_conf_matches} high confidence]")
    else:
        logging.info(f"No image match found after checking {matches_checked} images in {elapsed:.2f}s")
    
    return (best_match, best_score) if best_match else None

def verify_image_matches(best_matches, product_names, haereum_images, kogift_images, naver_images):
    """
    ì´ë¯¸ì§€ ë§¤ì¹­ ê²°ê³¼ë¥¼ ê²€ì¦í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    í”„ë¡œë•íŠ¸ ì´ë¦„ê³¼ íŒŒì¼ ì´ë¦„ ê°„ì˜ ê³µí†µ í† í°ì„ í™•ì¸í•˜ì—¬ ë§¤ì¹­ í’ˆì§ˆì„ ê²€ì¦í•©ë‹ˆë‹¤.
    
    Args:
        best_matches: find_best_image_matches í•¨ìˆ˜ì˜ ê²°ê³¼
        product_names: ìƒí’ˆëª… ëª©ë¡
        haereum_images: í•´ì˜¤ë¦„ ì´ë¯¸ì§€ ì •ë³´
        kogift_images: ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€ ì •ë³´
        naver_images: ë„¤ì´ë²„ ì´ë¯¸ì§€ ì •ë³´
        
    Returns:
        ê²€ì¦ëœ ë§¤ì¹­ ê²°ê³¼
    """
    verified_matches = []
    
    # ID ê¸°ë°˜ ë§¤ì¹­ì— ì‚¬ìš©ë˜ëŠ” ì •ê·œ í‘œí˜„ì‹
    id_pattern = re.compile(r'_([a-f0-9]{10})(?:\.jpg|\.png|_nobg\.png)?$')
    
    for idx, (product_name, match_set) in enumerate(zip(product_names, best_matches)):
        # Handle cases where match_set may not have exactly 3 elements
        # or if elements within are not as expected (e.g. None instead of (path, score) tuple)
        haereum_data, kogift_data, naver_data = None, None, None # Initialize with None

        if match_set and len(match_set) == 3:
            haereum_data, kogift_data, naver_data = match_set
        else:
            logging.warning(f"Invalid match_set for product '{product_name}': {match_set}. Using None values for all sources.")
            # haereum_data, kogift_data, naver_data remain None as initialized

        product_tokens = set(tokenize_product_name(product_name))
        
        # ë§¤ì¹­ í’ˆì§ˆ ê¸°ë¡
        match_quality = {
            'haereum': {'score': 0, 'match': haereum_data, 'id': None}, # Store data tuple directly
            'kogift': {'score': 0, 'match': kogift_data, 'id': None},  # Store data tuple directly
            'naver': {'score': 0, 'match': naver_data, 'id': None}    # Store data tuple directly
        }
        
        # í•´ì˜¤ë¦„ ë§¤ì¹­ ê²€ì¦
        if haereum_data and isinstance(haereum_data, tuple) and len(haereum_data) == 2:
            haereum_path, haereum_score = haereum_data
            match_quality['haereum']['score'] = haereum_score # Use the propagated score
            
            if haereum_path and haereum_path in haereum_images:
                haereum_filename = os.path.basename(haereum_path)
                haereum_id = None
                id_match = id_pattern.search(haereum_filename)
                if id_match:
                    haereum_id = id_match.group(1)
                
                haereum_tokens = set(tokenize_product_name(haereum_images[haereum_path].get('clean_name', 
                                                        haereum_images[haereum_path].get('name_for_matching', ''))))
                common_tokens = product_tokens & haereum_tokens
                token_ratio = len(common_tokens) / max(len(product_tokens), 1)
                
                # Refine score, but start with the propagated score
                match_quality['haereum']['score'] = haereum_score * (1 + token_ratio) 
                match_quality['haereum']['id'] = haereum_id
            else:
                logging.warning(f"Haereum path '{haereum_path}' not found in haereum_images or path is None.")
                match_quality['haereum']['match'] = None # Invalidate if path issue
                match_quality['haereum']['score'] = 0
        elif haereum_data: # Was not None, but not a (path, score) tuple
             logging.warning(f"Unexpected format for haereum_data: {haereum_data}. Clearing Haereum match.")
             match_quality['haereum']['match'] = None
             match_quality['haereum']['score'] = 0
        
        # ê³ ë ¤ê¸°í”„íŠ¸ ë§¤ì¹­ ê²€ì¦
        if kogift_data and isinstance(kogift_data, tuple) and len(kogift_data) == 2:
            kogift_path, kogift_score = kogift_data
            match_quality['kogift']['score'] = kogift_score # Use the propagated score

            if kogift_path and kogift_path in kogift_images:
                kogift_filename = os.path.basename(kogift_path)
                kogift_id = None
                id_match = id_pattern.search(kogift_filename)
                if id_match:
                    kogift_id = id_match.group(1)
                
                haereum_id_for_comparison = match_quality['haereum'].get('id')
                if haereum_id_for_comparison and haereum_id_for_comparison == kogift_id:
                    match_quality['kogift']['score'] = max(kogift_score, 0.8) * 1.5
                else:
                    kogift_tokens = set(tokenize_product_name(kogift_images[kogift_path].get('clean_name',
                                                         kogift_images[kogift_path].get('name_for_matching', ''))))
                    common_tokens = product_tokens & kogift_tokens
                    token_ratio = len(common_tokens) / max(len(product_tokens), 1)
                    match_quality['kogift']['score'] = kogift_score * (1 + token_ratio)
                match_quality['kogift']['id'] = kogift_id
            else:
                logging.warning(f"Kogift path '{kogift_path}' not found in kogift_images or path is None.")
                match_quality['kogift']['match'] = None # Invalidate if path issue
                match_quality['kogift']['score'] = 0
        elif kogift_data: # Was not None, but not a (path, score) tuple
             logging.warning(f"Unexpected format for kogift_data: {kogift_data}. Clearing Kogift match.")
             match_quality['kogift']['match'] = None
             match_quality['kogift']['score'] = 0

        # ë„¤ì´ë²„ ë§¤ì¹­ ê²€ì¦
        if naver_data and isinstance(naver_data, tuple) and len(naver_data) == 2:
            naver_path, naver_score = naver_data
            match_quality['naver']['score'] = naver_score # Use the propagated score

            if naver_path and naver_path in naver_images:
                naver_filename = os.path.basename(naver_path)
                naver_id = None
                id_match = id_pattern.search(naver_filename)
                if id_match:
                    naver_id = id_match.group(1)
                
                haereum_id_for_comparison = match_quality['haereum'].get('id')
                if haereum_id_for_comparison and haereum_id_for_comparison == naver_id:
                    match_quality['naver']['score'] = max(naver_score, 0.8) * 1.5
                else:
                    naver_tokens = set(tokenize_product_name(naver_images[naver_path].get('clean_name',
                                                        naver_images[naver_path].get('name_for_matching', ''))))
                    common_tokens = product_tokens & naver_tokens
                    token_ratio = len(common_tokens) / max(len(product_tokens), 1)
                    # Refine score, but start with the propagated score
                    match_quality['naver']['score'] = naver_score * (1 + token_ratio) 
                match_quality['naver']['id'] = naver_id
            else:
                logging.warning(f"Naver path '{naver_path}' not found in naver_images or path is None.")
                match_quality['naver']['match'] = None # Invalidate if path issue
                match_quality['naver']['score'] = 0
        elif naver_data: # Was not None, but not a (path, score) tuple
             logging.warning(f"Unexpected format for naver_data: {naver_data}. Clearing Naver match.")
             match_quality['naver']['match'] = None
             match_quality['naver']['score'] = 0
        
        # ê²€ì¦ ê²°ê³¼ë¥¼ ë¡œê·¸ë¡œ ì¶œë ¥
        logging.debug(f"Product: '{product_name}' - Verification scores: Haereum={match_quality['haereum']['score']:.2f}, Kogift={match_quality['kogift']['score']:.2f}, Naver={match_quality['naver']['score']:.2f}")
        
        # ìµœì¢… ê²€ì¦ëœ ë§¤ì¹­ ê²°ê³¼ ì¶”ê°€
        verified_matches.append((
            match_quality['haereum']['match'],
            match_quality['kogift']['match'],
            match_quality['naver']['match']
        ))
    
    return verified_matches

def integrate_images(df: pd.DataFrame, config: configparser.ConfigParser) -> pd.DataFrame:
    """
    ì„¸ ê°€ì§€ ì´ë¯¸ì§€ ì†ŒìŠ¤(í•´ì˜¤ë¦„, ê³ ë ¤ê¸°í”„íŠ¸, ë„¤ì´ë²„)ì˜ ì´ë¯¸ì§€ë¥¼ DataFrameì— í†µí•©í•©ë‹ˆë‹¤.
    ìƒí’ˆë³„ë¡œ ì¼ê´€ëœ ì´ë¯¸ì§€ ë§¤ì¹­ì„ ë³´ì¥í•©ë‹ˆë‹¤.
    
    Note: Haereum images (ë³¸ì‚¬ ì´ë¯¸ì§€) are ALWAYS included, regardless of matching scores.
    
    Args:
        df: ì²˜ë¦¬í•  DataFrame (data_processing.pyì˜ format_product_data_for_outputì„ ê±°ì¹œ ìƒíƒœì—¬ì•¼ í•¨)
        config: ì„¤ì • íŒŒì¼
        
    Returns:
        ì´ë¯¸ì§€ê°€ í†µí•©ëœ DataFrame
    """
    try:
        logging.info("í†µí•©: ì´ë¯¸ì§€ í†µí•© í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")
        result_df = df.copy() # dfëŠ” ì´ë¯¸ format_product_data_for_outputì„ ê±°ì³ ì´ë¯¸ì§€ ì»¬ëŸ¼ì— dictê°€ ìˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒ

        # These column names are expected to be in the input df, potentially holding original scraped URLs
        # if they were not already incorporated into the image dictionaries by data_processing.py
        scraped_haereum_url_col_input_df = 'ë³¸ì‚¬ì´ë¯¸ì§€URL' 
        scraped_kogift_url_col_input_df = 'ê³ ë ¤ê¸°í”„íŠ¸ì´ë¯¸ì§€URL' 
        scraped_naver_url_col_input_df = 'ë„¤ì´ë²„ì´ë¯¸ì§€URL'
        
        # ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        main_img_dir = Path(config.get('Paths', 'image_main_dir', fallback='C:\\\\RPA\\\\Image\\\\Main'))
        haereum_dir = main_img_dir / 'Haereum'
        kogift_dir = main_img_dir / 'Kogift'
        naver_dir = main_img_dir / 'Naver'
        
        # ë””ë ‰í† ë¦¬ ì¡´ì¬ ì²´í¬
        if not haereum_dir.exists():
            logging.warning(f"í•´ì˜¤ë¦„ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {haereum_dir}")
        if not kogift_dir.exists():
            logging.warning(f"ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {kogift_dir}")
        if not naver_dir.exists():
            logging.warning(f"ë„¤ì´ë²„ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {naver_dir}")
        
        # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ì¤€ë¹„
        haereum_images = prepare_image_metadata(haereum_dir, 'haereum_', prefer_original=True, prefer_jpg=True)
        kogift_images = prepare_image_metadata(kogift_dir, 'kogift_', prefer_original=True, prefer_jpg=True)
        naver_images = prepare_image_metadata(naver_dir, 'naver_', prefer_original=True, prefer_jpg=True)
        
        # í•„ìš”í•œ ì—´ ì¶”ê°€
        if 'ë³¸ì‚¬ ì´ë¯¸ì§€' not in result_df.columns:
            result_df['ë³¸ì‚¬ ì´ë¯¸ì§€'] = None
        if 'ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€' not in result_df.columns:
            result_df['ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€'] = None
        if 'ë„¤ì´ë²„ ì´ë¯¸ì§€' not in result_df.columns:
            result_df['ë„¤ì´ë²„ ì´ë¯¸ì§€'] = None
        
        # Ensure target columns for image data exist before processing
        # These are the final column names used for output (e.g., in Excel)
        target_cols = ['ë³¸ì‚¬ ì´ë¯¸ì§€', 'ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€', 'ë„¤ì´ë²„ ì´ë¯¸ì§€']
        for col in target_cols:
            if col not in result_df.columns:
                # Initialize with a suitable default, e.g., None or '-'
                # Using None initially might be better if subsequent logic checks for None
                result_df[col] = None 
                logging.debug(f"Added missing target image column: {col}")

        # ìƒí’ˆ ëª©ë¡ ì¶”ì¶œ
        product_names = result_df['ìƒí’ˆëª…'].tolist()
        
        # ì œí’ˆ ìˆ˜ì™€ ì´ë¯¸ì§€ ìˆ˜ ë¡œê¹…
        logging.info(f"ì œí’ˆ ìˆ˜: {len(product_names)}ê°œ")
        logging.info(f"í•´ì˜¤ë¦„ ì´ë¯¸ì§€: {len(haereum_images)}ê°œ")
        logging.info(f"ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€: {len(kogift_images)}ê°œ")
        logging.info(f"ë„¤ì´ë²„ ì´ë¯¸ì§€: {len(naver_images)}ê°œ")
        
        # ìƒí’ˆëª… ìƒ˜í”Œ ë¡œê¹…
        if product_names:
            sample_products = product_names[:3] if len(product_names) > 3 else product_names
            logging.debug(f"ì œí’ˆ ìƒ˜í”Œ: {sample_products}")
        
        # Retrieve similarity threshold from config with higher quality defaults
        # 1) Primary key: Matching.text_threshold (higher quality standard)
        # 2) Secondary key: Matching.image_threshold 
        # 3) Tertiary key: ImageMatching.minimum_match_confidence
        # 4) Fallback: 0.4 (high quality default)
        
        # Get thresholds in priority order
        text_threshold = None
        image_threshold = None
        min_match_confidence = None
        
        try:
            if config.has_option('Matching', 'text_threshold'):
                text_threshold = config.getfloat('Matching', 'text_threshold')
                logging.debug(f"Read text_threshold: {text_threshold}")
        except (configparser.Error, ValueError) as e:
            logging.warning(f"Could not read [Matching] text_threshold: {e}")
            
        try:
            if config.has_option('Matching', 'image_threshold'):
                image_threshold = config.getfloat('Matching', 'image_threshold')
                logging.debug(f"Read image_threshold: {image_threshold}")
        except (configparser.Error, ValueError) as e:
            logging.warning(f"Could not read [Matching] image_threshold: {e}")

        try:
            if config.has_option('ImageMatching', 'minimum_match_confidence'):
                min_match_confidence = config.getfloat('ImageMatching', 'minimum_match_confidence')
                logging.debug(f"Read minimum_match_confidence: {min_match_confidence}")
        except (configparser.Error, ValueError) as e:
            logging.warning(f"Could not read [ImageMatching] minimum_match_confidence: {e}")

        # Use the first available threshold, with higher defaults
        if text_threshold is not None:
            similarity_threshold = text_threshold
        elif image_threshold is not None:
            similarity_threshold = image_threshold
        elif min_match_confidence is not None:
            similarity_threshold = min_match_confidence
        else:
            similarity_threshold = 0.4  # Higher quality default
            logging.warning(f"Using higher quality default similarity_threshold of {similarity_threshold} as specific values not found or invalid in config.")
        
        # Set the initial matching threshold
        initial_matching_threshold = similarity_threshold # Use the general similarity_threshold from config or default
        
        logging.info(f"ì´ë¯¸ì§€ ë§¤ì¹­ ìœ ì‚¬ë„ ì„ê³„ê°’ (for find_best_image_matches): {initial_matching_threshold}")
        
        # ìµœì  ë§¤ì¹˜ ì°¾ê¸° (ì¼ê´€ì„± ë³´ì¥)
        best_matches = find_best_image_matches(
            product_names,
            haereum_images,
            kogift_images,
            naver_images,
            similarity_threshold=initial_matching_threshold,  # Use lower threshold for initial matching
            config=config,
            df=result_df  # Pass DataFrame for product code matching
        )
        
        # ë§¤ì¹­ ê²°ê³¼ ê²€ì¦
        logging.info(f"ì´ë¯¸ì§€ ë§¤ì¹­ ê²€ì¦ ì¤‘...")
        verified_matches = verify_image_matches(
            best_matches,
            product_names,
            haereum_images,
            kogift_images,
            naver_images
        )
        
        # ê²°ê³¼ë¥¼ DataFrameì— ì ìš©
        # Map for matching web URL columns with their correct names in the dataframe
        assumed_url_cols = {
            'haereum': 'ë³¸ì‚¬ìƒí’ˆë§í¬',      # Changed from 'ë³¸ì‚¬ë§í¬'
            'kogift': 'ê³ ë ¤ê¸°í”„íŠ¸ ìƒí’ˆë§í¬', # Changed from 'ê³ ë ¤ ë§í¬'
            'naver': 'ë„¤ì´ë²„ ì‡¼í•‘ ë§í¬'     # Changed from 'ë„¤ì´ë²„ ë§í¬'
        }

        # Pre-compute Koreagift product info existence for all rows
        # Will be used to determine if image should be assigned
        kogift_product_info_exists = []
        for idx in range(len(result_df)):
            if idx >= len(result_df):
                kogift_product_info_exists.append(False)
                continue
                
            row_data = result_df.iloc[idx]
            has_kogift_info = False
            
            # Check for Koreagift link
            kogift_link_col = 'ê³ ë ¤ê¸°í”„íŠ¸ ìƒí’ˆë§í¬'
            if kogift_link_col in row_data and row_data[kogift_link_col]:
                if isinstance(row_data[kogift_link_col], str) and row_data[kogift_link_col].strip() not in ['', '-', 'None', None]:
                    has_kogift_info = True
            
            # Check for Koreagift price
            if not has_kogift_info:
                kogift_price_col = 'íŒë§¤ê°€(Ví¬í•¨)(2)'
                if kogift_price_col in row_data and pd.notna(row_data[kogift_price_col]) and row_data[kogift_price_col] not in [0, '-', '', None]:
                    has_kogift_info = True
                    
            # Check for alternative price column
            if not has_kogift_info:
                alt_kogift_price_col = 'íŒë§¤ë‹¨ê°€(Ví¬í•¨)(2)'
                if alt_kogift_price_col in row_data and pd.notna(row_data[alt_kogift_price_col]) and row_data[alt_kogift_price_col] not in [0, '-', '', None]:
                    has_kogift_info = True
            
            kogift_product_info_exists.append(has_kogift_info)
        
        logging.info(f"Pre-computed Koreagift product info existence for {len(kogift_product_info_exists)} rows")
        logging.info(f"Found {sum(kogift_product_info_exists)} rows with Koreagift product info")

        # Apply results to DataFrame
        mismatch_count = 0
        naver_mismatch_count = 0
        
        # Keep track of used Haereum images
        used_haereum_images = set()
        
        for idx, (match_set, product_name) in enumerate(zip(verified_matches, product_names)):
            if idx >= len(result_df):
                continue
                
            if match_set:
                haereum_data, kogift_data, naver_data = match_set
                
                # Process Haereum image
                if haereum_data and isinstance(haereum_data, tuple) and len(haereum_data) >= 2:
                    haereum_path, haereum_score = haereum_data[:2]  # Correctly extract path and score
                    if haereum_path and haereum_path in haereum_images:
                        used_haereum_images.add(haereum_path)
                        haereum_image_info_from_metadata = haereum_images[haereum_path] # Metadata from disk scan
                        
                        # Determine web URL for the Haereum image
                        current_haereum_url = None
                        
                        # 1. Try extracting from existing image dict in the input df
                        if idx < len(df) and 'ë³¸ì‚¬ ì´ë¯¸ì§€' in df.columns and isinstance(df.iloc[idx].get('ë³¸ì‚¬ ì´ë¯¸ì§€'), dict):
                            url_from_dict = df.iloc[idx]['ë³¸ì‚¬ ì´ë¯¸ì§€'].get('url')
                            if isinstance(url_from_dict, str) and url_from_dict.startswith(('http://', 'https://')):
                                current_haereum_url = url_from_dict
                        
                        # 2. Fallback: Check a separate URL column in input df (e.g., 'ë³¸ì‚¬ì´ë¯¸ì§€URL')
                        if not current_haereum_url and idx < len(df) and scraped_haereum_url_col_input_df in df.columns:
                            url_val_from_separate_col = df.iloc[idx].get(scraped_haereum_url_col_input_df)
                            if isinstance(url_val_from_separate_col, str) and url_val_from_separate_col.startswith(('http://', 'https://')):
                                current_haereum_url = url_val_from_separate_col
                        
                        # 3. Fallback: URL from metadata (less likely to be a web URL)
                        if not current_haereum_url:
                            current_haereum_url = haereum_image_info_from_metadata.get('url', '')

                        image_data = {
                            'url': current_haereum_url, 
                            'local_path': haereum_image_info_from_metadata.get('path', haereum_path),
                            'source': 'haereum',
                            'product_name': product_name,
                            'similarity': haereum_score,
                            'original_path': haereum_path
                        }
                        result_df.at[idx, 'ë³¸ì‚¬ ì´ë¯¸ì§€'] = image_data
                        
                        # Also update the separate URL column in result_df for consistency if it exists
                        if scraped_haereum_url_col_input_df not in result_df.columns:
                            result_df[scraped_haereum_url_col_input_df] = None
                        result_df.at[idx, scraped_haereum_url_col_input_df] = image_data['url']
                
                # Process Kogift image
                if kogift_data and isinstance(kogift_data, tuple) and len(kogift_data) >= 2:
                    kogift_path, kogift_score = kogift_data[:2]
                    if kogift_path and kogift_path in kogift_images:
                        has_kogift_product_info = idx < len(kogift_product_info_exists) and kogift_product_info_exists[idx]
                        if has_kogift_product_info:
                            kogift_image_info_from_metadata = kogift_images[kogift_path]
                            
                            current_kogift_url = None
                            original_url_for_dict = None
                            original_crawled_url_for_dict = None

                            # 1. Check URL from the dictionary in the input df's 'ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€' column
                            input_kogift_dict = None
                            if idx < len(df) and 'ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€' in df.columns and isinstance(df.iloc[idx].get('ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€'), dict):
                                input_kogift_dict = df.iloc[idx]['ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€']
                                url_from_dict = input_kogift_dict.get('url')
                                if isinstance(url_from_dict, str) and url_from_dict.startswith(('http://', 'https://')):
                                    current_kogift_url = url_from_dict
                                # Preserve original_url and original_crawled_url if they exist in the input dict
                                original_url_for_dict = input_kogift_dict.get('original_url')
                                original_crawled_url_for_dict = input_kogift_dict.get('original_crawled_url')
                            
                            # 2. Fallback: Check a separate URL column in input df (e.g., 'ê³ ë ¤ê¸°í”„íŠ¸ì´ë¯¸ì§€URL')
                            if not current_kogift_url and idx < len(df) and scraped_kogift_url_col_input_df in df.columns:
                                url_val_from_separate_col = df.iloc[idx].get(scraped_kogift_url_col_input_df)
                                if isinstance(url_val_from_separate_col, str) and url_val_from_separate_col.startswith(('http://', 'https://')):
                                    current_kogift_url = url_val_from_separate_col
                            
                            # 3. Fallback: URL from metadata (less likely to be a web URL)
                            if not current_kogift_url:
                                current_kogift_url = kogift_image_info_from_metadata.get('url', '')
                            
                            # If original_url/original_crawled_url were not in input_dict, try metadata
                            if not original_url_for_dict:
                                original_url_for_dict = kogift_image_info_from_metadata.get('original_url')
                            if not original_crawled_url_for_dict:
                                original_crawled_url_for_dict = kogift_image_info_from_metadata.get('original_crawled_url')

                            image_data = {
                                'url': current_kogift_url, 
                                'local_path': kogift_image_info_from_metadata.get('path', kogift_path),
                                'source': 'kogift',
                                'product_name': product_name,
                                'similarity': kogift_score,  
                                'original_path': kogift_path,
                                'original_url': original_url_for_dict,
                                'original_crawled_url': original_crawled_url_for_dict
                            }
                            result_df.at[idx, 'ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€'] = image_data
                            logger.info(f"Row {idx} ('{product_name}'): Assigned Kogift image '{os.path.basename(kogift_path)}' with URL: {current_kogift_url}")
                        else:
                            logger.warning(f"Row {idx} ('{product_name}'): Matched Kogift image '{os.path.basename(kogift_path)}' (Score: {kogift_score:.4f}) BUT product info missing. Discarding image.")
                            mismatch_count += 1
                    else:
                        logger.warning(f"Row {idx} ('{product_name}'): Kogift matched path '{kogift_path}' not in kogift_images dict or path is None.")
                elif kogift_data: # Matched but not in expected tuple format
                    logger.warning(f"Row {idx} ('{product_name}'): Kogift image matched but data format unexpected: {kogift_data}")
                else: # No Kogift match found by find_best_image_matches
                    logger.debug(f"Row {idx} ('{product_name}'): No Kogift image matched by find_best_image_matches.")

                # Process Naver image
                if naver_data and isinstance(naver_data, tuple) and len(naver_data) >= 2:
                    naver_path, naver_score = naver_data[:2]  # Correctly extract path and score
                    if naver_path and naver_path in naver_images:
                        # Check if there's Naver product info
                        naver_info_exists = False
                        
                        # Check for Naver link
                        naver_link_col = 'ë„¤ì´ë²„ ì‡¼í•‘ ë§í¬'
                        if naver_link_col in result_df.columns and pd.notna(result_df.at[idx, naver_link_col]) and result_df.at[idx, naver_link_col] not in ['', '-', 'None', None]:
                            naver_info_exists = True
                            
                        # Check for alternative Naver link column
                        alt_naver_link_col = 'ë„¤ì´ë²„ ë§í¬'
                        if not naver_info_exists and alt_naver_link_col in result_df.columns and pd.notna(result_df.at[idx, alt_naver_link_col]) and result_df.at[idx, alt_naver_link_col] not in ['', '-', 'None', None]:
                            naver_info_exists = True
                            
                        # Check for Naver price
                        naver_price_cols = ['íŒë§¤ë‹¨ê°€(Ví¬í•¨)(3)', 'ë„¤ì´ë²„ íŒë§¤ë‹¨ê°€']
                        for price_col in naver_price_cols:
                            if not naver_info_exists and price_col in result_df.columns and pd.notna(result_df.at[idx, price_col]) and result_df.at[idx, price_col] not in [0, '-', '', None]:
                                naver_info_exists = True
                                break
                                
                        # For Naver, we're more lenient - still add the image even without product info,
                        # but log the mismatch for tracking
                        if not naver_info_exists:
                            naver_mismatch_count += 1
                            logging.warning(f"Adding Naver image despite missing product info: {product_name}")
                            
                        naver_image_info_from_metadata = naver_images[naver_path]
                        
                        current_naver_image_url = None
                        product_page_url_for_dict = None

                        # 1. Check URL from the dictionary in the input df's 'ë„¤ì´ë²„ ì´ë¯¸ì§€' column
                        input_naver_dict = None
                        if idx < len(df) and 'ë„¤ì´ë²„ ì´ë¯¸ì§€' in df.columns and isinstance(df.iloc[idx].get('ë„¤ì´ë²„ ì´ë¯¸ì§€'), dict):
                            input_naver_dict = df.iloc[idx]['ë„¤ì´ë²„ ì´ë¯¸ì§€']
                            url_from_dict = input_naver_dict.get('url') # This should be direct image URL
                            if isinstance(url_from_dict, str) and (('phinf.pstatic.net' in url_from_dict) or any(url_from_dict.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif'])):
                                current_naver_image_url = url_from_dict
                            # Preserve product_page_url if it exists and is not an image URL
                            prod_page_url = input_naver_dict.get('product_page_url')
                            if isinstance(prod_page_url, str) and prod_page_url.startswith(('http://', 'https://')) and not (('phinf.pstatic.net' in prod_page_url) or any(prod_page_url.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif'])):
                                product_page_url_for_dict = prod_page_url
                        
                        # 2. Fallback: Check a separate URL column in input df for direct image URL
                        if not current_naver_image_url and idx < len(df) and scraped_naver_url_col_input_df in df.columns:
                            url_val_from_separate_col = df.iloc[idx].get(scraped_naver_url_col_input_df)
                            if isinstance(url_val_from_separate_col, str) and (('phinf.pstatic.net' in url_val_from_separate_col) or any(url_val_from_separate_col.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif'])):
                                current_naver_image_url = url_val_from_separate_col
                        
                        # 3. Fallback: URL from metadata (less likely to be a direct image web URL)
                        if not current_naver_image_url:
                            meta_url = naver_image_info_from_metadata.get('url')
                            if isinstance(meta_url, str) and (('phinf.pstatic.net' in meta_url) or any(meta_url.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif'])):
                                current_naver_image_url = meta_url

                        # For product_page_url, if not found in input_dict, try metadata's product_url
                        if not product_page_url_for_dict:
                            meta_prod_page_url = naver_image_info_from_metadata.get('product_url')
                            if isinstance(meta_prod_page_url, str) and meta_prod_page_url.startswith(('http://', 'https://')) and not (('phinf.pstatic.net' in meta_prod_page_url) or any(meta_prod_page_url.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif'])):
                                product_page_url_for_dict = meta_prod_page_url
                        
                        image_data = {
                            'url': current_naver_image_url, 
                            'local_path': naver_image_info_from_metadata.get('path', naver_path),
                            'source': 'naver',
                            'product_name': product_name,
                            'similarity': naver_score,  
                            'original_path': naver_path,
                            'product_page_url': product_page_url_for_dict
                        }
                        result_df.at[idx, 'ë„¤ì´ë²„ ì´ë¯¸ì§€'] = image_data
                        
                        # Logic for 'ë„¤ì´ë²„ ì‡¼í•‘ ë§í¬' (product page link in result_df)
                        shopping_link_col_in_result_df = 'ë„¤ì´ë²„ ì‡¼í•‘ ë§í¬'
                        final_product_page_link = None

                        # A. Prioritize product_page_url_for_dict if available from above
                        if product_page_url_for_dict:
                            final_product_page_link = product_page_url_for_dict
                        
                        # B. Fallback to checking a separate 'ë„¤ì´ë²„ ì‡¼í•‘ ë§í¬' column in the input df
                        if not final_product_page_link and idx < len(df) and shopping_link_col_in_result_df in df.columns:
                             url_val_from_input_shopping_link_col = df.iloc[idx].get(shopping_link_col_in_result_df)
                             if isinstance(url_val_from_input_shopping_link_col, str) and url_val_from_input_shopping_link_col.startswith(('http://', 'https://')) and not (('phinf.pstatic.net' in url_val_from_input_shopping_link_col) or any(url_val_from_input_shopping_link_col.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif'])):
                                 final_product_page_link = url_val_from_input_shopping_link_col
                        
                        # C. Fallback to naver_image_info_from_metadata's product_url or url if it's a page link (already covered by product_page_url_for_dict logic)
                        # No new logic needed here for C as it's incorporated into product_page_url_for_dict

                        if final_product_page_link:
                            if shopping_link_col_in_result_df not in result_df.columns: result_df[shopping_link_col_in_result_df] = None
                            # Set only if current value is empty/placeholder OR if the new link is different and valid
                            current_shopping_link_val = result_df.at[idx, shopping_link_col_in_result_df]
                            if not pd.notna(current_shopping_link_val) or current_shopping_link_val in ['', '-', 'None', None] or current_shopping_link_val != final_product_page_link:
                                result_df.at[idx, shopping_link_col_in_result_df] = final_product_page_link
        
        # Final check: Ensure EVERY row has a Haereum image 
        # If any row is missing a Haereum image, assign a random one
        haereum_count_before = sum(1 for i in range(len(result_df)) if isinstance(result_df.at[i, 'ë³¸ì‚¬ ì´ë¯¸ì§€'], dict))
        haereum_added_count = 0
        
        # Get available Haereum images that haven't been used yet
        available_haereum = [path for path in haereum_images if path not in used_haereum_images]
        if not available_haereum and haereum_images:
            # If all images are used but we still need more, reset and use them again
            available_haereum = list(haereum_images.keys())
            logging.warning(f"All Haereum images have been used. Reusing images for remaining rows.")
        
        for idx in range(len(result_df)):
            if idx >= len(result_df):
                continue
                
            # Check if this row is missing a Haereum image
            if not isinstance(result_df.at[idx, 'ë³¸ì‚¬ ì´ë¯¸ì§€'], dict):
                product_name = result_df.at[idx, 'ìƒí’ˆëª…'] if 'ìƒí’ˆëª…' in result_df.columns else f"Row {idx+1}"
                logging.warning(f"Row {idx} ('{product_name}'): Missing Haereum image. Assigning random image.")
                
                # Assign a random Haereum image
                if available_haereum:
                    haereum_path = available_haereum.pop(0)  # Take the first available
                    if not available_haereum and haereum_images:
                        # If we ran out, reset the list
                        available_haereum = list(haereum_images.keys())
                    
                    haereum_image_info_from_metadata = haereum_images[haereum_path]
                    
                    # Create image data
                    image_data = {
                        'url': haereum_image_info_from_metadata.get('url', ''), 
                        'local_path': haereum_image_info_from_metadata.get('path', haereum_path),
                        'source': 'haereum',
                        'product_name': product_name,
                        'similarity': 0.01,  # Very low score to indicate this is a desperate assignment
                        'original_path': haereum_path
                    }
                    result_df.at[idx, 'ë³¸ì‚¬ ì´ë¯¸ì§€'] = image_data
                    haereum_added_count += 1
        
        # Log the results of ensuring Haereum images
        haereum_count_after = sum(1 for i in range(len(result_df)) if isinstance(result_df.at[i, 'ë³¸ì‚¬ ì´ë¯¸ì§€'], dict))
        logging.info(f"Haereum image count: {haereum_count_before} -> {haereum_count_after} (Added {haereum_added_count} random images)")
        
        # ì´ë¯¸ì§€ ê²½ë¡œ ë¶ˆì¼ì¹˜ ìˆ˜ì • (ë¡œì»¬ íŒŒì¼ì´ ì´ë™ëœ ê²½ìš°)
        for idx in range(len(result_df)):
            if idx >= len(result_df):
                continue
                
            # Check and fix Koreagift image paths
            kogift_img = result_df.at[idx, 'ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€']
            if isinstance(kogift_img, dict) and 'local_path' in kogift_img:
                local_path = kogift_img['local_path']
                if not os.path.exists(local_path):
                    # Try to find the file in the Kogift directory by basename
                    basename = os.path.basename(local_path)
                    for directory in [kogift_dir, main_img_dir]:
                        possible_path = os.path.join(directory, basename)
                        if os.path.exists(possible_path):
                            kogift_img['local_path'] = possible_path
                            result_df.at[idx, 'ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€'] = kogift_img
                            logging.info(f"Fixed Koreagift image path from {local_path} to {possible_path}")
                            break
            
            # Check and fix Naver image paths
            naver_img = result_df.at[idx, 'ë„¤ì´ë²„ ì´ë¯¸ì§€']
            if isinstance(naver_img, dict) and 'local_path' in naver_img:
                local_path = naver_img['local_path']
                if not os.path.exists(local_path):
                    # Try to find the file in the Naver directory by basename or URL hash
                    basename = os.path.basename(local_path)
                    url_hash = None
                    if 'url' in naver_img and naver_img['url']:
                        url_hash = hashlib.md5(naver_img['url'].encode()).hexdigest()[:10]
                    
                    found = False
                    for directory in [naver_dir, main_img_dir]:
                        if os.path.exists(directory):
                            for filename in os.listdir(directory):
                                file_path = os.path.join(directory, filename)
                                if os.path.isfile(file_path) and (
                                    basename == filename or 
                                    (url_hash and url_hash in filename)
                                ):
                                    naver_img['local_path'] = file_path
                                    result_df.at[idx, 'ë„¤ì´ë²„ ì´ë¯¸ì§€'] = naver_img
                                    logging.info(f"Fixed Naver image path from {local_path} to {file_path}")
                                    found = True
                                    break
                        if found:
                            break
        
        # Count final images
        haereum_count = result_df['ë³¸ì‚¬ ì´ë¯¸ì§€'].apply(lambda x: isinstance(x, dict)).sum()
        kogift_count = result_df['ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€'].apply(lambda x: isinstance(x, dict)).sum()
        naver_count = result_df['ë„¤ì´ë²„ ì´ë¯¸ì§€'].apply(lambda x: isinstance(x, dict)).sum()
        
        logging.info(f"í†µí•©: ì´ë¯¸ì§€ ë§¤ì¹­ ì™„ë£Œ - í•´ì˜¤ë¦„: {haereum_count}ê°œ, ê³ ë ¤ê¸°í”„íŠ¸: {kogift_count}ê°œ, ë„¤ì´ë²„: {naver_count}ê°œ")
        
        return result_df
    
    except Exception as e:
        logging.error(f"í†µí•©: ì´ë¯¸ì§€ í†µí•© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return df

def improved_kogift_image_matching(df: pd.DataFrame) -> pd.DataFrame:
    """
    ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€ ë§¤ì¹­ ê°œì„  í•¨ìˆ˜
    
    ì´ë¯¸ì§€ URLì´ ëˆ„ë½ë˜ì—ˆê±°ë‚˜ ì˜ëª»ëœ ê²½ìš° ì‹¤ì œ ìƒí’ˆ ë§í¬ë¥¼ í™œìš©í•´ ì˜¬ë°”ë¥¸ ì´ë¯¸ì§€ URLì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Args:
        df: í˜„ì¬ DataFrame
        
    Returns:
        updated DataFrame with improved Kogift image URLs
    """
    try:
        if 'ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€' not in df.columns or 'ê³ ë ¤ê¸°í”„íŠ¸ ìƒí’ˆë§í¬' not in df.columns:
            logging.warning("í•„ìš”í•œ ì»¬ëŸ¼(ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€ ë˜ëŠ” ê³ ë ¤ê¸°í”„íŠ¸ ìƒí’ˆë§í¬)ì´ ì—†ì–´ ì´ë¯¸ì§€ ë§í¬ ìˆ˜ì • ë¶ˆê°€")
            return df
        
        update_count = 0
        result_df = df.copy()
        
        for idx, row in result_df.iterrows():
            # Check if already has a valid URL
            img_data = row.get('ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€')
            if not isinstance(img_data, dict):
                continue
                
            # Check if URL is missing or a placeholder
            url = img_data.get('url')
            if url and isinstance(url, str) and not url.startswith('http://placeholder.url/') and url.startswith(('http://', 'https://')):
                # ì´ë¯¸ ìœ íš¨í•œ URLì´ ìˆëŠ” ê²½ìš°
                continue
                
            # Check if we have an original_url - ì¶”ê°€ëœ ë¶€ë¶„
            original_url = img_data.get('original_url')
            if original_url and isinstance(original_url, str) and original_url.startswith(('http://', 'https://')):
                # ì›ë³¸ URL ì •ë³´ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
                img_data['url'] = original_url
                result_df.at[idx, 'ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€'] = img_data
                update_count += 1
                logging.info(f"Row {idx}: Using original URL {original_url[:50]}... for Kogift image")
                continue
                
            # Check if we have an original_crawled_url
            original_crawled_url = img_data.get('original_crawled_url')
            if original_crawled_url and isinstance(original_crawled_url, str) and original_crawled_url.startswith(('http://', 'https://')):
                # original_crawled_url ì •ë³´ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
                img_data['url'] = original_crawled_url
                result_df.at[idx, 'ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€'] = img_data
                update_count += 1
                logging.info(f"Row {idx}: Using original crawled URL {original_crawled_url[:50]}... for Kogift image")
                continue
                
            # ìƒí’ˆ ë§í¬ê°€ ìˆëŠ”ì§€ í™•ì¸
            product_link = row.get('ê³ ë ¤ê¸°í”„íŠ¸ ìƒí’ˆë§í¬')
            if not product_link or not isinstance(product_link, str) or not product_link.startswith(('http://', 'https://')):
                continue
                
            # Get product code from URL
            try:
                # ìƒí’ˆ ì½”ë“œ ì¶”ì¶œ (URL íŒ¨í„´ì— ë”°ë¼ ì¡°ì • í•„ìš”)
                product_code = None
                
                # URLì—ì„œ ìƒí’ˆ ì½”ë“œ ì¶”ì¶œ ì‹œë„ (ì—¬ëŸ¬ íŒ¨í„´ ì§€ì›)
                if 'goods_view.php' in product_link:
                    # goods_view.php?goodsno=12345 íŒ¨í„´ ì²˜ë¦¬
                    parts = product_link.split('goodsno=')
                    if len(parts) > 1:
                        product_code = parts[1].split('&')[0]
                elif '/goods/' in product_link:
                    # /goods/1234 íŒ¨í„´ ì²˜ë¦¬
                    parts = product_link.split('/goods/')
                    if len(parts) > 1:
                        product_code = parts[1].split('/')[0].split('?')[0]
                elif 'goodsDetail' in product_link:
                    # goodsDetail?goodsNo=1234 íŒ¨í„´ ì²˜ë¦¬
                    parts = product_link.split('goodsNo=')
                    if len(parts) > 1:
                        product_code = parts[1].split('&')[0]
                # ê³ ë ¤ê¸°í”„íŠ¸ íŠ¹í™” íŒ¨í„´
                elif 'no=' in product_link:
                    # ê³ ë ¤ê¸°í”„íŠ¸ URL íŒ¨í„´ ì²˜ë¦¬ (no=12345)
                    parts = product_link.split('no=')
                    if len(parts) > 1:
                        product_code = parts[1].split('&')[0]
                
                if not product_code:
                    logging.warning(f"Row {idx}: ìƒí’ˆ ë§í¬ì—ì„œ ì½”ë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ: {product_link}")
                    # ìƒí’ˆ ì½”ë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ëŠ” ê²½ìš° ìƒí’ˆ ë§í¬ ìì²´ë¥¼ ì´ë¯¸ì§€ URLë¡œ ì‚¬ìš©
                    img_data['url'] = product_link
                    img_data['original_url'] = product_link  # ì¶”ê°€: ì›ë³¸ URL ì €ì¥
                    result_df.at[idx, 'ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€'] = img_data
                    update_count += 1
                    logging.info(f"Row {idx}: ì½”ë“œ ì¶”ì¶œ ì‹¤íŒ¨, ìƒí’ˆ ë§í¬ë¥¼ ì´ë¯¸ì§€ URLë¡œ ì‚¬ìš© - {product_link}")
                    continue
                    
                # ìƒí’ˆ ì´ë¯¸ì§€ URL ìƒì„±
                if 'koreagift.com' in product_link.lower():
                    # ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€ íŒ¨í„´
                    # 1. ê¸°ë³¸ íŒ¨í„´: shop_{product_code}.jpg
                    # Ensure product_code is just the base number (e.g., 1707873892937710, not 1707873892937710_0)
                    image_url = f"https://koreagift.com/ez/upload/mall/shop_{product_code}_0.jpg"
                else:
                    # ì¼ë°˜ì ì¸ ì‡¼í•‘ëª° ì´ë¯¸ì§€ íŒ¨í„´
                    domain_parts = product_link.split('/')
                    if len(domain_parts) > 2:
                        base_domain = domain_parts[2]
                        image_url = f"https://{base_domain}/data/item/goods{product_code}/thumb-{product_code}_500x500.jpg"
                    else:
                        # ë„ë©”ì¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ëŠ” ê²½ìš° ìƒí’ˆ ë§í¬ ìì²´ë¥¼ ì´ë¯¸ì§€ URLë¡œ ì‚¬ìš©
                        image_url = product_link
                        logging.warning(f"Row {idx}: ìƒí’ˆ ë§í¬ {product_link}ì—ì„œ ë„ë©”ì¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ì–´ ìƒí’ˆ ë§í¬ ìì²´ë¥¼ ì‚¬ìš©")
                
                # ê¸°ì¡´ ì´ë¯¸ì§€ ë°ì´í„° ì—…ë°ì´íŠ¸
                img_data['url'] = image_url
                img_data['original_url'] = image_url  # ì¶”ê°€: ì›ë³¸ URL ì €ì¥
                img_data['original_crawled_url'] = image_url  # ì¶”ê°€: í¬ë¡¤ë§ëœ URL ì €ì¥
                img_data['product_id'] = product_code  # ì¶”ê°€: ìƒí’ˆ ì½”ë“œ ì €ì¥
                result_df.at[idx, 'ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€'] = img_data
                update_count += 1
                logging.debug(f"Row {idx}: ê³ ë ¤ê¸°í”„íŠ¸ URL ì¶”ê°€ - {image_url}")
                
            except Exception as e:
                logging.error(f"Row {idx}: ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€ URL ìƒì„± ì˜¤ë¥˜ - {str(e)}")
                # ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš°ì—ë„ ìƒí’ˆ ë§í¬ ìì²´ë¥¼ ì´ë¯¸ì§€ URLë¡œ ì‚¬ìš©
                if product_link and isinstance(product_link, str) and product_link.startswith(('http://', 'https://')):
                    img_data['url'] = product_link
                    img_data['original_url'] = product_link
                    result_df.at[idx, 'ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€'] = img_data
                    update_count += 1
                    logging.info(f"Row {idx}: ì˜¤ë¥˜ ë°œìƒ, ìƒí’ˆ ë§í¬ë¥¼ ì´ë¯¸ì§€ URLë¡œ ì‚¬ìš© - {product_link}")
                continue
                
        logging.info(f"improved_kogift_image_matching fixed {update_count} image links")
        return result_df
        
    except Exception as e:
        logging.error(f"ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€ ë§í¬ ê°œì„  ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return df

def integrate_and_filter_images(df: pd.DataFrame, config: configparser.ConfigParser, 
                            save_excel_output=False) -> pd.DataFrame:
    """
    Integrates and filters images from all sources, applying strict quality controls.
    
    This function performs the following steps:
    1. Integrate images from all sources with higher thresholds
    2. Improve Kogift image matching
    3. Apply strict filtering based on similarity scores
    4. Perform URL validation to reject invalid URLs (especially front/ URLs)
    5. Final quality control check
    
    Args:
        df: DataFrame with product data
        config: Configuration settings
        save_excel_output: Whether to save an Excel output file with images
        
    Returns:
        DataFrame with high-quality integrated and filtered images
    """
    logger.info("Integrating and filtering images with enhanced quality controls")
    
    # Step 1: Integrate images from all sources with higher thresholds
    df_with_images = integrate_images(df, config)
    logger.info(f"Image integration completed. DataFrame shape: {df_with_images.shape}")
    
    # Step 2: Improve Kogift image matching
    logger.info("Improving Kogift image matching with strict quality controls...")
    df_kogift_improved = improved_kogift_image_matching(df_with_images)
    logger.info(f"Kogift image matching improvement completed. DataFrame shape: {df_kogift_improved.shape}")
    
    # Step 3: Apply image filtering based on similarity and URL validity
    df_filtered = filter_images_by_similarity(df_kogift_improved, config)
    logger.info(f"Image filtering completed. DataFrame shape: {df_filtered.shape}")
    
    result_df = df_filtered.copy()
    
    # Ensure essential image columns exist in result_df before further processing and counting
    # This is crucial if any of the upstream functions returned the original df without these columns due to an error.
    required_image_columns = ['ë³¸ì‚¬ ì´ë¯¸ì§€', 'ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€', 'ë„¤ì´ë²„ ì´ë¯¸ì§€']
    for col_name in required_image_columns:
        if col_name not in result_df.columns:
            logger.warning(f"Column '{col_name}' missing in result_df before final validation and counting. Adding it with None values.")
            result_df[col_name] = None
            
    # Step 4: Final validation - ensure all URLs are valid and reject any problematic images
    # Now with less strict validation for Naver images
    logger.info("Performing final URL validation and quality check...")
    
    # Add an ImageFiltering section to config if it doesn't exist
    if 'ImageFiltering' not in config:
        config.add_section('ImageFiltering')
    
    # Get validation settings from config
    try:
        skip_naver_validation = config.getboolean('ImageFiltering', 'skip_naver_validation', fallback=True)
    except (configparser.NoSectionError, configparser.NoOptionError):
        skip_naver_validation = True  # Default to skipping Naver validation to avoid filtering
    
    for idx in range(len(result_df)):
        product_name = result_df.at[idx, 'ìƒí’ˆëª…'] if 'ìƒí’ˆëª…' in result_df.columns else f"Index {idx}"
        
        # Check Naver image URLs - much more lenient validation
        if 'ë„¤ì´ë²„ ì´ë¯¸ì§€' in result_df.columns:
            naver_data = result_df.at[idx, 'ë„¤ì´ë²„ ì´ë¯¸ì§€']
            if isinstance(naver_data, dict):
                # Check if there's a valid local_path
                local_path = naver_data.get('local_path')
                has_valid_local_path = local_path and os.path.exists(str(local_path))
                
                # Check URL validity but be very lenient
                url = naver_data.get('url')
                has_valid_url = url and isinstance(url, str) and url.startswith(('http://', 'https://'))
                
                # Special case: Keep Naver images with valid local path even without valid URL
                if has_valid_local_path:
                    logger.info(f"Row {idx} (Product: '{product_name}'): Keeping Naver image with invalid URL but valid local path.")
                    
                    # Try to fix/add URL from other available data
                    original_path = naver_data.get('original_path')
                    image_url = naver_data.get('original_url')
                    product_id = naver_data.get('product_id')
                    
                    # Try to find a valid URL but don't clear data if none found
                    if not has_valid_url:
                        if image_url and isinstance(image_url, str) and image_url.startswith(('http://', 'https://')):
                            naver_data['url'] = image_url
                        elif original_path and isinstance(original_path, str) and original_path.startswith(('http://', 'https://')):
                            naver_data['url'] = original_path
                        elif product_id:
                            # Construct URL from product_id as a last resort
                            constructed_url = f"https://shopping-phinf.pstatic.net/main_{product_id}/{product_id}.jpg"
                            naver_data['url'] = constructed_url
                            logger.info(f"Row {idx}: Constructed Naver URL from product_id: {constructed_url}")
                    
                    # Update DataFrame with possibly updated naver_data
                    result_df.at[idx, 'ë„¤ì´ë²„ ì´ë¯¸ì§€'] = naver_data
                elif not has_valid_url and skip_naver_validation:
                    # If we're skipping validation and there's no valid URL, warn but don't remove
                    logger.warning(f"Row {idx}: No valid URL found for Naver image, but keeping data due to skip_naver_validation=True.")
                elif not has_valid_url and not has_valid_local_path:
                    # Only clear if both URL and local file are invalid AND we're not skipping validation
                    logger.warning(f"Row {idx}: No valid URL or local path found for Naver image. Clearing Naver image data.")
                    result_df.at[idx, 'ë„¤ì´ë²„ ì´ë¯¸ì§€'] = None
                else:
                    # For all other cases, keep the data as is
                    pass
    
    # Count valid images after all processing
    naver_count = sum(1 for i in range(len(result_df)) if isinstance(result_df.at[i, 'ë„¤ì´ë²„ ì´ë¯¸ì§€'], dict))
    kogift_count = sum(1 for i in range(len(result_df)) if isinstance(result_df.at[i, 'ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€'], dict))
    haereum_count = sum(1 for i in range(len(result_df)) if isinstance(result_df.at[i, 'ë³¸ì‚¬ ì´ë¯¸ì§€'], dict))
    
    logger.info(f"Final image counts after validation: Haereum={haereum_count}, Kogift={kogift_count}, Naver={naver_count}")
    
    # Save Excel output if requested
    if save_excel_output:
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            excel_output = f"image_integration_result_{timestamp}.xlsx"
            
            # Create the Excel file with images
            create_excel_with_images(result_df, excel_output)
            logger.info(f"Created Excel output file with images: {excel_output}")
        except Exception as e:
            logger.error(f"Error creating Excel output: {e}")
    
    return result_df

def filter_images_by_similarity(df: pd.DataFrame, config: configparser.ConfigParser) -> pd.DataFrame:
    """
    Filter images based on similarity scores and URL validity.
    
    Args:
        df: DataFrame containing image data
        config: Configuration settings
        
    Returns:
        Filtered DataFrame
    """
    logger.info("Filtering images based on similarity scores...")
    
    # Get similarity threshold from config
    try:
        similarity_threshold = config.getfloat('ImageFiltering', 'similarity_threshold', fallback=0.4)
    except (configparser.NoSectionError, configparser.NoOptionError):
        similarity_threshold = 0.4  # Default threshold
    
    # Create a copy of the DataFrame to avoid modifying the original
    filtered_df = df.copy()
    
    # Process each row
    for idx in range(len(filtered_df)):
        # Check each image source
        for col_name in ['ë³¸ì‚¬ ì´ë¯¸ì§€', 'ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€', 'ë„¤ì´ë²„ ì´ë¯¸ì§€']:
            if col_name in filtered_df.columns:
                image_data = filtered_df.at[idx, col_name]
                
                # Skip if no image data
                if not isinstance(image_data, dict):
                    continue
                
                # Get similarity score - check both 'similarity' and 'score' keys
                score = image_data.get('similarity', image_data.get('score', 0.0))
                
                # Filter out low similarity scores
                if score < similarity_threshold:
                    logger.info(f"Filtering out {col_name} for row {idx} due to low similarity score: {score:.3f}")
                    filtered_df.at[idx, col_name] = None
    
    return filtered_df

def create_excel_with_images(df: pd.DataFrame, output_file: str):
    """
    Create an Excel file with embedded images.
    
    Args:
        df: DataFrame containing image data
        output_file: Path to output Excel file
    """
    logger.info(f"Creating Excel file with images: {output_file}")
    
    # Create a new Excel writer
    writer = pd.ExcelWriter(output_file, engine='openpyxl')
    
    # Write the DataFrame to Excel
    df.to_excel(writer, index=False, sheet_name='Images')
    
    # Get the worksheet
    worksheet = writer.sheets['Images']
    
    # Process each row
    for idx in range(len(df)):
        row_num = idx + 2  # Excel rows start at 1, and we have a header row
        
        # Process each image column
        for col_name in ['ë³¸ì‚¬ ì´ë¯¸ì§€', 'ê³ ë ¤ê¸°í”„íŠ¸ ì´ë¯¸ì§€', 'ë„¤ì´ë²„ ì´ë¯¸ì§€']:
            if col_name in df.columns:
                image_data = df.at[idx, col_name]
                
                # Skip if no image data
                if not isinstance(image_data, dict):
                    continue
                
                # Get image path
                image_path = image_data.get('local_path')
                if not image_path or not os.path.exists(str(image_path)):
                    continue
                
                # Add image to Excel
                try:
                    img = Image.open(str(image_path))
                    img_width, img_height = img.size
                    
                    # Resize image if too large
                    max_width = 200
                    if img_width > max_width:
                        ratio = max_width / img_width
                        img_width = max_width
                        img_height = int(img_height * ratio)
                    
                    # Create image cell
                    cell = worksheet.cell(row=row_num, column=df.columns.get_loc(col_name) + 1)
                    cell.value = f"Image: {os.path.basename(str(image_path))}"
                    
                    # Add image
                    img = openpyxl.drawing.image.Image(str(image_path))
                    img.width = img_width
                    img.height = img_height
                    worksheet.add_image(img, cell.coordinate)
                    
                except Exception as e:
                    logger.error(f"Error adding image to Excel: {e}")
    
    # Save the Excel file
    writer.close()
    logger.info(f"Excel file created successfully: {output_file}")

def calculate_text_similarity(text1: str, text2: str) -> float:
    """
    Calculate text similarity between two strings.
    Uses a combination of Levenshtein distance, token overlap, and character n-gram matching.
    """
    # Convert to strings if needed
    str1 = str(text1).lower()
    str2 = str(text2).lower()
    
    # Handle empty strings
    if not str1 or not str2:
        return 0.0
        
    try:
        # Try to use Levenshtein distance if available
        try:
            from Levenshtein import ratio
            lev_ratio = ratio(str1, str2)
        except ImportError:
            # Fallback to a basic similarity measure
            lev_ratio = len(set(str1) & set(str2)) / max(len(set(str1)), len(set(str2)))
        
        # Calculate token overlap
        tokens1 = set(str1.split())
        tokens2 = set(str2.split())
        
        # If either set is empty, default to character-based ratio
        if not tokens1 or not tokens2:
            return lev_ratio
            
        # Calculate Jaccard similarity coefficient
        intersection = tokens1.intersection(tokens2)
        union = tokens1.union(tokens2)
        
        if not union:
            return 0.0
            
        jaccard = len(intersection) / len(union)
        
        # Character n-gram matching (ë” ê´€ëŒ€í•œ ë§¤ì¹­ì„ ìœ„í•´ ì¶”ê°€)
        # 2-gramê³¼ 3-gram ë§¤ì¹­ ê³„ì‚°
        ngram_similarity = 0.0
        
        # 2-gram ë§¤ì¹­
        ngrams1_2 = set(str1[i:i+2] for i in range(len(str1)-1))
        ngrams2_2 = set(str2[i:i+2] for i in range(len(str2)-1))
        
        if ngrams1_2 and ngrams2_2:
            ngram_intersection_2 = ngrams1_2.intersection(ngrams2_2)
            ngram_union_2 = ngrams1_2.union(ngrams2_2)
            if ngram_union_2:
                ngram2_sim = len(ngram_intersection_2) / len(ngram_union_2)
                ngram_similarity += ngram2_sim
        
        # 3-gram ë§¤ì¹­ (ë” ê¸´ ë¬¸ìì—´ íŒ¨í„´ ë§¤ì¹­)
        if len(str1) >= 3 and len(str2) >= 3:
            ngrams1_3 = set(str1[i:i+3] for i in range(len(str1)-2))
            ngrams2_3 = set(str2[i:i+3] for i in range(len(str2)-2))
            
            if ngrams1_3 and ngrams2_3:
                ngram_intersection_3 = ngrams1_3.intersection(ngrams2_3)
                ngram_union_3 = ngrams1_3.union(ngrams2_3)
                if ngram_union_3:
                    ngram3_sim = len(ngram_intersection_3) / len(ngram_union_3)
                    ngram_similarity += ngram3_sim
        
        # Normalize n-gram similarity (if both n-grams used)
        ngram_similarity = ngram_similarity / 2 if len(str1) >= 3 and len(str2) >= 3 else ngram_similarity
        
        # Check for exact substring matches (ë¶€ë¶„ ë¬¸ìì—´ ì¼ì¹˜ í™•ì¸)
        # ê¸¸ì´ê°€ 3 ì´ìƒì¸ í† í°ì´ ë‹¤ë¥¸ ë¬¸ìì—´ì— í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ë³´ë„ˆìŠ¤ ì ìˆ˜
        substring_bonus = 0.0
        for token in tokens1:
            if len(token) >= 3 and token in str2:
                substring_bonus = max(substring_bonus, 0.15)  # ìµœëŒ€ 0.15 ë³´ë„ˆìŠ¤
                break
                
        for token in tokens2:
            if len(token) >= 3 and token in str1:
                substring_bonus = max(substring_bonus, 0.15)  # ìµœëŒ€ 0.15 ë³´ë„ˆìŠ¤
                break
        
        # Weighted average of all similarity measures
        # ê°€ì¤‘ì¹˜ ì¡°ì •ìœ¼ë¡œ ë” ê´€ëŒ€í•œ ë§¤ì¹­ í—ˆìš©
        combined_similarity = 0.2 * lev_ratio + 0.4 * jaccard + 0.25 * ngram_similarity + substring_bonus
        
        # ë„ˆë¬´ ë‚®ì€ ì ìˆ˜ì¼ ê²½ìš° ìµœì†Œê°’ìœ¼ë¡œ ì¡°ì • (ì™„ì „íˆ ê´€ë ¨ ì—†ëŠ” í•­ëª©ë„ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ)
        return max(combined_similarity, 0.01)  # ìµœì†Œ 0.01ì˜ ìœ ì‚¬ë„ ë°˜í™˜
        
    except Exception as e:
        logging.error(f"Error calculating text similarity: {e}")
        return 0.0

def extract_product_hash_from_filename(filename: str) -> Optional[str]:
    """
    íŒŒì¼ëª…ì—ì„œ 16ìë¦¬ ìƒí’ˆëª… í•´ì‹œê°’ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
    íŒŒì¼ëª… íŒ¨í„´:
    - prefix_[16ìí•´ì‹œ]_[8ìëœë¤].jpg (ì˜ˆ: haereum_1234567890abcdef_12345678.jpg)
    - prefix_[16ìí•´ì‹œ].jpg
        
    Args:
        filename: ì´ë¯¸ì§€ íŒŒì¼ëª…
            
    Returns:
        16ìë¦¬ ìƒí’ˆëª… í•´ì‹œê°’ ë˜ëŠ” None
    """
    try:
        # í™•ì¥ì ì œê±°
        name_without_ext = os.path.splitext(os.path.basename(filename))[0]
        
        # '_'ë¡œ ë¶„ë¦¬
        parts = name_without_ext.split('_')
        
        # prefix_hash_random ë˜ëŠ” prefix_hash íŒ¨í„´ í™•ì¸
        if len(parts) >= 2:
            # prefixë¥¼ ì œê±°í•˜ê³  ë‘ ë²ˆì§¸ ë¶€ë¶„ì´ 16ìë¦¬ í•´ì‹œì¸ì§€ í™•ì¸
            potential_hash = parts[1]
            if len(potential_hash) == 16 and all(c in '0123456789abcdef' for c in potential_hash.lower()):
                return potential_hash.lower()
        
        # ì „ì²´ê°€ 16ìë¦¬ í•´ì‹œì¸ ê²½ìš°ë„ í™•ì¸ (prefixê°€ ì—†ëŠ” ê²½ìš°)
        if len(name_without_ext) == 16 and all(c in '0123456789abcdef' for c in name_without_ext.lower()):
            return name_without_ext.lower()
                    
        return None
    except Exception as e:
        logging.debug(f"Error extracting hash from filename {filename}: {e}")
        return None

def generate_product_name_hash(product_name: str) -> str:
    """
    ìƒí’ˆëª…ìœ¼ë¡œë¶€í„° 16ìë¦¬ MD5 í•´ì‹œê°’ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    ì •ê·œí™” ê³¼ì •:
    1. ê³µë°± ë¬¸ì ì œê±°
    2. ì†Œë¬¸ì ë³€í™˜
    3. íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
    4. MD5 í•´ì‹œì˜ ì²« 16ìë¦¬ ë°˜í™˜
        
    Args:
        product_name: ìƒí’ˆëª…
            
    Returns:
        16ìë¦¬ í•´ì‹œê°’ (ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´)
    """
    try:
        if not product_name or not isinstance(product_name, str):
            logging.debug(f"ì˜ëª»ëœ ìƒí’ˆëª… ì…ë ¥: {product_name}")
            return ""
        
        # ìƒí’ˆëª… ì •ê·œí™”
        # 1. ì•ë’¤ ê³µë°± ì œê±°
        normalized = product_name.strip()
        
        # 2. ë‚´ë¶€ ê³µë°±ë“¤ì„ ëª¨ë‘ ì œê±°
        normalized = ''.join(normalized.split())
        
        # 3. ì†Œë¬¸ì ë³€í™˜
        normalized = normalized.lower()
        
        # 4. í•œê¸€ ì™¸ì˜ íŠ¹ìˆ˜ë¬¸ìëŠ” ìœ ì§€ (ë¸Œëœë“œëª… ë“±ì— í¬í•¨ë  ìˆ˜ ìˆìŒ)
        # ë‹¨, ì¼ê´€ì„±ì„ ìœ„í•´ ì¼ë¶€ íŠ¹ìˆ˜ë¬¸ìëŠ” ì •ë¦¬
        import re
        # ì—°ì†ëœ íŠ¹ìˆ˜ë¬¸ìëŠ” í•˜ë‚˜ë¡œ í†µì¼
        normalized = re.sub(r'[^\wê°€-í£]+', '', normalized)
        
        if not normalized:
            logging.debug(f"ì •ê·œí™” í›„ ë¹ˆ ë¬¸ìì—´: '{product_name}'")
            return ""
        
        # MD5 í•´ì‹œ ìƒì„±
        import hashlib
        hash_obj = hashlib.md5(normalized.encode('utf-8'))
        hash_result = hash_obj.hexdigest()[:16]
        
        logging.debug(f"í•´ì‹œ ìƒì„± ì™„ë£Œ: '{product_name}' -> '{normalized}' -> {hash_result}")
        
        return hash_result
        
    except Exception as e:
        logging.error(f"ìƒí’ˆëª… í•´ì‹œ ìƒì„± ì˜¤ë¥˜ '{product_name}': {e}")
        return ""

# ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ìš© ì½”ë“œ
if __name__ == "__main__":
    # ê¸°ë³¸ ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.DEBUG, # Change level to DEBUG for testing
        format='%(asctime)s - %(levelname)s - %(name)s - [%(funcName)s:%(lineno)d] - %(message)s',
        handlers=[logging.StreamHandler()]
    )
    
    # ì„¤ì • íŒŒì¼ ë¡œë“œ
    config = configparser.ConfigParser()
    # Assuming config.ini is in the parent directory of PythonScript
    config_path = Path(__file__).resolve().parent.parent / 'config.ini'
    if not config_path.exists():
        print(f"Error: config.ini not found at {config_path}")
        sys.exit(1)
    config.read(config_path, encoding='utf-8')
    
    # Test data setup needs careful handling of image paths
    # Ensure the image paths used for testing actually exist or simulate them.
    # For this example, we'll assume the paths are placeholders.
    
    # Get image dirs from config
    main_img_dir = Path(config.get('Paths', 'image_main_dir', fallback='C:\\\\RPA\\\\Image\\\\Main'))
    haereum_dir = main_img_dir / 'Haereum'
    kogift_dir = main_img_dir / 'Kogift'
    naver_dir = main_img_dir / 'Naver'

    # Create dummy image files for testing if they don't exist
    # (This part might need adjustment based on your actual test environment)
    dummy_haereum_img = haereum_dir / "haereum_test_product_1_dummy.jpg"
    dummy_kogift_img = kogift_dir / "kogift_test_product_2_dummy.jpg"
    dummy_naver_img = naver_dir / "naver_test_product_3_dummy.jpg"
    
    for d in [haereum_dir, kogift_dir, naver_dir]:
        d.mkdir(parents=True, exist_ok=True)
        
    for img_file in [dummy_haereum_img, dummy_kogift_img, dummy_naver_img]:
        if not img_file.exists():
            try:
                img_file.touch() # Create empty file
                print(f"Created dummy image file: {img_file}")
            except Exception as e:
                print(f"Could not create dummy file {img_file}: {e}")

    test_df = pd.DataFrame({
        'ë²ˆí˜¸': [1, 2, 3, 4],
        'ìƒí’ˆëª…': ['í…ŒìŠ¤íŠ¸ ìƒí’ˆ 1', 'Test Product 2', 'í•´ì˜¤ë¦„ í…ŒìŠ¤íŠ¸', 'ì € ìœ ì‚¬ë„ ìƒí’ˆ'],
        # Use source URL columns from scraping (example names)
        'í•´ì˜¤ë¦„ì´ë¯¸ì§€URL': ['http://example.com/hae1.jpg', None, 'https://www.jclgift.com/upload/product/simg3/DDAC0001000s.jpg', 'http://example.com/hae4.jpg'],
        'ê³ ë ¤ê¸°í”„íŠ¸ URL': [None, 'https://koreagift.com/ez/upload/mall/shop_1707873892937710_0.jpg', None, 'http://example.com/ko4.jpg'],
        'ë„¤ì´ë²„ì´ë¯¸ì§€ URL': ['https://shop-phinf.pstatic.net/20240101_1/image.jpg', None, None, 'http://example.com/na4.jpg'],
        'ì´ë¯¸ì§€_ìœ ì‚¬ë„': [0.6, 0.8, 0.9, 0.2], # This column should now be ignored by filter_images_by_similarity
        # Add other necessary columns from FINAL_COLUMN_ORDER for the test
        'êµ¬ë¶„': ['A', 'A', 'P', 'A'], 'ë‹´ë‹¹ì': ['Test']*4, 'ì—…ì²´ëª…': ['Test']*4, 'ì—…ì²´ì½”ë“œ': ['123']*4, 'Code': ['T01', 'T02', 'T03', 'T04'], 'ì¤‘ë¶„ë¥˜ì¹´í…Œê³ ë¦¬': ['Test']*4,
        'ê¸°ë³¸ìˆ˜ëŸ‰(1)': [100]*4, 'íŒë§¤ë‹¨ê°€(Ví¬í•¨)': [1000]*4, 'ë³¸ì‚¬ìƒí’ˆë§í¬': ['http://example.com/1']*4,
        'ê¸°ë³¸ìˆ˜ëŸ‰(2)': [100]*4, 'íŒë§¤ê°€(Ví¬í•¨)(2)': [1100]*4, 'ê°€ê²©ì°¨ì´(2)': [100]*4, 'ê°€ê²©ì°¨ì´(2)(%)': [10]*4, 'ê³ ë ¤ê¸°í”„íŠ¸ ìƒí’ˆë§í¬': ['http://example.com/2']*4,
        'ê¸°ë³¸ìˆ˜ëŸ‰(3)': [100]*4, 'íŒë§¤ë‹¨ê°€(Ví¬í•¨)(3)': [900]*4, 'ê°€ê²©ì°¨ì´(3)': [-100]*4, 'ê°€ê²©ì°¨ì´(3)(%)': [-10]*4, 'ê³µê¸‰ì‚¬ëª…': ['Test']*4, 'ë„¤ì´ë²„ ì‡¼í•‘ ë§í¬': ['http://example.com/3']*4, 'ê³µê¸‰ì‚¬ ìƒí’ˆë§í¬': ['http://example.com/supplier']*4
    })
    
    # --- Simulate adding image dicts (as would be done by integrate_images) ---
    # This is crucial for testing filter_images_by_similarity correctly
    # We manually add the 'score' key here based on example values
    test_df['í•´ì˜¤ë¦„(ì´ë¯¸ì§€ë§í¬)'] = [
        {'local_path': str(dummy_haereum_img), 'url': 'http://example.com/hae1.jpg', 'source': 'haereum', 'score': 0.85},
        None,
        {'local_path': str(dummy_haereum_img), 'url': 'https://www.jclgift.com/upload/product/simg3/DDAC0001000s.jpg', 'source': 'haereum', 'score': 0.95},
         {'local_path': str(dummy_haereum_img), 'url': 'http://example.com/hae4.jpg', 'source': 'haereum', 'score': 0.90} # High score, should not be filtered
    ]
    test_df['ê³ ë ¤ê¸°í”„íŠ¸(ì´ë¯¸ì§€ë§í¬)'] = [
        None,
        {'local_path': str(dummy_kogift_img), 'url': 'https://koreagift.com/ez/upload/mall/shop_1707873892937710_0.jpg', 'source': 'kogift', 'score': 0.75},
        None,
        {'local_path': str(dummy_kogift_img), 'url': 'http://example.com/ko4.jpg', 'source': 'kogift', 'score': 0.25} # Low score, should be filtered
    ]
    test_df['ë„¤ì´ë²„ì‡¼í•‘(ì´ë¯¸ì§€ë§í¬)'] = [
        {'local_path': str(dummy_naver_img), 'url': 'https://shop-phinf.pstatic.net/20240101_1/image.jpg', 'source': 'naver', 'score': 0.65},
        None,
        None,
        {'local_path': str(dummy_naver_img), 'url': 'http://example.com/na4.jpg', 'source': 'naver', 'score': 0.15} # Low score, should be filtered
    ]
    
    # --- Run only the filtering part for isolated testing ---
    logging.info("--- Testing filter_images_by_similarity ---")
    filtered_df = filter_images_by_similarity(test_df.copy(), config) # Use copy
    
    logging.info(f"Test filter results - DataFrame shape: {filtered_df.shape}")
    logging.info(f"í•´ì˜¤ë¦„(ì´ë¯¸ì§€ë§í¬) after filter: {filtered_df['í•´ì˜¤ë¦„(ì´ë¯¸ì§€ë§í¬)'].tolist()}")
    logging.info(f"ê³ ë ¤ê¸°í”„íŠ¸(ì´ë¯¸ì§€ë§í¬) after filter: {filtered_df['ê³ ë ¤ê¸°í”„íŠ¸(ì´ë¯¸ì§€ë§í¬)'].tolist()}")
    logging.info(f"ë„¤ì´ë²„ì‡¼í•‘(ì´ë¯¸ì§€ë§í¬) after filter: {filtered_df['ë„¤ì´ë²„ì‡¼í•‘(ì´ë¯¸ì§€ë§í¬)'].tolist()}")
    
    # --- Run the full integrate_and_filter process ---
    logging.info("--- Testing integrate_and_filter_images ---")
    # Use a fresh copy for the full test
    full_result_df = integrate_and_filter_images(test_df.copy(), config, save_excel_output=True) 
    
    # ê²°ê³¼ ì¶œë ¥ (using the new final column names)
    logging.info(f"Full process result - DataFrame shape: {full_result_df.shape}")
    logging.info(f"í•´ì˜¤ë¦„(ì´ë¯¸ì§€ë§í¬) final data: {full_result_df['í•´ì˜¤ë¦„(ì´ë¯¸ì§€ë§í¬)'].tolist()}")
    logging.info(f"ê³ ë ¤ê¸°í”„íŠ¸(ì´ë¯¸ì§€ë§í¬) final data: {full_result_df['ê³ ë ¤ê¸°í”„íŠ¸(ì´ë¯¸ì§€ë§í¬)'].tolist()}")
    logging.info(f"ë„¤ì´ë²„ì‡¼í•‘(ì´ë¯¸ì§€ë§í¬) final data: {full_result_df['ë„¤ì´ë²„ì‡¼í•‘(ì´ë¯¸ì§€ë§í¬)'].tolist()}") 


def get_system_status_summary(config: configparser.ConfigParser = None) -> Dict:
    """
    ê°œì„ ëœ ì´ë¯¸ì§€ ë§¤ì¹­ ì‹œìŠ¤í…œì˜ í˜„ì¬ ìƒíƒœë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.
    
    Returns:
        ì‹œìŠ¤í…œ ìƒíƒœ ì •ë³´ê°€ ë‹´ê¸´ ë”•ì…”ë„ˆë¦¬
    """
    
    try:
        import psutil
        import platform
        from datetime import datetime
        
        status = {
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'system_info': {
                'platform': platform.platform(),
                'python_version': platform.python_version(),
                'cpu_count': psutil.cpu_count(),
                'memory_total_gb': round(psutil.virtual_memory().total / (1024**3), 2),
                'memory_available_gb': round(psutil.virtual_memory().available / (1024**3), 2)
            },
            'matching_system': {
                'version': 'ê°œì„ ëœ í•´ì‹œ ê¸°ë°˜ 2ë‹¨ê³„ ë§¤ì¹­',
                'hash_algorithm': 'MD5 (16ìë¦¬)',
                'image_similarity_threshold': 0.8,
                'enhanced_matcher_available': False,
                'gpu_enabled': False
            },
            'performance_metrics': {
                'expected_hash_match_rate': '95%+ (ë™ì¼ ìƒí’ˆ)',
                'expected_processing_speed': '~80% í–¥ìƒ',
                'memory_usage_reduction': '~60% ì ˆì•½',
                'cache_enabled': True
            },
            'improvements': [
                'âœ… í•´ì‹œ ê¸°ë°˜ 1ì°¨ ì •í™•í•œ ë§¤ì¹­',
                'âœ… ì´ë¯¸ì§€ ìœ ì‚¬ë„ 2ì°¨ ê²€ì¦',
                'âœ… ëœë¤ í• ë‹¹ ì œê±°',
                'âœ… ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€',
                'âœ… ëª…í™•í•œ ë¡œê·¸ ë° ìƒíƒœ í‘œì‹œ',
                'âœ… ì„±ëŠ¥ ìµœì í™”'
            ]
        }
        
        # Enhanced Image Matcher ìƒíƒœ í™•ì¸
        try:
            from enhanced_image_matcher import EnhancedImageMatcher
            enhanced_matcher = EnhancedImageMatcher(config)
            status['matching_system']['enhanced_matcher_available'] = True
            status['matching_system']['gpu_enabled'] = getattr(enhanced_matcher, 'use_gpu', False)
            
            # GPU ì •ë³´ ì¶”ê°€
            if status['matching_system']['gpu_enabled']:
                try:
                    import tensorflow as tf
                    gpus = tf.config.list_physical_devices('GPU')
                    status['gpu_info'] = {
                        'gpu_count': len(gpus),
                        'gpu_devices': [str(gpu) for gpu in gpus] if gpus else []
                    }
                except Exception:
                    status['gpu_info'] = {'error': 'GPU ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨'}
        except Exception as e:
            status['matching_system']['enhanced_matcher_error'] = str(e)
        
        # ë””ë ‰í† ë¦¬ ìƒíƒœ í™•ì¸
        if config:
            try:
                main_img_dir = Path(config.get('Paths', 'image_main_dir', fallback='C:\\RPA\\Image\\Main'))
                directories = {
                    'haereum': main_img_dir / 'Haereum',
                    'kogift': main_img_dir / 'Kogift', 
                    'naver': main_img_dir / 'Naver'
                }
                
                dir_status = {}
                for name, path in directories.items():
                    if path.exists():
                        image_files = list(path.glob('*.jpg')) + list(path.glob('*.png'))
                        dir_status[name] = {
                            'exists': True,
                            'image_count': len(image_files),
                            'path': str(path)
                        }
                    else:
                        dir_status[name] = {
                            'exists': False,
                            'path': str(path)
                        }
                
                status['image_directories'] = dir_status
            except Exception as e:
                status['image_directories'] = {'error': str(e)}
        
        return status
        
    except Exception as e:
        return {
            'error': f"ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ ì˜¤ë¥˜: {e}",
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }


def print_system_status(config: configparser.ConfigParser = None):
    """ì‹œìŠ¤í…œ ìƒíƒœë¥¼ ì½˜ì†”ì— ì¶œë ¥í•©ë‹ˆë‹¤."""
    
    status = get_system_status_summary(config)
    
    print("\n" + "="*60)
    print("ğŸš€ ê°œì„ ëœ ì´ë¯¸ì§€ ë§¤ì¹­ ì‹œìŠ¤í…œ ìƒíƒœ")
    print("="*60)
    
    if 'error' in status:
        print(f"âŒ ì˜¤ë¥˜: {status['error']}")
        return
    
    print(f"ğŸ“… ì¡°íšŒ ì‹œê°„: {status['timestamp']}")
    
    # ì‹œìŠ¤í…œ ì •ë³´
    sys_info = status['system_info']
    print(f"\nğŸ’» ì‹œìŠ¤í…œ ì •ë³´:")
    print(f"   OS: {sys_info['platform']}")
    print(f"   Python: {sys_info['python_version']}")
    print(f"   CPU: {sys_info['cpu_count']}ì½”ì–´")
    print(f"   ë©”ëª¨ë¦¬: {sys_info['memory_available_gb']:.1f}GB / {sys_info['memory_total_gb']:.1f}GB")
    
    # ë§¤ì¹­ ì‹œìŠ¤í…œ
    match_sys = status['matching_system']
    print(f"\nğŸ¯ ë§¤ì¹­ ì‹œìŠ¤í…œ:")
    print(f"   ë²„ì „: {match_sys['version']}")
    print(f"   í•´ì‹œ ì•Œê³ ë¦¬ì¦˜: {match_sys['hash_algorithm']}")
    print(f"   ìœ ì‚¬ë„ ì„ê³„ê°’: {match_sys['image_similarity_threshold']}")
    print(f"   ê³ ê¸‰ ë§¤ì²˜: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if match_sys['enhanced_matcher_available'] else 'âŒ ì‚¬ìš© ë¶ˆê°€'}")
    print(f"   GPU ê°€ì†: {'âœ… í™œì„±í™”' if match_sys['gpu_enabled'] else 'âŒ ë¹„í™œì„±í™”'}")
    
    # GPU ì •ë³´
    if 'gpu_info' in status:
        gpu_info = status['gpu_info']
        if 'error' not in gpu_info:
            print(f"   GPU ê°œìˆ˜: {gpu_info['gpu_count']}ê°œ")
            for i, gpu in enumerate(gpu_info['gpu_devices']):
                print(f"     GPU {i}: {gpu}")
    
    # ì„±ëŠ¥ ë©”íŠ¸ë¦­
    perf = status['performance_metrics']
    print(f"\nğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
    print(f"   ì˜ˆìƒ í•´ì‹œ ë§¤ì¹­ë¥ : {perf['expected_hash_match_rate']}")
    print(f"   ì²˜ë¦¬ ì†ë„ ê°œì„ : {perf['expected_processing_speed']}")
    print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì ˆì•½: {perf['memory_usage_reduction']}")
    print(f"   ìºì‹œ ì‚¬ìš©: {'âœ…' if perf['cache_enabled'] else 'âŒ'}")
    
    # ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ìƒíƒœ
    if 'image_directories' in status and 'error' not in status['image_directories']:
        print(f"\nğŸ“ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬:")
        for name, info in status['image_directories'].items():
            if info['exists']:
                print(f"   {name}: âœ… {info['image_count']}ê°œ ì´ë¯¸ì§€")
            else:
                print(f"   {name}: âŒ ë””ë ‰í† ë¦¬ ì—†ìŒ")
    
    # ê°œì„ ì‚¬í•­
    print(f"\nğŸ‰ ì£¼ìš” ê°œì„ ì‚¬í•­:")
    for improvement in status['improvements']:
        print(f"   {improvement}")
    
    print("\n" + "="*60)